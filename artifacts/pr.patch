diff --git a/ci-requirements.txt b/ci-requirements.txt
index 7f18321..976fc9d 100644
--- a/ci-requirements.txt
+++ b/ci-requirements.txt
@@ -20,5 +20,3 @@ pip-tools>=7.4
 setuptools>=80.9
 wheel>=0.45
 pip>=24.3
-google-auth>=2.38
-google-api-python-client>=2.166
diff --git a/lan_app/api.py b/lan_app/api.py
index c18888e..4ff6995 100644
--- a/lan_app/api.py
+++ b/lan_app/api.py
@@ -51,7 +51,6 @@ from .healthchecks import (
     check_worker_health,
     collect_health_checks,
 )
-from .locks import release_ingest_lock, try_acquire_ingest_lock
 from .ops import run_retention_cleanup
 from .reaper import run_stuck_job_reaper_once
 from .uploads import (
@@ -442,38 +441,6 @@ async def api_upload_file(file: UploadFile = File(...)) -> dict[str, object]:
                 delete_recording(recording_id, settings=_settings)
 
 
-@app.post("/api/actions/ingest")
-async def api_ingest_once() -> dict[str, object]:
-    """Trigger a single Google Drive ingest cycle."""
-    from .gdrive import ingest_once
-
-    try:
-        acquired, retry_after, lock_token = try_acquire_ingest_lock(_settings)
-    except Exception as exc:
-        raise HTTPException(status_code=503, detail=f"Ingest lock unavailable: {exc}")
-
-    if not acquired:
-        return JSONResponse(
-            status_code=409,
-            content={
-                "detail": "Ingest is already running.",
-                "retry_after_seconds": retry_after,
-            },
-        )
-
-    try:
-        results = ingest_once(settings=_settings)
-    except ValueError as exc:
-        raise HTTPException(status_code=422, detail=str(exc))
-    except Exception as exc:
-        raise HTTPException(status_code=503, detail=f"Ingest failed: {exc}")
-    finally:
-        if lock_token is not None:
-            with suppress(Exception):
-                release_ingest_lock(_settings, token=lock_token)
-    return {"ingested": results, "count": len(results)}
-
-
 def set_current_result(result: TranscriptResult | None) -> None:
     global _current_result
     _current_result = result
diff --git a/lan_app/config.py b/lan_app/config.py
index 7b47dc3..8e25753 100644
--- a/lan_app/config.py
+++ b/lan_app/config.py
@@ -101,24 +101,6 @@ class AppSettings(BaseSettings):
         ),
     )
 
-    # Google Drive ingest
-    gdrive_sa_json_path: Path | None = Field(
-        default=None,
-        validation_alias=AliasChoices("GDRIVE_SA_JSON_PATH", "LAN_GDRIVE_SA_JSON_PATH"),
-    )
-    gdrive_inbox_folder_id: str | None = Field(
-        default=None,
-        validation_alias=AliasChoices(
-            "GDRIVE_INBOX_FOLDER_ID", "LAN_GDRIVE_INBOX_FOLDER_ID"
-        ),
-    )
-    gdrive_poll_interval_seconds: int = Field(
-        default=60,
-        validation_alias=AliasChoices(
-            "GDRIVE_POLL_INTERVAL_SECONDS", "LAN_GDRIVE_POLL_INTERVAL_SECONDS"
-        ),
-    )
-
     routing_auto_select_threshold: float = Field(
         default=0.65,
         ge=0.0,
@@ -144,14 +126,6 @@ class AppSettings(BaseSettings):
             "API_BEARER_TOKEN",
         ),
     )
-    ingest_lock_ttl_seconds: int = Field(
-        default=300,
-        ge=1,
-        validation_alias=AliasChoices(
-            "LAN_INGEST_LOCK_TTL_SECONDS",
-            "INGEST_LOCK_TTL_SECONDS",
-        ),
-    )
     upload_max_bytes: int | None = Field(
         default=None,
         ge=1,
diff --git a/lan_app/gdrive.py b/lan_app/gdrive.py
deleted file mode 100644
index fda4ff5..0000000
--- a/lan_app/gdrive.py
+++ /dev/null
@@ -1,243 +0,0 @@
-"""Google Drive ingest: Service Account + shared Inbox folder.
-
-Polls a shared Drive folder for new audio files, downloads them,
-creates recording rows and enqueues the processing pipeline.
-"""
-
-from __future__ import annotations
-
-import logging
-import re
-from datetime import datetime, timezone
-from pathlib import Path
-from typing import Any
-from uuid import uuid4
-
-from google.oauth2.service_account import Credentials
-from googleapiclient.discovery import build
-from googleapiclient.http import MediaIoBaseDownload
-
-from .config import AppSettings
-from .constants import (
-    JOB_TYPE_PRECHECK,
-    RECORDING_STATUS_QUEUED,
-)
-import shutil
-
-from .db import connect, create_recording, init_db
-
-logger = logging.getLogger(__name__)
-
-SCOPES = ["https://www.googleapis.com/auth/drive.readonly"]
-
-_PIPELINE_STEPS = (
-    JOB_TYPE_PRECHECK,
-)
-
-# Plaud filename patterns:
-#   "2026-02-18 16_01_43.mp3"
-#   "2026-02-18 16-01-43.mp3"
-_PLAUD_RE = re.compile(
-    r"(\d{4})-(\d{2})-(\d{2})\s+(\d{2})[_\-](\d{2})[_\-](\d{2})"
-)
-
-
-def parse_plaud_captured_at(filename: str) -> str | None:
-    """Parse captured_at ISO-8601 timestamp from a Plaud-style filename.
-
-    Returns ISO-8601 string (UTC) on success, None if no match.
-    """
-    m = _PLAUD_RE.search(filename)
-    if m is None:
-        return None
-    year, month, day, hour, minute, second = (int(g) for g in m.groups())
-    try:
-        dt = datetime(year, month, day, hour, minute, second, tzinfo=timezone.utc)
-    except ValueError:
-        return None
-    return dt.isoformat().replace("+00:00", "Z")
-
-
-def build_drive_service(sa_json_path: Path) -> Any:
-    """Create an authenticated Google Drive API v3 service."""
-    creds = Credentials.from_service_account_file(str(sa_json_path), scopes=SCOPES)
-    return build("drive", "v3", credentials=creds, cache_discovery=False)
-
-
-def list_inbox_files(service: Any, folder_id: str) -> list[dict[str, Any]]:
-    """List files in the Inbox folder.
-
-    Returns a list of Drive file metadata dicts with id, name,
-    md5Checksum, mimeType, createdTime.
-    """
-    query = f"'{folder_id}' in parents and trashed=false"
-    fields = "files(id,name,md5Checksum,mimeType,createdTime)"
-    results: list[dict[str, Any]] = []
-    page_token: str | None = None
-
-    while True:
-        resp = (
-            service.files()
-            .list(
-                q=query,
-                fields=f"nextPageToken,{fields}",
-                pageToken=page_token,
-                pageSize=100,
-            )
-            .execute()
-        )
-        results.extend(resp.get("files", []))
-        page_token = resp.get("nextPageToken")
-        if not page_token:
-            break
-    return results
-
-
-def _known_drive_file_ids(settings: AppSettings) -> set[str]:
-    """Return set of drive_file_id values already in the DB."""
-    init_db(settings)
-    with connect(settings) as conn:
-        rows = conn.execute(
-            "SELECT drive_file_id FROM recordings WHERE drive_file_id IS NOT NULL"
-        ).fetchall()
-    return {row[0] for row in rows}
-
-
-def download_file(service: Any, file_id: str, dest: Path) -> Path:
-    """Download a Drive file to *dest*."""
-    dest.parent.mkdir(parents=True, exist_ok=True)
-    request = service.files().get_media(fileId=file_id)
-    with dest.open("wb") as fh:
-        downloader = MediaIoBaseDownload(fh, request)
-        done = False
-        while not done:
-            _, done = downloader.next_chunk()
-    return dest
-
-
-def _suffix_from_name(name: str) -> str:
-    """Extract file extension from filename, default to .mp3."""
-    idx = name.rfind(".")
-    if idx > 0:
-        return name[idx:]
-    return ".mp3"
-
-
-def _enqueue_pipeline_jobs(
-    recording_id: str,
-    settings: AppSettings,
-) -> list[str]:
-    """Enqueue the single full-pipeline job into Redis/RQ."""
-    from .jobs import enqueue_recording_job
-
-    rj = enqueue_recording_job(
-        recording_id,
-        job_type=JOB_TYPE_PRECHECK,
-        settings=settings,
-    )
-    return [rj.job_id]
-
-
-def ingest_once(
-    settings: AppSettings | None = None,
-    *,
-    service: Any | None = None,
-) -> list[dict[str, Any]]:
-    """Run a single ingest cycle.
-
-    Returns a list of dicts describing each newly ingested recording.
-    """
-    cfg = settings or AppSettings()
-
-    if not cfg.gdrive_sa_json_path or not str(cfg.gdrive_sa_json_path).strip():
-        raise ValueError("GDRIVE_SA_JSON_PATH is not configured")
-    if not cfg.gdrive_inbox_folder_id or not cfg.gdrive_inbox_folder_id.strip():
-        raise ValueError("GDRIVE_INBOX_FOLDER_ID is not configured")
-
-    init_db(cfg)
-
-    if service is None:
-        service = build_drive_service(cfg.gdrive_sa_json_path)
-
-    inbox_files = list_inbox_files(service, cfg.gdrive_inbox_folder_id)
-    known_ids = _known_drive_file_ids(cfg)
-    new_files = [f for f in inbox_files if f["id"] not in known_ids]
-
-    ingested: list[dict[str, Any]] = []
-
-    for drive_file in new_files:
-        file_id = drive_file["id"]
-        file_name = drive_file["name"]
-        drive_md5 = drive_file.get("md5Checksum")
-
-        recording_id = f"trs_{uuid4().hex[:8]}"
-        ext = _suffix_from_name(file_name)
-        raw_dir = cfg.recordings_root / recording_id / "raw"
-        dest = raw_dir / f"audio{ext}"
-
-        try:
-            download_file(service, file_id, dest)
-        except Exception:
-            logger.exception("Failed to download %s (%s)", file_name, file_id)
-            # Clean up the partially written recording directory
-            rec_dir = cfg.recordings_root / recording_id
-            shutil.rmtree(rec_dir, ignore_errors=True)
-            continue
-
-        # Parse captured_at from Plaud filename; fall back to Drive createdTime
-        captured_at = parse_plaud_captured_at(file_name)
-        capture_warning = None
-        if captured_at is None:
-            created_time = drive_file.get("createdTime")
-            if created_time:
-                captured_at = created_time
-            else:
-                captured_at = (
-                    datetime.now(tz=timezone.utc)
-                    .replace(microsecond=0)
-                    .isoformat()
-                    .replace("+00:00", "Z")
-                )
-            capture_warning = "captured_at parsed from Drive createdTime (filename parse failed)"
-
-        create_recording(
-            recording_id,
-            source="gdrive",
-            source_filename=file_name,
-            captured_at=captured_at,
-            status=RECORDING_STATUS_QUEUED,
-            drive_file_id=file_id,
-            drive_md5=drive_md5,
-            settings=cfg,
-        )
-
-        job_ids = _enqueue_pipeline_jobs(recording_id, cfg)
-
-        result: dict[str, Any] = {
-            "recording_id": recording_id,
-            "drive_file_id": file_id,
-            "source_filename": file_name,
-            "captured_at": captured_at,
-            "jobs_created": len(job_ids),
-        }
-        if capture_warning:
-            result["warning"] = capture_warning
-
-        ingested.append(result)
-        logger.info(
-            "Ingested %s -> %s (%d jobs)",
-            file_name,
-            recording_id,
-            len(job_ids),
-        )
-
-    return ingested
-
-
-__all__ = [
-    "build_drive_service",
-    "download_file",
-    "ingest_once",
-    "list_inbox_files",
-    "parse_plaud_captured_at",
-]
diff --git a/lan_app/locks.py b/lan_app/locks.py
deleted file mode 100644
index d296870..0000000
--- a/lan_app/locks.py
+++ /dev/null
@@ -1,63 +0,0 @@
-from __future__ import annotations
-
-from uuid import uuid4
-
-from redis import Redis
-
-from .config import AppSettings
-
-INGEST_LOCK_KEY = "lan:ingest:lock"
-_RELEASE_IF_VALUE_MATCHES_SCRIPT = """
-if redis.call("GET", KEYS[1]) == ARGV[1] then
-    return redis.call("DEL", KEYS[1])
-end
-return 0
-"""
-
-
-def _ingest_lock_ttl(settings: AppSettings) -> int:
-    return max(1, int(settings.ingest_lock_ttl_seconds))
-
-
-def _redis_client(settings: AppSettings) -> Redis:
-    return Redis.from_url(settings.redis_url)
-
-
-def try_acquire_ingest_lock(
-    settings: AppSettings,
-    *,
-    redis_client: Redis | None = None,
-    key: str = INGEST_LOCK_KEY,
-) -> tuple[bool, int, str | None]:
-    client = redis_client or _redis_client(settings)
-    ttl_seconds = _ingest_lock_ttl(settings)
-    lock_token = uuid4().hex
-    acquired = bool(client.set(key, lock_token, nx=True, ex=ttl_seconds))
-    if acquired:
-        return True, ttl_seconds, lock_token
-
-    retry_after = client.ttl(key)
-    if retry_after is None or int(retry_after) <= 0:
-        return False, ttl_seconds, None
-    return False, int(retry_after), None
-
-
-def release_ingest_lock(
-    settings: AppSettings,
-    *,
-    token: str | None,
-    redis_client: Redis | None = None,
-    key: str = INGEST_LOCK_KEY,
-) -> bool:
-    if token is None:
-        return False
-    client = redis_client or _redis_client(settings)
-    result = client.eval(_RELEASE_IF_VALUE_MATCHES_SCRIPT, 1, key, token)
-    return int(result) == 1
-
-
-__all__ = [
-    "INGEST_LOCK_KEY",
-    "release_ingest_lock",
-    "try_acquire_ingest_lock",
-]
diff --git a/lan_app/templates/base.html b/lan_app/templates/base.html
index 8ba1e1a..47c2e55 100644
--- a/lan_app/templates/base.html
+++ b/lan_app/templates/base.html
@@ -74,12 +74,11 @@ tr:hover{background:#e8ecf8}
 <nav>
   <span class="nav-logo">LAN Transcriber</span>
   <a href="/" {% if active=='dashboard' %}class="active"{% endif %}>Dashboard</a>
-  <a href="/recordings" {% if active=='recordings' %}class="active"{% endif %}>Recordings</a>
   <a href="/upload" {% if active=='upload' %}class="active"{% endif %}>Upload</a>
+  <a href="/recordings" {% if active=='recordings' %}class="active"{% endif %}>Recordings</a>
   <a href="/queue" {% if active=='queue' %}class="active"{% endif %}>Queue</a>
   <a href="/projects" {% if active=='projects' %}class="active"{% endif %}>Projects</a>
   <a href="/voices" {% if active=='voices' %}class="active"{% endif %}>Voices</a>
-  <a href="/connections" {% if active=='connections' %}class="active"{% endif %}>Connections</a>
 </nav>
 <main>
 {% block content %}{% endblock %}
diff --git a/lan_app/templates/connections.html b/lan_app/templates/connections.html
deleted file mode 100644
index 53042f5..0000000
--- a/lan_app/templates/connections.html
+++ /dev/null
@@ -1,87 +0,0 @@
-{% extends "base.html" %}
-{% block title %}Connections â€“ LAN Transcriber{% endblock %}
-{% block content %}
-<h1>Connections</h1>
-
-<div class="conn-card">
-  <h3>Google Drive (Service Account)</h3>
-  {% if gdrive.configured %}
-  <div class="conn-row">
-    <span class="dot-ok"></span>
-    <span>Configured</span>
-  </div>
-  <div class="info-grid" style="margin-top:8px">
-    <div class="k">Service account key</div>
-    <div class="v"><code>{{ gdrive.sa_path }}</code></div>
-    <div class="k">Inbox folder ID</div>
-    <div class="v"><code>{{ gdrive.folder_id }}</code></div>
-  </div>
-  <div class="conn-row" style="margin-top:8px">
-    <button
-      type="button"
-      class="btn"
-      hx-post="/ui/connections/gdrive/test"
-      hx-target="#gdrive-test-result"
-      hx-swap="innerHTML"
-    >
-      Test connection
-    </button>
-    <button type="button" class="btn" onclick="runIngestNow()">Run ingest now</button>
-  </div>
-  <div id="gdrive-test-result" style="font-size:11px;color:#666;margin-top:4px"></div>
-  <div id="gdrive-ingest-result" style="font-size:11px;color:#666;margin-top:2px"></div>
-  {% else %}
-  <div class="conn-row">
-    <span class="dot-na"></span>
-    <span style="color:#888">Not configured.</span>
-  </div>
-  <div style="font-size:11px;color:#999;margin-top:4px">
-    Set <code>GDRIVE_SA_JSON_PATH</code> and <code>GDRIVE_INBOX_FOLDER_ID</code>.
-  </div>
-  {% endif %}
-  <div style="font-size:11px;color:#999;margin-top:4px">
-    Ingest: Service Account + shared folder (Inbox).<br>
-    Credentials: <code>/data/secrets/gdrive_sa.json</code>
-  </div>
-</div>
-
-<script>
-async function runIngestNow() {
-  var out = document.getElementById('gdrive-ingest-result');
-  if (!out) return;
-  out.style.color = '#666';
-  out.textContent = 'Running ingest...';
-
-  var response = null;
-  var data = {};
-  try {
-    response = await fetch('/api/actions/ingest', {method: 'POST'});
-    data = await response.json();
-  } catch (_err) {
-    out.style.color = '#b42318';
-    out.textContent = 'Ingest request failed.';
-    return;
-  }
-
-  if (response.status === 409) {
-    out.style.color = '#92400e';
-    var retryAfter = data.retry_after_seconds;
-    if (typeof retryAfter === 'number') {
-      out.textContent = 'Ingest is already running. Retry in ' + retryAfter + 's.';
-    } else {
-      out.textContent = 'Ingest is already running.';
-    }
-    return;
-  }
-
-  if (!response.ok) {
-    out.style.color = '#b42318';
-    out.textContent = data.detail || 'Ingest failed.';
-    return;
-  }
-
-  out.style.color = '#14532d';
-  out.textContent = 'Ingest completed. New recordings: ' + (data.count || 0) + '.';
-}
-</script>
-{% endblock %}
diff --git a/lan_app/ui_routes.py b/lan_app/ui_routes.py
index 2a52d31..9915cee 100644
--- a/lan_app/ui_routes.py
+++ b/lan_app/ui_routes.py
@@ -62,7 +62,6 @@ from .db import (
     set_recording_status,
 )
 from .exporter import build_export_zip_bytes, build_onenote_markdown
-from .gdrive import build_drive_service
 from .jobs import (
     DuplicateRecordingJobError,
     enqueue_recording_job,
@@ -157,48 +156,6 @@ def _pipeline_stage_label(stage: object) -> str:
     return text.replace("_", " ").title()
 
 
-def _gdrive_connection_state(settings: AppSettings) -> dict[str, Any]:
-    sa_path_value = str(settings.gdrive_sa_json_path or "").strip()
-    folder_id_value = str(settings.gdrive_inbox_folder_id or "").strip()
-    return {
-        "configured": bool(sa_path_value and folder_id_value),
-        "sa_path": sa_path_value,
-        "folder_id": folder_id_value,
-    }
-
-
-def _test_gdrive_connection(settings: AppSettings) -> dict[str, Any]:
-    state = _gdrive_connection_state(settings)
-    if not state["configured"]:
-        raise ValueError("Google Drive is not configured.")
-    sa_path = Path(str(state["sa_path"]))
-    folder_id = str(state["folder_id"])
-    service = build_drive_service(sa_path)
-    query = f"'{folder_id}' in parents and trashed=false"
-    response = (
-        service.files()
-        .list(
-            q=query,
-            fields="files(id,name,createdTime)",
-            pageSize=1,
-        )
-        .execute()
-    )
-    rows = response.get("files", []) or []
-    if rows:
-        first = rows[0]
-        name = str(first.get("name") or "").strip() or "(unnamed)"
-        file_id = str(first.get("id") or "").strip() or "unknown-id"
-        return {
-            "ok": True,
-            "message": f"Connected. Sample file: {name} ({file_id}).",
-        }
-    return {
-        "ok": True,
-        "message": "Connected. Inbox is reachable, but no files were found.",
-    }
-
-
 def _load_json_dict(path: Path) -> dict[str, Any]:
     if not path.exists():
         return {}
@@ -1533,35 +1490,6 @@ async def ui_upload(request: Request) -> Any:
     )
 
 
-# ---------------------------------------------------------------------------
-# Connections
-# ---------------------------------------------------------------------------
-
-
-@ui_router.get("/connections", response_class=HTMLResponse)
-async def ui_connections(request: Request) -> Any:
-    gdrive_state = _gdrive_connection_state(_settings)
-    return templates.TemplateResponse(
-        request,
-        "connections.html",
-        {
-            "active": "connections",
-            "gdrive": gdrive_state,
-        },
-    )
-
-
-@ui_router.post("/ui/connections/gdrive/test", response_class=HTMLResponse)
-async def ui_test_gdrive_connection() -> Any:
-    try:
-        result = await run_in_threadpool(_test_gdrive_connection, _settings)
-    except ValueError as exc:
-        return HTMLResponse(f"<span style='color:#92400e'>{exc}</span>")
-    except Exception as exc:
-        return HTMLResponse(f"<span style='color:#b42318'>Google Drive test failed: {exc}</span>")
-    return HTMLResponse(f"<span style='color:#14532d'>{result['message']}</span>")
-
-
 # ---------------------------------------------------------------------------
 # Inline recording actions (HTMX targets returning HX-Redirect)
 # ---------------------------------------------------------------------------
@@ -1746,4 +1674,3 @@ async def ui_retranscribe_language(
     except Exception as exc:
         return HTMLResponse(f"Re-transcribe failed: {exc}", status_code=503)
     return RedirectResponse(f"/recordings/{recording_id}?tab=log", status_code=303)
-
diff --git a/pyproject.toml b/pyproject.toml
index 147b222..7da418c 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -19,8 +19,6 @@ dependencies = [
     "pyyaml",
     "pydantic-settings",
     "rapidfuzz",
-    "google-auth",
-    "google-api-python-client",
 ]
 
 [tool.setuptools.packages.find]
diff --git a/requirements.txt b/requirements.txt
index 7ec8d56..20bd54e 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -15,6 +15,4 @@ pydantic-settings>=2.10,<3.0
 python-multipart>=0.0.9
 prometheus_client>=0.19
 pyyaml>=6.0
-google-auth>=2.38
-google-api-python-client>=2.166
 uvicorn>=0.30
diff --git a/tasks/QUEUE.md b/tasks/QUEUE.md
index f67a598..df29f4c 100644
--- a/tasks/QUEUE.md
+++ b/tasks/QUEUE.md
@@ -157,7 +157,7 @@ Queue (in order)
 - Depends on: PR-UI-PROGRESS-02
 
 30) PR-REMOVE-GDRIVE-01: Remove Google Drive ingest, Connections page, ingest lock, and Google API deps
-- Status: TODO
+- Status: DONE
 - Tasks file: tasks/PR-REMOVE-GDRIVE-01.md
 - Depends on: PR-REMOVE-MS-01
 
diff --git a/tests/test_app_config.py b/tests/test_app_config.py
index 9f0ce65..3c86857 100644
--- a/tests/test_app_config.py
+++ b/tests/test_app_config.py
@@ -47,11 +47,9 @@ def test_routing_threshold_from_env(monkeypatch):
 
 def test_security_settings_from_env(monkeypatch):
     monkeypatch.setenv("LAN_API_BEARER_TOKEN", "secret-token")
-    monkeypatch.setenv("LAN_INGEST_LOCK_TTL_SECONDS", "123")
 
     cfg = AppSettings()
     assert cfg.api_bearer_token == "secret-token"
-    assert cfg.ingest_lock_ttl_seconds == 123
 
 
 def test_worker_and_reaper_settings_from_env(monkeypatch):
diff --git a/tests/test_gdrive.py b/tests/test_gdrive.py
deleted file mode 100644
index 031adaa..0000000
--- a/tests/test_gdrive.py
+++ /dev/null
@@ -1,591 +0,0 @@
-from __future__ import annotations
-
-from pathlib import Path
-from typing import Any
-
-import pytest
-from fastapi.testclient import TestClient
-
-from lan_app import api
-from lan_app.config import AppSettings
-from lan_app.db import get_recording, init_db, list_jobs, list_recordings
-from lan_app.gdrive import (
-    _PIPELINE_STEPS,
-    _known_drive_file_ids,
-    _suffix_from_name,
-    download_file,
-    ingest_once,
-    list_inbox_files,
-    parse_plaud_captured_at,
-)
-from lan_app.jobs import RecordingJob
-
-
-def _test_settings(tmp_path: Path) -> AppSettings:
-    cfg = AppSettings(
-        data_root=tmp_path,
-        recordings_root=tmp_path / "recordings",
-        db_path=tmp_path / "db" / "app.db",
-    )
-    cfg.metrics_snapshot_path = tmp_path / "metrics.snap"
-    cfg.gdrive_sa_json_path = tmp_path / "sa.json"
-    cfg.gdrive_inbox_folder_id = "folder-abc"
-    return cfg
-
-
-def _test_settings_no_gdrive(tmp_path: Path) -> AppSettings:
-    cfg = AppSettings(
-        data_root=tmp_path,
-        recordings_root=tmp_path / "recordings",
-        db_path=tmp_path / "db" / "app.db",
-    )
-    cfg.metrics_snapshot_path = tmp_path / "metrics.snap"
-    return cfg
-
-
-def _stub_enqueue(monkeypatch: Any, cfg: AppSettings) -> None:
-    """Replace enqueue_recording_job with a DB-only stub (no Redis)."""
-    from lan_app.db import create_job, set_recording_status
-    from lan_app.constants import JOB_STATUS_QUEUED, RECORDING_STATUS_QUEUED
-    from uuid import uuid4
-
-    def _fake_enqueue(
-        recording_id: str,
-        *,
-        job_type: str = "precheck",
-        settings: AppSettings | None = None,
-    ) -> RecordingJob:
-        effective = settings or cfg
-        job_id = uuid4().hex
-        create_job(
-            job_id=job_id,
-            recording_id=recording_id,
-            job_type=job_type,
-            status=JOB_STATUS_QUEUED,
-            settings=effective,
-        )
-        set_recording_status(
-            recording_id, RECORDING_STATUS_QUEUED, settings=effective
-        )
-        return RecordingJob(
-            job_id=job_id, recording_id=recording_id, job_type=job_type
-        )
-
-    monkeypatch.setattr("lan_app.jobs.enqueue_recording_job", _fake_enqueue)
-
-
-# --------------------------------------------------------------------------
-# parse_plaud_captured_at
-# --------------------------------------------------------------------------
-
-
-class TestParsePlaudCapturedAt:
-    def test_underscore_separator(self):
-        result = parse_plaud_captured_at("2026-02-18 16_01_43.mp3")
-        assert result == "2026-02-18T16:01:43Z"
-
-    def test_dash_separator(self):
-        result = parse_plaud_captured_at("2026-02-18 16-01-43.mp3")
-        assert result == "2026-02-18T16:01:43Z"
-
-    def test_mixed_separator(self):
-        result = parse_plaud_captured_at("2026-02-18 16_01-43.mp3")
-        assert result == "2026-02-18T16:01:43Z"
-
-    def test_no_match_returns_none(self):
-        assert parse_plaud_captured_at("random_file.mp3") is None
-
-    def test_invalid_date_returns_none(self):
-        assert parse_plaud_captured_at("2026-13-40 99_99_99.mp3") is None
-
-    def test_embedded_in_longer_name(self):
-        result = parse_plaud_captured_at("Plaud_2026-02-18 16_01_43_notes.mp3")
-        assert result == "2026-02-18T16:01:43Z"
-
-
-# --------------------------------------------------------------------------
-# _suffix_from_name
-# --------------------------------------------------------------------------
-
-
-class TestSuffixFromName:
-    def test_mp3(self):
-        assert _suffix_from_name("file.mp3") == ".mp3"
-
-    def test_wav(self):
-        assert _suffix_from_name("recording.wav") == ".wav"
-
-    def test_no_extension_defaults_mp3(self):
-        assert _suffix_from_name("noext") == ".mp3"
-
-    def test_dotfile_defaults_mp3(self):
-        assert _suffix_from_name(".hidden") == ".mp3"
-
-
-# --------------------------------------------------------------------------
-# list_inbox_files
-# --------------------------------------------------------------------------
-
-
-class _FakeFilesResource:
-    """Fake Drive files().list() chain."""
-
-    def __init__(self, pages: list[dict[str, Any]]):
-        self._pages = pages
-        self._call_idx = 0
-
-    def list(self, **kwargs: Any) -> "_FakeFilesResource":
-        return self
-
-    def execute(self) -> dict[str, Any]:
-        page = self._pages[self._call_idx]
-        self._call_idx += 1
-        return page
-
-
-class _FakeService:
-    def __init__(self, pages: list[dict[str, Any]]):
-        self._files = _FakeFilesResource(pages)
-
-    def files(self) -> _FakeFilesResource:
-        return self._files
-
-
-def test_list_inbox_files_single_page():
-    files = [{"id": "f1", "name": "a.mp3"}]
-    svc = _FakeService([{"files": files}])
-    result = list_inbox_files(svc, "folder-abc")
-    assert result == files
-
-
-def test_list_inbox_files_pagination():
-    svc = _FakeService([
-        {"files": [{"id": "f1"}], "nextPageToken": "tok"},
-        {"files": [{"id": "f2"}]},
-    ])
-    result = list_inbox_files(svc, "folder-abc")
-    assert len(result) == 2
-    assert result[0]["id"] == "f1"
-    assert result[1]["id"] == "f2"
-
-
-# --------------------------------------------------------------------------
-# _known_drive_file_ids
-# --------------------------------------------------------------------------
-
-
-def test_known_drive_file_ids(tmp_path: Path):
-    cfg = _test_settings(tmp_path)
-    init_db(cfg)
-    from lan_app.db import create_recording
-
-    create_recording(
-        "rec-1",
-        source="gdrive",
-        source_filename="a.mp3",
-        drive_file_id="drive-1",
-        settings=cfg,
-    )
-    create_recording(
-        "rec-2",
-        source="test",
-        source_filename="b.mp3",
-        settings=cfg,
-    )
-    ids = _known_drive_file_ids(cfg)
-    assert ids == {"drive-1"}
-
-
-# --------------------------------------------------------------------------
-# download_file
-# --------------------------------------------------------------------------
-
-
-class _FakeMediaDownload:
-    """Simulates MediaIoBaseDownload by writing bytes to the file handle."""
-
-    def __init__(self, fh: Any, request: Any):
-        self._fh = fh
-        self._data = request._data
-        self._done = False
-
-    def next_chunk(self) -> tuple[None, bool]:
-        if not self._done:
-            self._fh.write(self._data)
-            self._done = True
-        return None, True
-
-
-class _FakeGetMediaRequest:
-    def __init__(self, data: bytes):
-        self._data = data
-
-
-class _FakeDownloadService:
-    def __init__(self, data: bytes):
-        self._data = data
-
-    def files(self) -> "_FakeDownloadService":
-        return self
-
-    def get_media(self, fileId: str) -> _FakeGetMediaRequest:
-        return _FakeGetMediaRequest(self._data)
-
-
-def test_download_file(tmp_path: Path, monkeypatch):
-    monkeypatch.setattr(
-        "lan_app.gdrive.MediaIoBaseDownload", _FakeMediaDownload
-    )
-    svc = _FakeDownloadService(b"hello audio")
-    dest = tmp_path / "raw" / "audio.mp3"
-    result = download_file(svc, "file-1", dest)
-    assert result == dest
-    assert dest.read_bytes() == b"hello audio"
-
-
-# --------------------------------------------------------------------------
-# ingest_once
-# --------------------------------------------------------------------------
-
-
-def test_ingest_once_raises_when_no_sa_path(tmp_path: Path):
-    cfg = _test_settings_no_gdrive(tmp_path)
-    cfg.gdrive_inbox_folder_id = "folder-abc"
-    with pytest.raises(ValueError, match="GDRIVE_SA_JSON_PATH"):
-        ingest_once(cfg)
-
-
-def test_ingest_once_raises_when_no_folder_id(tmp_path: Path):
-    cfg = _test_settings_no_gdrive(tmp_path)
-    cfg.gdrive_sa_json_path = tmp_path / "sa.json"
-    with pytest.raises(ValueError, match="GDRIVE_INBOX_FOLDER_ID"):
-        ingest_once(cfg)
-
-
-def test_ingest_once_raises_when_empty_folder_id(tmp_path: Path):
-    cfg = _test_settings_no_gdrive(tmp_path)
-    cfg.gdrive_sa_json_path = tmp_path / "sa.json"
-    cfg.gdrive_inbox_folder_id = ""
-    with pytest.raises(ValueError, match="GDRIVE_INBOX_FOLDER_ID"):
-        ingest_once(cfg)
-
-
-def test_ingest_once_raises_when_whitespace_folder_id(tmp_path: Path):
-    cfg = _test_settings_no_gdrive(tmp_path)
-    cfg.gdrive_sa_json_path = tmp_path / "sa.json"
-    cfg.gdrive_inbox_folder_id = "   "
-    with pytest.raises(ValueError, match="GDRIVE_INBOX_FOLDER_ID"):
-        ingest_once(cfg)
-
-
-def test_ingest_once_skips_known_files(tmp_path: Path, monkeypatch):
-    cfg = _test_settings(tmp_path)
-    init_db(cfg)
-    _stub_enqueue(monkeypatch, cfg)
-
-    from lan_app.db import create_recording
-
-    create_recording(
-        "rec-existing",
-        source="gdrive",
-        source_filename="old.mp3",
-        drive_file_id="drive-known",
-        settings=cfg,
-    )
-
-    inbox_files = [
-        {
-            "id": "drive-known",
-            "name": "old.mp3",
-            "md5Checksum": "abc",
-            "createdTime": "2026-01-01T00:00:00Z",
-        },
-    ]
-    svc = _FakeService([{"files": inbox_files}])
-
-    results = ingest_once(cfg, service=svc)
-    assert results == []
-
-
-def test_ingest_once_downloads_and_creates_recording(tmp_path: Path, monkeypatch):
-    cfg = _test_settings(tmp_path)
-    init_db(cfg)
-    _stub_enqueue(monkeypatch, cfg)
-
-    inbox_files = [
-        {
-            "id": "drive-new-1",
-            "name": "2026-02-18 16_01_43.mp3",
-            "md5Checksum": "md5abc",
-            "createdTime": "2026-02-18T16:01:43Z",
-        },
-    ]
-    svc = _FakeService([{"files": inbox_files}])
-
-    monkeypatch.setattr(
-        "lan_app.gdrive.download_file",
-        lambda svc, fid, dest: _write_fake_download(dest),
-    )
-
-    results = ingest_once(cfg, service=svc)
-    assert len(results) == 1
-    result = results[0]
-    assert result["drive_file_id"] == "drive-new-1"
-    assert result["captured_at"] == "2026-02-18T16:01:43Z"
-    assert result["jobs_created"] == len(_PIPELINE_STEPS)
-    assert "warning" not in result
-
-    # Verify DB recording
-    rec = get_recording(result["recording_id"], settings=cfg)
-    assert rec is not None
-    assert rec["source"] == "gdrive"
-    assert rec["drive_file_id"] == "drive-new-1"
-    assert rec["drive_md5"] == "md5abc"
-    assert rec["captured_at"] == "2026-02-18T16:01:43Z"
-
-    # Verify DB jobs
-    jobs, total = list_jobs(settings=cfg, recording_id=result["recording_id"])
-    assert total == len(_PIPELINE_STEPS)
-
-
-def test_ingest_once_fallback_captured_at_from_drive(tmp_path: Path, monkeypatch):
-    cfg = _test_settings(tmp_path)
-    init_db(cfg)
-    _stub_enqueue(monkeypatch, cfg)
-
-    inbox_files = [
-        {
-            "id": "drive-noparse",
-            "name": "meeting_notes.mp3",
-            "md5Checksum": "md5xyz",
-            "createdTime": "2026-03-01T10:00:00Z",
-        },
-    ]
-    svc = _FakeService([{"files": inbox_files}])
-    monkeypatch.setattr(
-        "lan_app.gdrive.download_file",
-        lambda svc, fid, dest: _write_fake_download(dest),
-    )
-
-    results = ingest_once(cfg, service=svc)
-    assert len(results) == 1
-    assert results[0]["captured_at"] == "2026-03-01T10:00:00Z"
-    assert "warning" in results[0]
-
-
-def test_ingest_once_continues_on_download_failure(tmp_path: Path, monkeypatch):
-    cfg = _test_settings(tmp_path)
-    init_db(cfg)
-    _stub_enqueue(monkeypatch, cfg)
-
-    inbox_files = [
-        {
-            "id": "drive-fail",
-            "name": "2026-02-18 10_00_00.mp3",
-            "createdTime": "2026-02-18T10:00:00Z",
-        },
-        {
-            "id": "drive-ok",
-            "name": "2026-02-18 11_00_00.mp3",
-            "createdTime": "2026-02-18T11:00:00Z",
-        },
-    ]
-    svc = _FakeService([{"files": inbox_files}])
-
-    def _download_maybe_fail(svc: Any, fid: str, dest: Path) -> Path:
-        if fid == "drive-fail":
-            # Simulate a partial write before failure
-            dest.parent.mkdir(parents=True, exist_ok=True)
-            dest.write_bytes(b"partial")
-            raise OSError("network error")
-        return _write_fake_download(dest)
-
-    monkeypatch.setattr("lan_app.gdrive.download_file", _download_maybe_fail)
-
-    results = ingest_once(cfg, service=svc)
-    assert len(results) == 1
-    assert results[0]["drive_file_id"] == "drive-ok"
-
-
-def test_ingest_once_cleans_up_partial_files_on_download_failure(
-    tmp_path: Path, monkeypatch
-):
-    """Failed downloads should not leave orphaned recording directories."""
-    cfg = _test_settings(tmp_path)
-    init_db(cfg)
-    _stub_enqueue(monkeypatch, cfg)
-
-    inbox_files = [
-        {
-            "id": "drive-partial",
-            "name": "2026-02-18 10_00_00.mp3",
-            "createdTime": "2026-02-18T10:00:00Z",
-        },
-    ]
-    svc = _FakeService([{"files": inbox_files}])
-
-    created_dirs: list[Path] = []
-
-    def _download_fail(svc: Any, fid: str, dest: Path) -> Path:
-        dest.parent.mkdir(parents=True, exist_ok=True)
-        dest.write_bytes(b"partial data")
-        created_dirs.append(dest.parent.parent)  # recordings/<id>
-        raise OSError("network error")
-
-    monkeypatch.setattr("lan_app.gdrive.download_file", _download_fail)
-
-    results = ingest_once(cfg, service=svc)
-    assert results == []
-    assert len(created_dirs) == 1
-    # The orphaned directory should have been cleaned up
-    assert not created_dirs[0].exists()
-
-
-def test_ingest_idempotent(tmp_path: Path, monkeypatch):
-    """Running ingest twice with the same files should not duplicate."""
-    cfg = _test_settings(tmp_path)
-    init_db(cfg)
-    _stub_enqueue(monkeypatch, cfg)
-
-    inbox_files = [
-        {
-            "id": "drive-idem",
-            "name": "2026-02-18 16_01_43.mp3",
-            "md5Checksum": "md5idem",
-            "createdTime": "2026-02-18T16:01:43Z",
-        },
-    ]
-    svc = _FakeService([{"files": inbox_files}])
-    monkeypatch.setattr(
-        "lan_app.gdrive.download_file",
-        lambda svc, fid, dest: _write_fake_download(dest),
-    )
-
-    first = ingest_once(cfg, service=svc)
-    assert len(first) == 1
-
-    # Second call: same file already known
-    svc2 = _FakeService([{"files": inbox_files}])
-    second = ingest_once(cfg, service=svc2)
-    assert len(second) == 0
-
-    # Only 1 recording in DB
-    recs, total = list_recordings(settings=cfg)
-    assert total == 1
-
-
-# --------------------------------------------------------------------------
-# API endpoint
-# --------------------------------------------------------------------------
-
-
-def test_api_ingest_endpoint_returns_422_when_not_configured(
-    tmp_path: Path, monkeypatch
-):
-    cfg = _test_settings_no_gdrive(tmp_path)
-    monkeypatch.setattr(api, "_settings", cfg)
-    monkeypatch.setattr(
-        api,
-        "try_acquire_ingest_lock",
-        lambda *_args, **_kwargs: (True, 300, "lock-token"),
-    )
-    monkeypatch.setattr(api, "release_ingest_lock", lambda *_args, **_kwargs: None)
-    init_db(cfg)
-    client = TestClient(api.app)
-    resp = client.post("/api/actions/ingest")
-    assert resp.status_code == 422
-
-
-def test_api_ingest_endpoint_success(tmp_path: Path, monkeypatch):
-    cfg = _test_settings(tmp_path)
-    monkeypatch.setattr(api, "_settings", cfg)
-    monkeypatch.setattr(
-        api,
-        "try_acquire_ingest_lock",
-        lambda *_args, **_kwargs: (True, 300, "lock-token"),
-    )
-    monkeypatch.setattr(api, "release_ingest_lock", lambda *_args, **_kwargs: None)
-    init_db(cfg)
-
-    def _fake_ingest(settings: Any = None, *, service: Any = None) -> list[dict]:
-        return [{"recording_id": "trs_test1", "drive_file_id": "f1"}]
-
-    monkeypatch.setattr("lan_app.gdrive.ingest_once", _fake_ingest)
-
-    client = TestClient(api.app)
-    resp = client.post("/api/actions/ingest")
-    assert resp.status_code == 200
-    data = resp.json()
-    assert data["count"] == 1
-    assert data["ingested"][0]["recording_id"] == "trs_test1"
-
-
-def test_api_ingest_requires_auth_when_token_configured(tmp_path: Path, monkeypatch):
-    cfg = _test_settings(tmp_path)
-    cfg.api_bearer_token = "top-secret"
-    monkeypatch.setattr(api, "_settings", cfg)
-    init_db(cfg)
-
-    client = TestClient(api.app)
-    resp = client.post("/api/actions/ingest")
-    assert resp.status_code == 401
-
-
-def test_api_ingest_with_auth_succeeds_when_lock_acquired(tmp_path: Path, monkeypatch):
-    cfg = _test_settings(tmp_path)
-    cfg.api_bearer_token = "top-secret"
-    monkeypatch.setattr(api, "_settings", cfg)
-    init_db(cfg)
-
-    monkeypatch.setattr(
-        api,
-        "try_acquire_ingest_lock",
-        lambda *_args, **_kwargs: (True, 300, "lock-token"),
-    )
-    monkeypatch.setattr(api, "release_ingest_lock", lambda *_args, **_kwargs: None)
-
-    def _fake_ingest(settings: Any = None, *, service: Any = None) -> list[dict]:
-        return [{"recording_id": "trs_auth_1"}]
-
-    monkeypatch.setattr("lan_app.gdrive.ingest_once", _fake_ingest)
-
-    client = TestClient(api.app)
-    resp = client.post(
-        "/api/actions/ingest",
-        headers={"Authorization": "Bearer top-secret"},
-    )
-    assert resp.status_code == 200
-    assert resp.json()["count"] == 1
-
-
-def test_api_ingest_with_auth_returns_409_when_lock_held(tmp_path: Path, monkeypatch):
-    cfg = _test_settings(tmp_path)
-    cfg.api_bearer_token = "top-secret"
-    monkeypatch.setattr(api, "_settings", cfg)
-    init_db(cfg)
-
-    monkeypatch.setattr(
-        api,
-        "try_acquire_ingest_lock",
-        lambda *_args, **_kwargs: (False, 19, None),
-    )
-
-    client = TestClient(api.app)
-    resp = client.post(
-        "/api/actions/ingest",
-        headers={"Authorization": "Bearer top-secret"},
-    )
-    assert resp.status_code == 409
-    payload = resp.json()
-    assert payload["retry_after_seconds"] == 19
-
-
-# --------------------------------------------------------------------------
-# Helpers
-# --------------------------------------------------------------------------
-
-
-def _write_fake_download(dest: Path) -> Path:
-    dest.parent.mkdir(parents=True, exist_ok=True)
-    dest.write_bytes(b"fake audio data")
-    return dest
diff --git a/tests/test_locks.py b/tests/test_locks.py
deleted file mode 100644
index 340059f..0000000
--- a/tests/test_locks.py
+++ /dev/null
@@ -1,101 +0,0 @@
-from __future__ import annotations
-
-from pathlib import Path
-
-from lan_app.config import AppSettings
-from lan_app.locks import release_ingest_lock, try_acquire_ingest_lock
-
-
-class _FakeRedis:
-    def __init__(self):
-        self._store: dict[str, tuple[str, int]] = {}
-
-    def set(self, key: str, value: str, *, nx: bool, ex: int) -> bool:
-        if nx and key in self._store:
-            return False
-        self._store[key] = (str(value), int(ex))
-        return True
-
-    def ttl(self, key: str) -> int:
-        row = self._store.get(key)
-        if row is None:
-            return -2
-        return row[1]
-
-    def get(self, key: str) -> str | None:
-        row = self._store.get(key)
-        if row is None:
-            return None
-        return row[0]
-
-    def delete(self, key: str) -> int:
-        return int(self._store.pop(key, None) is not None)
-
-    def eval(self, _script: str, _numkeys: int, key: str, token: str) -> int:
-        current = self.get(key)
-        if current == token:
-            self.delete(key)
-            return 1
-        return 0
-
-
-def _cfg(tmp_path: Path) -> AppSettings:
-    cfg = AppSettings(
-        data_root=tmp_path,
-        recordings_root=tmp_path / "recordings",
-        db_path=tmp_path / "db" / "app.db",
-    )
-    cfg.ingest_lock_ttl_seconds = 123
-    return cfg
-
-
-def test_try_acquire_ingest_lock_returns_true_when_free(tmp_path: Path):
-    cfg = _cfg(tmp_path)
-    redis = _FakeRedis()
-
-    acquired, retry_after, token = try_acquire_ingest_lock(cfg, redis_client=redis)
-
-    assert acquired is True
-    assert retry_after == 123
-    assert token is not None
-
-
-def test_try_acquire_ingest_lock_returns_retry_after_when_held(tmp_path: Path):
-    cfg = _cfg(tmp_path)
-    redis = _FakeRedis()
-    redis.set("lan:ingest:lock", "owned", nx=True, ex=31)
-
-    acquired, retry_after, token = try_acquire_ingest_lock(cfg, redis_client=redis)
-
-    assert acquired is False
-    assert retry_after == 31
-    assert token is None
-
-
-def test_release_ingest_lock_deletes_key(tmp_path: Path):
-    cfg = _cfg(tmp_path)
-    redis = _FakeRedis()
-    acquired, _retry_after, token = try_acquire_ingest_lock(cfg, redis_client=redis)
-    assert acquired is True
-    assert token is not None
-
-    released = release_ingest_lock(cfg, token=token, redis_client=redis)
-    assert released is True
-
-    acquired, retry_after, _token = try_acquire_ingest_lock(cfg, redis_client=redis)
-    assert acquired is True
-    assert retry_after == 123
-
-
-def test_release_ingest_lock_does_not_delete_new_owner_lock(tmp_path: Path):
-    cfg = _cfg(tmp_path)
-    redis = _FakeRedis()
-    acquired, _retry_after, token = try_acquire_ingest_lock(cfg, redis_client=redis)
-    assert acquired is True
-    assert token is not None
-    redis._store["lan:ingest:lock"] = ("other-owner", 31)
-
-    released = release_ingest_lock(cfg, token=token, redis_client=redis)
-
-    assert released is False
-    assert redis.get("lan:ingest:lock") == "other-owner"
diff --git a/tests/test_ui_routes.py b/tests/test_ui_routes.py
index fefabea..d47f26c 100644
--- a/tests/test_ui_routes.py
+++ b/tests/test_ui_routes.py
@@ -891,68 +891,6 @@ def test_upload_page(client):
     assert 'id="file-input"' in r.text
 
 
-# ---------------------------------------------------------------------------
-# Connections
-# ---------------------------------------------------------------------------
-
-
-def test_connections(client):
-    r = client.get("/connections")
-    assert r.status_code == 200
-    assert "Google Drive" in r.text
-
-
-def test_connections_shows_configured_gdrive(tmp_path, monkeypatch):
-    cfg = _cfg(tmp_path)
-    cfg.gdrive_sa_json_path = cfg.data_root / "secrets" / "gdrive_sa.json"
-    cfg.gdrive_inbox_folder_id = "folder-abc"
-    monkeypatch.setattr(api, "_settings", cfg)
-    monkeypatch.setattr(ui_routes, "_settings", cfg)
-    init_db(cfg)
-
-    client = TestClient(api.app, follow_redirects=True)
-    resp = client.get("/connections")
-    assert resp.status_code == 200
-    assert "Configured" in resp.text
-    assert "folder-abc" in resp.text
-    assert "Test connection" in resp.text
-    assert "Run ingest now" in resp.text
-
-
-def test_ui_test_gdrive_connection_endpoint_success(tmp_path, monkeypatch):
-    cfg = _cfg(tmp_path)
-    monkeypatch.setattr(api, "_settings", cfg)
-    monkeypatch.setattr(ui_routes, "_settings", cfg)
-    init_db(cfg)
-    monkeypatch.setattr(
-        ui_routes,
-        "_test_gdrive_connection",
-        lambda _settings: {"ok": True, "message": "Connected. Sample file: demo.mp3 (id-1)."},
-    )
-
-    client = TestClient(api.app, follow_redirects=True)
-    resp = client.post("/ui/connections/gdrive/test")
-    assert resp.status_code == 200
-    assert "Connected. Sample file: demo.mp3 (id-1)." in resp.text
-
-
-def test_ui_test_gdrive_connection_endpoint_not_configured(tmp_path, monkeypatch):
-    cfg = _cfg(tmp_path)
-    monkeypatch.setattr(api, "_settings", cfg)
-    monkeypatch.setattr(ui_routes, "_settings", cfg)
-    init_db(cfg)
-
-    def _boom(_settings):
-        raise ValueError("Google Drive is not configured.")
-
-    monkeypatch.setattr(ui_routes, "_test_gdrive_connection", _boom)
-
-    client = TestClient(api.app, follow_redirects=True)
-    resp = client.post("/ui/connections/gdrive/test")
-    assert resp.status_code == 200
-    assert "Google Drive is not configured." in resp.text
-
-
 def test_ui_root_redirects_to_login_when_auth_enabled(tmp_path, monkeypatch):
     cfg = _cfg(tmp_path)
     cfg.api_bearer_token = "secret-ui-token"
