diff --git a/lan_transcriber/pipeline.py b/lan_transcriber/pipeline.py
index 6932635..5910e0f 100644
--- a/lan_transcriber/pipeline.py
+++ b/lan_transcriber/pipeline.py
@@ -973,7 +973,7 @@ async def run_pipeline(
             unknown_chunks=snippet_paths,
             segments=serialised_segments,
         )
-    except Exception:
+    except Exception as exc:
         error_rate_total.inc()
         atomic_write_json(
             artifact_paths.summary_json_path,
@@ -984,6 +984,23 @@ async def run_pipeline(
                 "status": "failed",
             },
         )
+        atomic_write_json(
+            artifact_paths.metrics_json_path,
+            {
+                "status": "failed",
+                "version": 1,
+                "precheck": {
+                    "duration_sec": precheck_result.duration_sec,
+                    "speech_ratio": precheck_result.speech_ratio,
+                    "quarantine_reason": None,
+                },
+                "language": language_info,
+                "asr_segments": len(asr_segments),
+                "diar_segments": len(diar_segments),
+                "speaker_turns": len(speaker_turns),
+                "error": str(exc) or exc.__class__.__name__,
+            },
+        )
         raise
     finally:
         p95_latency_seconds.observe(time.perf_counter() - start)
diff --git a/tests/test_pipeline.py b/tests/test_pipeline.py
index 5a5c8c5..4709f03 100644
--- a/tests/test_pipeline.py
+++ b/tests/test_pipeline.py
@@ -570,6 +570,55 @@ async def test_pipeline_no_speech_clears_stale_snippets(tmp_path: Path, mocker):
     assert list(snippets_root.iterdir()) == []
 
 
+@pytest.mark.asyncio
+async def test_pipeline_error_marks_metrics_failed(tmp_path: Path, mocker):
+    mocker.patch(
+        "whisperx.transcribe",
+        return_value=(
+            [{"start": 0.0, "end": 1.0, "text": "hello world today"}],
+            {"language": "en", "language_probability": 0.9},
+        ),
+    )
+    mocker.patch(
+        "transformers.pipeline",
+        lambda *a, **k: lambda text: [{"label": "positive", "score": 0.7}],
+    )
+
+    class _FailingLLM:
+        async def generate(self, **_kwargs):
+            raise RuntimeError("llm boom")
+
+    cfg = pipeline.Settings(
+        speaker_db=tmp_path / "db.yaml",
+        tmp_root=tmp_path,
+        recordings_root=tmp_path / "recordings",
+    )
+    audio = wav_audio(
+        tmp_path,
+        name="llm-fail.wav",
+        duration_sec=24.0,
+        speech=True,
+    )
+
+    with pytest.raises(RuntimeError, match="llm boom"):
+        await pipeline.run_pipeline(
+            audio_path=audio,
+            cfg=cfg,
+            llm=_FailingLLM(),
+            diariser=DummyDiariser(),
+            recording_id="rec-llm-fail-1",
+            precheck=precheck_ok(),
+        )
+
+    derived = cfg.recordings_root / "rec-llm-fail-1" / "derived"
+    summary_data = json.loads((derived / "summary.json").read_text(encoding="utf-8"))
+    metrics_data = json.loads((derived / "metrics.json").read_text(encoding="utf-8"))
+
+    assert summary_data["status"] == "failed"
+    assert metrics_data["status"] == "failed"
+    assert metrics_data["error"] == "llm boom"
+
+
 def test_run_precheck_quarantine_rules(tmp_path: Path):
     cfg = pipeline.Settings(
         speaker_db=tmp_path / "db.yaml",
