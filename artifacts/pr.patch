diff --git a/lan_app/api.py b/lan_app/api.py
index 5f0f368..d7877e5 100644
--- a/lan_app/api.py
+++ b/lan_app/api.py
@@ -63,6 +63,7 @@ from .ms_graph import (
 )
 from .onenote import PublishPreconditionError, publish_recording_to_onenote
 from .ops import run_retention_cleanup
+from .reaper import run_stuck_job_reaper_once
 from .ui_routes import _STATIC_DIR, ui_router
 
 app = FastAPI()
@@ -73,6 +74,7 @@ _subscribers: List[asyncio.Queue[str]] = []
 _current_result: TranscriptResult | None = None
 _settings = AppSettings()
 _cleanup_task: asyncio.Task[None] | None = None
+_reaper_task: asyncio.Task[None] | None = None
 _CLEANUP_INTERVAL_SECONDS = 3600
 _logger = logging.getLogger(__name__)
 
@@ -174,6 +176,25 @@ async def _retention_cleanup_loop() -> None:
         await asyncio.sleep(_CLEANUP_INTERVAL_SECONDS)
 
 
+async def _stuck_job_reaper_loop() -> None:
+    while True:
+        try:
+            summary = await run_in_threadpool(
+                run_stuck_job_reaper_once,
+                settings=_settings,
+            )
+            recovered_jobs = int(summary.get("recovered_jobs") or 0)
+            if recovered_jobs > 0:
+                _logger.warning(
+                    "Recovered %s stuck jobs for %s recordings",
+                    recovered_jobs,
+                    int(summary.get("recovered_recordings") or 0),
+                )
+        except Exception:
+            _logger.exception("Stuck-job reaper failed")
+        await asyncio.sleep(_settings.reaper_interval_seconds)
+
+
 @app.get("/api/connections/ms/verify")
 async def api_verify_ms_connection() -> dict[str, object]:
     """Validate Microsoft Graph auth by calling /me via cached delegated token."""
@@ -224,22 +245,29 @@ async def api_get_ms_connection_status(session_id: str) -> dict[str, object]:
 
 @app.on_event("startup")
 async def _start_metrics() -> None:
-    global _cleanup_task
+    global _cleanup_task, _reaper_task
     init_db(_settings)
     _settings.metrics_snapshot_path.parent.mkdir(parents=True, exist_ok=True)
     asyncio.create_task(write_metrics_snapshot(_settings.metrics_snapshot_path))
     _cleanup_task = asyncio.create_task(_retention_cleanup_loop())
+    _reaper_task = asyncio.create_task(_stuck_job_reaper_loop())
 
 
 @app.on_event("shutdown")
 async def _stop_background_tasks() -> None:
-    global _cleanup_task
-    if _cleanup_task is None:
-        return
-    _cleanup_task.cancel()
-    with suppress(asyncio.CancelledError):
-        await _cleanup_task
-    _cleanup_task = None
+    global _cleanup_task, _reaper_task
+
+    if _cleanup_task is not None:
+        _cleanup_task.cancel()
+        with suppress(asyncio.CancelledError):
+            await _cleanup_task
+        _cleanup_task = None
+
+    if _reaper_task is not None:
+        _reaper_task.cancel()
+        with suppress(asyncio.CancelledError):
+            await _reaper_task
+        _reaper_task = None
 
 
 @app.post("/alias/{speaker_id}")
diff --git a/lan_app/config.py b/lan_app/config.py
index 2dc5664..cae96b1 100644
--- a/lan_app/config.py
+++ b/lan_app/config.py
@@ -43,6 +43,38 @@ class AppSettings(BaseSettings):
         default=False,
         validation_alias=AliasChoices("LAN_RQ_WORKER_BURST", "RQ_WORKER_BURST"),
     )
+    rq_job_timeout_seconds: int = Field(
+        default=7200,
+        ge=1,
+        validation_alias=AliasChoices(
+            "LAN_RQ_JOB_TIMEOUT_SECONDS",
+            "RQ_JOB_TIMEOUT_SECONDS",
+        ),
+    )
+    max_job_attempts: int = Field(
+        default=3,
+        ge=1,
+        validation_alias=AliasChoices(
+            "LAN_MAX_JOB_ATTEMPTS",
+            "MAX_JOB_ATTEMPTS",
+        ),
+    )
+    stuck_job_seconds: int = Field(
+        default=7200,
+        ge=1,
+        validation_alias=AliasChoices(
+            "LAN_STUCK_JOB_SECONDS",
+            "STUCK_JOB_SECONDS",
+        ),
+    )
+    reaper_interval_seconds: int = Field(
+        default=300,
+        ge=1,
+        validation_alias=AliasChoices(
+            "LAN_REAPER_INTERVAL_SECONDS",
+            "REAPER_INTERVAL_SECONDS",
+        ),
+    )
 
     # Google Drive ingest
     gdrive_sa_json_path: Path | None = Field(
diff --git a/lan_app/db.py b/lan_app/db.py
index 64d8f2d..988c1a6 100644
--- a/lan_app/db.py
+++ b/lan_app/db.py
@@ -15,6 +15,7 @@ from .constants import (
     JOB_STATUS_STARTED,
     JOB_TYPES,
     RECORDING_STATUSES,
+    RECORDING_STATUS_PROCESSING,
     RECORDING_STATUS_PUBLISHED,
     RECORDING_STATUS_QUEUED,
     RECORDING_STATUS_QUARANTINE,
@@ -739,6 +740,97 @@ def list_jobs(
     return [_as_dict(row) or {} for row in rows], total
 
 
+def list_stale_started_jobs(
+    *,
+    before_started_at: str,
+    settings: AppSettings | None = None,
+    limit: int = 500,
+) -> list[dict[str, Any]]:
+    init_db(settings)
+    safe_limit = max(1, min(limit, 500))
+    with connect(settings) as conn:
+        rows = conn.execute(
+            """
+            SELECT
+                j.*,
+                r.status AS recording_status
+            FROM jobs AS j
+            JOIN recordings AS r ON r.id = j.recording_id
+            WHERE j.status = ?
+              AND j.started_at IS NOT NULL
+              AND j.started_at < ?
+            ORDER BY j.started_at ASC, j.created_at ASC
+            LIMIT ?
+            """,
+            (JOB_STATUS_STARTED, before_started_at, safe_limit),
+        ).fetchall()
+    return [_as_dict(row) or {} for row in rows]
+
+
+def list_processing_recordings_without_started_job(
+    *,
+    settings: AppSettings | None = None,
+    limit: int = 500,
+) -> list[dict[str, Any]]:
+    init_db(settings)
+    safe_limit = max(1, min(limit, 500))
+    with connect(settings) as conn:
+        rows = conn.execute(
+            """
+            SELECT
+                r.*,
+                p.name AS project_name,
+                sp.name AS suggested_project_name,
+            (
+                SELECT j.id
+                FROM jobs AS j
+                WHERE j.recording_id = r.id
+                  AND j.status IN (?, ?)
+                ORDER BY
+                    CASE WHEN j.status = ? THEN 0 ELSE 1 END ASC,
+                    j.updated_at DESC,
+                    j.created_at DESC
+                LIMIT 1
+            ) AS active_job_id,
+            (
+                SELECT j.type
+                FROM jobs AS j
+                WHERE j.recording_id = r.id
+                  AND j.status IN (?, ?)
+                ORDER BY
+                    CASE WHEN j.status = ? THEN 0 ELSE 1 END ASC,
+                    j.updated_at DESC,
+                    j.created_at DESC
+                LIMIT 1
+            ) AS active_job_type
+            FROM recordings AS r
+            LEFT JOIN projects AS p ON p.id = r.project_id
+            LEFT JOIN projects AS sp ON sp.id = r.suggested_project_id
+            WHERE r.status = ?
+              AND NOT EXISTS (
+                    SELECT 1
+                    FROM jobs AS sj
+                    WHERE sj.recording_id = r.id
+                      AND sj.status = ?
+              )
+            ORDER BY r.updated_at ASC, r.created_at ASC
+            LIMIT ?
+            """,
+            (
+                JOB_STATUS_STARTED,
+                JOB_STATUS_QUEUED,
+                JOB_STATUS_STARTED,
+                JOB_STATUS_STARTED,
+                JOB_STATUS_QUEUED,
+                JOB_STATUS_STARTED,
+                RECORDING_STATUS_PROCESSING,
+                JOB_STATUS_STARTED,
+                safe_limit,
+            ),
+        ).fetchall()
+    return [_as_dict(row) or {} for row in rows]
+
+
 def _find_active_job_for_recording_row(
     conn: sqlite3.Connection,
     *,
@@ -1625,6 +1717,8 @@ __all__ = [
     "create_job",
     "get_job",
     "list_jobs",
+    "list_stale_started_jobs",
+    "list_processing_recordings_without_started_job",
     "find_active_job_for_recording",
     "create_job_if_no_active_for_recording",
     "start_job",
diff --git a/lan_app/jobs.py b/lan_app/jobs.py
index ae645f2..7739837 100644
--- a/lan_app/jobs.py
+++ b/lan_app/jobs.py
@@ -165,6 +165,7 @@ def enqueue_recording_job(
             recording_id,
             job_type,
             job_id=job_id,
+            job_timeout=cfg.rq_job_timeout_seconds,
         )
     except Exception as exc:
         # Keep DB queue state terminal when Redis/RQ enqueue fails.
diff --git a/lan_app/templates/recording_detail.html b/lan_app/templates/recording_detail.html
index c32e7c0..66b6592 100644
--- a/lan_app/templates/recording_detail.html
+++ b/lan_app/templates/recording_detail.html
@@ -16,6 +16,12 @@
   <a href="/recordings" class="btn" style="text-decoration:none">&#8592; Back</a>
 </div>
 
+{% if recovery_warning %}
+<p style="margin:0 0 10px 0;padding:8px 10px;border:1px solid #f59e0b;background:#fffbeb;color:#92400e">
+  {{ recovery_warning }}
+</p>
+{% endif %}
+
 <div class="tabs">
   {% for t in tabs %}
   <a href="/recordings/{{ rec.id }}?tab={{ t }}" class="tab {% if current_tab == t %}active{% endif %}">{{ t|capitalize }}</a>
diff --git a/lan_app/ui_routes.py b/lan_app/ui_routes.py
index 514a3d9..4ced40f 100644
--- a/lan_app/ui_routes.py
+++ b/lan_app/ui_routes.py
@@ -128,6 +128,7 @@ _LANGUAGE_CODE_MAP: dict[str, str] = {
 }
 
 _COMMON_LANGUAGE_CODES = ("en", "es", "fr", "de", "pt", "it", "zh", "ja", "ko", "ru")
+_STUCK_JOB_RECOVERY_ERROR = "stuck job recovered"
 
 
 def _normalise_language_code(value: object | None) -> str | None:
@@ -162,6 +163,21 @@ def _parse_language_form_value(value: str, *, field_name: str) -> str | None:
     return parsed
 
 
+def _recording_recovery_warning(jobs: list[dict[str, Any]]) -> str | None:
+    for job in jobs:
+        error = str(job.get("error") or "").strip().lower()
+        if error != _STUCK_JOB_RECOVERY_ERROR:
+            continue
+        finished_at = str(job.get("finished_at") or "").strip()
+        if finished_at:
+            return (
+                "Warning: this recording was recovered from a stuck job at "
+                f"{finished_at[:19].replace('T', ' ')} UTC."
+            )
+        return "Warning: this recording was recovered from a stuck job."
+    return None
+
+
 def _load_json_dict(path: Path) -> dict[str, Any]:
     if not path.exists():
         return {}
@@ -1068,6 +1084,7 @@ async def ui_recording_detail(
         return HTMLResponse("<h1>404 â€“ Recording not found</h1>", status_code=404)
     onenote_page_url = _recording_onenote_page_url(rec)
     jobs, _ = list_jobs(settings=_settings, recording_id=recording_id, limit=100)
+    recovery_warning = _recording_recovery_warning(jobs)
     tabs = ["overview", "calendar", "project", "speakers", "language", "metrics", "log"]
     current_tab = tab if tab in tabs else "overview"
     calendar: dict[str, Any] | None = None
@@ -1109,6 +1126,7 @@ async def ui_recording_detail(
             "active": "recordings",
             "rec": rec,
             "jobs": jobs,
+            "recovery_warning": recovery_warning,
             "tabs": tabs,
             "current_tab": current_tab,
             "calendar": calendar,
diff --git a/lan_app/worker.py b/lan_app/worker.py
index 9f7be66..d994a04 100644
--- a/lan_app/worker.py
+++ b/lan_app/worker.py
@@ -1,17 +1,40 @@
 from __future__ import annotations
 
+import logging
+import signal
+from types import FrameType
+
 from redis import Redis
 from rq import Worker
 
 from .config import AppSettings
 from .db import init_db
 
+_logger = logging.getLogger(__name__)
+
+
+def _install_signal_handlers(worker: Worker) -> None:
+    def _request_shutdown(signum: int, frame: FrameType | None) -> None:
+        try:
+            signal_name = signal.Signals(signum).name
+        except ValueError:
+            signal_name = str(signum)
+        _logger.info(
+            "Shutdown requested via %s; worker will stop after current job",
+            signal_name,
+        )
+        worker.request_stop(signum, frame)
+
+    signal.signal(signal.SIGTERM, _request_shutdown)
+    signal.signal(signal.SIGINT, _request_shutdown)
+
 
 def main() -> None:
     settings = AppSettings()
     init_db(settings)
     connection = Redis.from_url(settings.redis_url)
     worker = Worker([settings.rq_queue_name], connection=connection)
+    _install_signal_handlers(worker)
     worker.work(with_scheduler=False, burst=settings.rq_worker_burst)
 
 
diff --git a/lan_app/worker_tasks.py b/lan_app/worker_tasks.py
index dd524a0..87da3d5 100644
--- a/lan_app/worker_tasks.py
+++ b/lan_app/worker_tasks.py
@@ -149,6 +149,7 @@ _JOB_RETRY_POLICIES: dict[str, RetryPolicy] = {
     JOB_TYPE_PUBLISH: RetryPolicy(max_attempts=2, backoff_seconds=(3,)),
     JOB_TYPE_CLEANUP: RetryPolicy(max_attempts=2, backoff_seconds=(5,)),
 }
+_MAX_ATTEMPTS_ERROR = "max attempts exceeded"
 
 
 def _retry_policy(job_type: str) -> RetryPolicy:
@@ -238,6 +239,38 @@ def _record_failure(
         pass
 
 
+def _record_max_attempts_exceeded(
+    *,
+    job_id: str,
+    job_type: str,
+    recording_id: str,
+    settings: AppSettings,
+    log_path: Path,
+) -> None:
+    try:
+        fail_job(job_id, _MAX_ATTEMPTS_ERROR, settings=settings)
+    except Exception:
+        pass
+    try:
+        set_recording_status(
+            recording_id,
+            RECORDING_STATUS_NEEDS_REVIEW,
+            settings=settings,
+        )
+    except Exception:
+        pass
+    try:
+        _append_step_log(
+            log_path,
+            (
+                f"terminal failure job={job_id} type={job_type}: "
+                f"{_MAX_ATTEMPTS_ERROR}"
+            ),
+        )
+    except Exception:
+        pass
+
+
 def _resolve_raw_audio_path(recording_id: str, settings: AppSettings) -> Path | None:
     raw_dir = settings.recordings_root / recording_id / "raw"
     candidates = sorted(raw_dir.glob("audio.*"))
@@ -543,11 +576,15 @@ def process_job(job_id: str, recording_id: str, job_type: str) -> dict[str, str]
         }
 
     retry_policy = _retry_policy(job_type)
+    max_attempts = max(1, min(retry_policy.max_attempts, settings.max_job_attempts))
 
     while True:
         try:
             if not start_job(job_id, settings=settings):
                 raise ValueError(f"Job not found: {job_id}")
+            attempt = _job_attempt(job_id, settings)
+            if attempt > settings.max_job_attempts:
+                raise RuntimeError(_MAX_ATTEMPTS_ERROR)
             if not set_recording_status(
                 recording_id,
                 RECORDING_STATUS_PROCESSING,
@@ -578,10 +615,19 @@ def process_job(job_id: str, recording_id: str, job_type: str) -> dict[str, str]
             break
         except Exception as exc:
             attempt = _job_attempt(job_id, settings)
+            if attempt >= settings.max_job_attempts:
+                _record_max_attempts_exceeded(
+                    job_id=job_id,
+                    job_type=job_type,
+                    recording_id=recording_id,
+                    settings=settings,
+                    log_path=log_path,
+                )
+                raise
             if (
                 _is_retryable_exception(exc)
                 and attempt > 0
-                and attempt < retry_policy.max_attempts
+                and attempt < max_attempts
             ):
                 delay_seconds = _retry_delay_seconds(retry_policy, attempt)
                 _record_retry(
@@ -589,7 +635,7 @@ def process_job(job_id: str, recording_id: str, job_type: str) -> dict[str, str]
                     job_type=job_type,
                     recording_id=recording_id,
                     attempt=attempt,
-                    max_attempts=retry_policy.max_attempts,
+                    max_attempts=max_attempts,
                     delay_seconds=delay_seconds,
                     settings=settings,
                     log_path=log_path,
diff --git a/tasks/QUEUE.md b/tasks/QUEUE.md
index 748cca1..2cb4621 100644
--- a/tasks/QUEUE.md
+++ b/tasks/QUEUE.md
@@ -107,7 +107,7 @@ Queue (in order)
 - Depends on: PR-STAGING-01
 
 20) PR-WORKER-ROBUST-01: Worker robustness: graceful shutdown, timeouts, stuck job recovery, terminal failures
-- Status: TODO
+- Status: DONE
 - Tasks file: tasks/PR-WORKER-ROBUST-01.md
 - Depends on: PR-SECURITY-01
 
diff --git a/tests/test_app_config.py b/tests/test_app_config.py
index b7615b0..80c44d9 100644
--- a/tests/test_app_config.py
+++ b/tests/test_app_config.py
@@ -74,3 +74,16 @@ def test_security_settings_from_env(monkeypatch):
     cfg = AppSettings()
     assert cfg.api_bearer_token == "secret-token"
     assert cfg.ingest_lock_ttl_seconds == 123
+
+
+def test_worker_and_reaper_settings_from_env(monkeypatch):
+    monkeypatch.setenv("LAN_RQ_JOB_TIMEOUT_SECONDS", "1800")
+    monkeypatch.setenv("LAN_MAX_JOB_ATTEMPTS", "5")
+    monkeypatch.setenv("LAN_STUCK_JOB_SECONDS", "900")
+    monkeypatch.setenv("LAN_REAPER_INTERVAL_SECONDS", "60")
+
+    cfg = AppSettings()
+    assert cfg.rq_job_timeout_seconds == 1800
+    assert cfg.max_job_attempts == 5
+    assert cfg.stuck_job_seconds == 900
+    assert cfg.reaper_interval_seconds == 60
diff --git a/tests/test_db_queue.py b/tests/test_db_queue.py
index a6ca6ff..f09dc31 100644
--- a/tests/test_db_queue.py
+++ b/tests/test_db_queue.py
@@ -24,6 +24,7 @@ from lan_app.constants import (
     JOB_TYPE_PRECHECK,
     JOB_TYPE_STT,
     RECORDING_STATUS_FAILED,
+    RECORDING_STATUS_NEEDS_REVIEW,
     RECORDING_STATUS_QUARANTINE,
     RECORDING_STATUS_QUEUED,
     RECORDING_STATUS_READY,
@@ -697,7 +698,56 @@ def test_worker_retryable_failure_retries_before_marking_failed(
     assert recording is not None
     assert job["attempt"] == 3
     assert job["status"] == JOB_STATUS_FAILED
-    assert recording["status"] == RECORDING_STATUS_FAILED
+    assert job["error"] == "max attempts exceeded"
+    assert recording["status"] == RECORDING_STATUS_NEEDS_REVIEW
+
+
+def test_worker_max_attempts_exceeded_before_processing_sets_terminal_state(
+    tmp_path: Path,
+    monkeypatch,
+):
+    cfg = _test_settings(tmp_path)
+    monkeypatch.setenv("LAN_DATA_ROOT", str(cfg.data_root))
+    monkeypatch.setenv("LAN_RECORDINGS_ROOT", str(cfg.recordings_root))
+    monkeypatch.setenv("LAN_DB_PATH", str(cfg.db_path))
+    monkeypatch.setenv("LAN_PROM_SNAPSHOT_PATH", str(cfg.metrics_snapshot_path))
+    monkeypatch.setenv("LAN_MAX_JOB_ATTEMPTS", "3")
+
+    init_db(cfg)
+    create_recording(
+        "rec-worker-max-attempts-1",
+        source="test",
+        source_filename="max-attempts.wav",
+        settings=cfg,
+    )
+    create_job(
+        "job-worker-max-attempts-1",
+        recording_id="rec-worker-max-attempts-1",
+        job_type=JOB_TYPE_PRECHECK,
+        settings=cfg,
+        attempt=3,
+    )
+
+    def _should_not_run_pipeline(*_args, **_kwargs):
+        raise AssertionError("pipeline execution should not run after max attempts")
+
+    monkeypatch.setattr("lan_app.worker_tasks._run_precheck_pipeline", _should_not_run_pipeline)
+
+    with pytest.raises(RuntimeError, match="max attempts exceeded"):
+        process_job(
+            "job-worker-max-attempts-1",
+            "rec-worker-max-attempts-1",
+            JOB_TYPE_PRECHECK,
+        )
+
+    job = get_job("job-worker-max-attempts-1", settings=cfg)
+    recording = get_recording("rec-worker-max-attempts-1", settings=cfg)
+    assert job is not None
+    assert recording is not None
+    assert job["attempt"] == 4
+    assert job["status"] == JOB_STATUS_FAILED
+    assert job["error"] == "max attempts exceeded"
+    assert recording["status"] == RECORDING_STATUS_NEEDS_REVIEW
 
 
 def test_enqueue_sets_recording_status_to_queued_on_success(tmp_path: Path, monkeypatch):
@@ -727,6 +777,35 @@ def test_enqueue_sets_recording_status_to_queued_on_success(tmp_path: Path, monk
     assert recording["status"] == RECORDING_STATUS_QUEUED
 
 
+def test_enqueue_uses_configured_rq_job_timeout(tmp_path: Path, monkeypatch):
+    cfg = _test_settings(tmp_path)
+    cfg.rq_job_timeout_seconds = 321
+    init_db(cfg)
+    create_recording(
+        "rec-rq-timeout-1",
+        source="test",
+        source_filename="timeout.mp3",
+        settings=cfg,
+    )
+
+    captured: dict[str, object] = {}
+
+    class _QueueCapture:
+        def enqueue(self, *_args, **kwargs):
+            captured.update(kwargs)
+            return None
+
+    monkeypatch.setattr("lan_app.jobs.get_queue", lambda _cfg: _QueueCapture())
+
+    enqueue_recording_job(
+        "rec-rq-timeout-1",
+        job_type=JOB_TYPE_PRECHECK,
+        settings=cfg,
+    )
+
+    assert captured["job_timeout"] == 321
+
+
 def test_purge_pending_recording_jobs_deletes_only_pending(tmp_path: Path, monkeypatch):
     cfg = _test_settings(tmp_path)
     init_db(cfg)
diff --git a/tests/test_ui_routes.py b/tests/test_ui_routes.py
index d659949..625ca7b 100644
--- a/tests/test_ui_routes.py
+++ b/tests/test_ui_routes.py
@@ -197,6 +197,33 @@ def test_recording_detail_project_tab(seeded_client):
     assert "Routing Rationale" in r.text
 
 
+def test_recording_detail_shows_stuck_recovery_warning(tmp_path, monkeypatch):
+    cfg = _cfg(tmp_path)
+    monkeypatch.setattr(api, "_settings", cfg)
+    monkeypatch.setattr(ui_routes, "_settings", cfg)
+    init_db(cfg)
+    create_recording(
+        "rec-ui-stuck-1",
+        source="drive",
+        source_filename="stuck.mp3",
+        status=RECORDING_STATUS_NEEDS_REVIEW,
+        settings=cfg,
+    )
+    create_job(
+        "job-ui-stuck-1",
+        recording_id="rec-ui-stuck-1",
+        job_type=JOB_TYPE_PRECHECK,
+        status=JOB_STATUS_FAILED,
+        error="stuck job recovered",
+        settings=cfg,
+    )
+
+    c = TestClient(api.app, follow_redirects=True)
+    r = c.get("/recordings/rec-ui-stuck-1")
+    assert r.status_code == 200
+    assert "recovered from a stuck job" in r.text
+
+
 def test_recording_detail_project_assignment_trains_routing(tmp_path, monkeypatch):
     cfg = _cfg(tmp_path)
     monkeypatch.setattr(api, "_settings", cfg)
diff --git a/lan_app/reaper.py b/lan_app/reaper.py
new file mode 100644
index 0000000..0a7351f
--- /dev/null
+++ b/lan_app/reaper.py
@@ -0,0 +1,106 @@
+from __future__ import annotations
+
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+from typing import Any
+
+from .config import AppSettings
+from .constants import DEFAULT_REQUEUE_JOB_TYPE, RECORDING_STATUS_NEEDS_REVIEW
+from .db import (
+    fail_job,
+    list_processing_recordings_without_started_job,
+    list_stale_started_jobs,
+    set_recording_status,
+)
+
+_RECOVERY_ERROR = "stuck job recovered"
+
+
+def _utc_now() -> datetime:
+    return datetime.now(tz=timezone.utc).replace(microsecond=0)
+
+
+def _iso_z(value: datetime) -> str:
+    return value.astimezone(timezone.utc).replace(microsecond=0).isoformat().replace(
+        "+00:00",
+        "Z",
+    )
+
+
+def _step_log_path(recording_id: str, job_type: str, settings: AppSettings) -> Path:
+    return settings.recordings_root / recording_id / "logs" / f"step-{job_type}.log"
+
+
+def _append_step_log(path: Path, message: str, *, now: datetime) -> None:
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with path.open("a", encoding="utf-8") as fh:
+        fh.write(f"[{_iso_z(now)}] {message}\n")
+
+
+def run_stuck_job_reaper_once(
+    *,
+    settings: AppSettings | None = None,
+    now: datetime | None = None,
+) -> dict[str, Any]:
+    cfg = settings or AppSettings()
+    current_time = now or _utc_now()
+    threshold = max(int(cfg.stuck_job_seconds), 1)
+    stale_before = _iso_z(current_time - timedelta(seconds=threshold))
+
+    recovered_job_ids: list[str] = []
+    recovered_recording_ids: set[str] = set()
+    stale_rows = list_stale_started_jobs(
+        before_started_at=stale_before,
+        settings=cfg,
+    )
+    for row in stale_rows:
+        job_id = str(row.get("id") or "").strip()
+        recording_id = str(row.get("recording_id") or "").strip()
+        job_type = str(row.get("type") or "").strip() or DEFAULT_REQUEUE_JOB_TYPE
+        if not job_id or not recording_id:
+            continue
+        fail_job(job_id, _RECOVERY_ERROR, settings=cfg)
+        set_recording_status(recording_id, RECORDING_STATUS_NEEDS_REVIEW, settings=cfg)
+        _append_step_log(
+            _step_log_path(recording_id, job_type, cfg),
+            f"stuck job recovery applied job={job_id}",
+            now=current_time,
+        )
+        recovered_job_ids.append(job_id)
+        recovered_recording_ids.add(recording_id)
+
+    processing_rows = list_processing_recordings_without_started_job(settings=cfg)
+    for row in processing_rows:
+        recording_id = str(row.get("id") or "").strip()
+        if not recording_id:
+            continue
+        active_job_id = str(row.get("active_job_id") or "").strip()
+        active_job_type = (
+            str(row.get("active_job_type") or "").strip() or DEFAULT_REQUEUE_JOB_TYPE
+        )
+        if active_job_id:
+            fail_job(active_job_id, _RECOVERY_ERROR, settings=cfg)
+            recovered_job_ids.append(active_job_id)
+        set_recording_status(recording_id, RECORDING_STATUS_NEEDS_REVIEW, settings=cfg)
+        _append_step_log(
+            _step_log_path(recording_id, active_job_type, cfg),
+            (
+                "stuck job recovery applied "
+                f"recording={recording_id} "
+                f"job={active_job_id or 'none'}"
+            ),
+            now=current_time,
+        )
+        recovered_recording_ids.add(recording_id)
+
+    return {
+        "stale_started_jobs": len(stale_rows),
+        "processing_without_started": len(processing_rows),
+        "recovered_jobs": len(recovered_job_ids),
+        "recovered_recordings": len(recovered_recording_ids),
+        "job_ids": recovered_job_ids,
+        "recording_ids": sorted(recovered_recording_ids),
+    }
+
+
+__all__ = ["run_stuck_job_reaper_once"]
diff --git a/tests/test_reaper.py b/tests/test_reaper.py
new file mode 100644
index 0000000..1996ffd
--- /dev/null
+++ b/tests/test_reaper.py
@@ -0,0 +1,114 @@
+from __future__ import annotations
+
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+
+from lan_app.config import AppSettings
+from lan_app.constants import (
+    JOB_STATUS_FAILED,
+    JOB_STATUS_QUEUED,
+    JOB_STATUS_STARTED,
+    JOB_TYPE_PRECHECK,
+    RECORDING_STATUS_NEEDS_REVIEW,
+    RECORDING_STATUS_PROCESSING,
+)
+from lan_app.db import connect, create_job, create_recording, get_job, get_recording, init_db
+from lan_app.reaper import run_stuck_job_reaper_once
+
+
+def _test_settings(tmp_path: Path) -> AppSettings:
+    cfg = AppSettings(
+        data_root=tmp_path,
+        recordings_root=tmp_path / "recordings",
+        db_path=tmp_path / "db" / "app.db",
+    )
+    cfg.metrics_snapshot_path = tmp_path / "metrics.snap"
+    cfg.stuck_job_seconds = 60
+    return cfg
+
+
+def test_reaper_recovers_stale_started_job(tmp_path: Path):
+    cfg = _test_settings(tmp_path)
+    init_db(cfg)
+    create_recording(
+        "rec-reaper-stale-1",
+        source="test",
+        source_filename="stale.wav",
+        status=RECORDING_STATUS_PROCESSING,
+        settings=cfg,
+    )
+    create_job(
+        "job-reaper-stale-1",
+        recording_id="rec-reaper-stale-1",
+        job_type=JOB_TYPE_PRECHECK,
+        status=JOB_STATUS_STARTED,
+        settings=cfg,
+        attempt=1,
+    )
+    with connect(cfg) as conn:
+        conn.execute(
+            """
+            UPDATE jobs
+            SET started_at = ?, updated_at = ?
+            WHERE id = ?
+            """,
+            ("2026-02-22T00:00:00Z", "2026-02-22T00:00:00Z", "job-reaper-stale-1"),
+        )
+        conn.commit()
+
+    summary = run_stuck_job_reaper_once(
+        settings=cfg,
+        now=datetime(2026, 2, 23, 0, 0, 0, tzinfo=timezone.utc),
+    )
+
+    job = get_job("job-reaper-stale-1", settings=cfg)
+    recording = get_recording("rec-reaper-stale-1", settings=cfg)
+    assert job is not None
+    assert recording is not None
+    assert summary["stale_started_jobs"] == 1
+    assert summary["recovered_jobs"] == 1
+    assert job["status"] == JOB_STATUS_FAILED
+    assert job["error"] == "stuck job recovered"
+    assert recording["status"] == RECORDING_STATUS_NEEDS_REVIEW
+
+    step_log = cfg.recordings_root / "rec-reaper-stale-1" / "logs" / "step-precheck.log"
+    assert step_log.exists()
+    assert "stuck job recovery applied" in step_log.read_text(encoding="utf-8")
+
+
+def test_reaper_recovers_processing_recording_without_started_job(tmp_path: Path):
+    cfg = _test_settings(tmp_path)
+    init_db(cfg)
+    create_recording(
+        "rec-reaper-orphan-1",
+        source="test",
+        source_filename="orphan.wav",
+        status=RECORDING_STATUS_PROCESSING,
+        settings=cfg,
+    )
+    create_job(
+        "job-reaper-orphan-1",
+        recording_id="rec-reaper-orphan-1",
+        job_type=JOB_TYPE_PRECHECK,
+        status=JOB_STATUS_QUEUED,
+        settings=cfg,
+    )
+
+    summary = run_stuck_job_reaper_once(
+        settings=cfg,
+        now=datetime(2026, 2, 23, 0, 0, 0, tzinfo=timezone.utc) + timedelta(seconds=1),
+    )
+
+    job = get_job("job-reaper-orphan-1", settings=cfg)
+    recording = get_recording("rec-reaper-orphan-1", settings=cfg)
+    assert job is not None
+    assert recording is not None
+    assert summary["processing_without_started"] == 1
+    assert summary["recovered_jobs"] == 1
+    assert job["status"] == JOB_STATUS_FAILED
+    assert job["error"] == "stuck job recovered"
+    assert recording["status"] == RECORDING_STATUS_NEEDS_REVIEW
+
+    step_log = cfg.recordings_root / "rec-reaper-orphan-1" / "logs" / "step-precheck.log"
+    assert step_log.exists()
+    assert "stuck job recovery applied" in step_log.read_text(encoding="utf-8")
