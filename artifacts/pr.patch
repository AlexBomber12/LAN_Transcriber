diff --git a/lan_app/api.py b/lan_app/api.py
index 0e8b659..5f0f368 100644
--- a/lan_app/api.py
+++ b/lan_app/api.py
@@ -479,7 +479,7 @@ async def api_ingest_once() -> dict[str, object]:
     from .gdrive import ingest_once
 
     try:
-        acquired, retry_after = try_acquire_ingest_lock(_settings)
+        acquired, retry_after, lock_token = try_acquire_ingest_lock(_settings)
     except Exception as exc:
         raise HTTPException(status_code=503, detail=f"Ingest lock unavailable: {exc}")
 
@@ -499,8 +499,9 @@ async def api_ingest_once() -> dict[str, object]:
     except Exception as exc:
         raise HTTPException(status_code=503, detail=f"Ingest failed: {exc}")
     finally:
-        with suppress(Exception):
-            release_ingest_lock(_settings)
+        if lock_token is not None:
+            with suppress(Exception):
+                release_ingest_lock(_settings, token=lock_token)
     return {"ingested": results, "count": len(results)}
 
 
diff --git a/lan_app/db.py b/lan_app/db.py
index 4ae84f0..64d8f2d 100644
--- a/lan_app/db.py
+++ b/lan_app/db.py
@@ -739,6 +739,29 @@ def list_jobs(
     return [_as_dict(row) or {} for row in rows], total
 
 
+def _find_active_job_for_recording_row(
+    conn: sqlite3.Connection,
+    *,
+    recording_id: str,
+    job_type: str,
+) -> sqlite3.Row | None:
+    return conn.execute(
+        """
+        SELECT *
+        FROM jobs
+        WHERE recording_id = ? AND type = ? AND status IN (?, ?)
+        ORDER BY created_at ASC
+        LIMIT 1
+        """,
+        (
+            recording_id,
+            job_type,
+            JOB_STATUS_QUEUED,
+            JOB_STATUS_STARTED,
+        ),
+    ).fetchone()
+
+
 def find_active_job_for_recording(
     recording_id: str,
     *,
@@ -748,22 +771,63 @@ def find_active_job_for_recording(
     init_db(settings)
     _validate_job_type(job_type)
     with connect(settings) as conn:
-        row = conn.execute(
+        row = _find_active_job_for_recording_row(
+            conn,
+            recording_id=recording_id,
+            job_type=job_type,
+        )
+    return _as_dict(row)
+
+
+def create_job_if_no_active_for_recording(
+    *,
+    job_id: str,
+    recording_id: str,
+    job_type: str,
+    settings: AppSettings | None = None,
+    status: str = JOB_STATUS_QUEUED,
+    attempt: int = 0,
+    error: str | None = None,
+) -> tuple[dict[str, Any] | None, dict[str, Any] | None]:
+    init_db(settings)
+    _validate_job_type(job_type)
+    _validate_job_status(status)
+    now = _utc_now()
+    with connect(settings) as conn:
+        conn.execute("BEGIN IMMEDIATE")
+        existing = _find_active_job_for_recording_row(
+            conn,
+            recording_id=recording_id,
+            job_type=job_type,
+        )
+        if existing is not None:
+            conn.rollback()
+            return None, _as_dict(existing)
+
+        conn.execute(
             """
-            SELECT *
-            FROM jobs
-            WHERE recording_id = ? AND type = ? AND status IN (?, ?)
-            ORDER BY created_at ASC
-            LIMIT 1
+            INSERT INTO jobs (
+                id, recording_id, type, status, attempt, error,
+                started_at, finished_at, created_at, updated_at
+            )
+            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
             """,
             (
+                job_id,
                 recording_id,
                 job_type,
-                JOB_STATUS_QUEUED,
-                JOB_STATUS_STARTED,
+                status,
+                attempt,
+                error,
+                None,
+                None,
+                now,
+                now,
             ),
-        ).fetchone()
-    return _as_dict(row)
+        )
+        created = conn.execute("SELECT * FROM jobs WHERE id = ?", (job_id,)).fetchone()
+        conn.commit()
+    return _as_dict(created) or {}, None
 
 
 def start_job(
@@ -1562,6 +1626,7 @@ __all__ = [
     "get_job",
     "list_jobs",
     "find_active_job_for_recording",
+    "create_job_if_no_active_for_recording",
     "start_job",
     "requeue_job",
     "finish_job",
diff --git a/lan_app/jobs.py b/lan_app/jobs.py
index efb7536..ae645f2 100644
--- a/lan_app/jobs.py
+++ b/lan_app/jobs.py
@@ -16,9 +16,9 @@ from .constants import (
     RECORDING_STATUS_QUEUED,
 )
 from .db import (
+    create_job_if_no_active_for_recording,
     create_job,
     fail_job,
-    find_active_job_for_recording,
     get_recording,
     init_db,
     list_jobs,
@@ -130,11 +130,14 @@ def enqueue_recording_job(
     _validate_job_type(job_type)
     if get_recording(recording_id, settings=cfg) is None:
         raise RecordingNotFoundError(f"Recording not found: {recording_id}")
+    job_id = uuid4().hex
     if job_type == DEFAULT_REQUEUE_JOB_TYPE:
-        existing = find_active_job_for_recording(
-            recording_id,
+        _created, existing = create_job_if_no_active_for_recording(
+            job_id=job_id,
+            recording_id=recording_id,
             job_type=job_type,
             settings=cfg,
+            status=JOB_STATUS_QUEUED,
         )
         if existing is not None:
             existing_job_id = str(existing.get("id") or "").strip()
@@ -143,15 +146,14 @@ def enqueue_recording_job(
                     recording_id=recording_id,
                     job_id=existing_job_id,
                 )
-
-    job_id = uuid4().hex
-    create_job(
-        job_id=job_id,
-        recording_id=recording_id,
-        job_type=job_type,
-        status=JOB_STATUS_QUEUED,
-        settings=cfg,
-    )
+    else:
+        create_job(
+            job_id=job_id,
+            recording_id=recording_id,
+            job_type=job_type,
+            status=JOB_STATUS_QUEUED,
+            settings=cfg,
+        )
 
     from .worker_tasks import process_job
 
diff --git a/lan_app/locks.py b/lan_app/locks.py
index 7cce1d0..d296870 100644
--- a/lan_app/locks.py
+++ b/lan_app/locks.py
@@ -1,10 +1,18 @@
 from __future__ import annotations
 
+from uuid import uuid4
+
 from redis import Redis
 
 from .config import AppSettings
 
 INGEST_LOCK_KEY = "lan:ingest:lock"
+_RELEASE_IF_VALUE_MATCHES_SCRIPT = """
+if redis.call("GET", KEYS[1]) == ARGV[1] then
+    return redis.call("DEL", KEYS[1])
+end
+return 0
+"""
 
 
 def _ingest_lock_ttl(settings: AppSettings) -> int:
@@ -20,27 +28,32 @@ def try_acquire_ingest_lock(
     *,
     redis_client: Redis | None = None,
     key: str = INGEST_LOCK_KEY,
-) -> tuple[bool, int]:
+) -> tuple[bool, int, str | None]:
     client = redis_client or _redis_client(settings)
     ttl_seconds = _ingest_lock_ttl(settings)
-    acquired = bool(client.set(key, "1", nx=True, ex=ttl_seconds))
+    lock_token = uuid4().hex
+    acquired = bool(client.set(key, lock_token, nx=True, ex=ttl_seconds))
     if acquired:
-        return True, ttl_seconds
+        return True, ttl_seconds, lock_token
 
     retry_after = client.ttl(key)
     if retry_after is None or int(retry_after) <= 0:
-        return False, ttl_seconds
-    return False, int(retry_after)
+        return False, ttl_seconds, None
+    return False, int(retry_after), None
 
 
 def release_ingest_lock(
     settings: AppSettings,
     *,
+    token: str | None,
     redis_client: Redis | None = None,
     key: str = INGEST_LOCK_KEY,
-) -> None:
+) -> bool:
+    if token is None:
+        return False
     client = redis_client or _redis_client(settings)
-    client.delete(key)
+    result = client.eval(_RELEASE_IF_VALUE_MATCHES_SCRIPT, 1, key, token)
+    return int(result) == 1
 
 
 __all__ = [
diff --git a/tests/test_gdrive.py b/tests/test_gdrive.py
index 4db8b69..031adaa 100644
--- a/tests/test_gdrive.py
+++ b/tests/test_gdrive.py
@@ -484,7 +484,11 @@ def test_api_ingest_endpoint_returns_422_when_not_configured(
 ):
     cfg = _test_settings_no_gdrive(tmp_path)
     monkeypatch.setattr(api, "_settings", cfg)
-    monkeypatch.setattr(api, "try_acquire_ingest_lock", lambda *_args, **_kwargs: (True, 300))
+    monkeypatch.setattr(
+        api,
+        "try_acquire_ingest_lock",
+        lambda *_args, **_kwargs: (True, 300, "lock-token"),
+    )
     monkeypatch.setattr(api, "release_ingest_lock", lambda *_args, **_kwargs: None)
     init_db(cfg)
     client = TestClient(api.app)
@@ -495,7 +499,11 @@ def test_api_ingest_endpoint_returns_422_when_not_configured(
 def test_api_ingest_endpoint_success(tmp_path: Path, monkeypatch):
     cfg = _test_settings(tmp_path)
     monkeypatch.setattr(api, "_settings", cfg)
-    monkeypatch.setattr(api, "try_acquire_ingest_lock", lambda *_args, **_kwargs: (True, 300))
+    monkeypatch.setattr(
+        api,
+        "try_acquire_ingest_lock",
+        lambda *_args, **_kwargs: (True, 300, "lock-token"),
+    )
     monkeypatch.setattr(api, "release_ingest_lock", lambda *_args, **_kwargs: None)
     init_db(cfg)
 
@@ -529,7 +537,11 @@ def test_api_ingest_with_auth_succeeds_when_lock_acquired(tmp_path: Path, monkey
     monkeypatch.setattr(api, "_settings", cfg)
     init_db(cfg)
 
-    monkeypatch.setattr(api, "try_acquire_ingest_lock", lambda *_args, **_kwargs: (True, 300))
+    monkeypatch.setattr(
+        api,
+        "try_acquire_ingest_lock",
+        lambda *_args, **_kwargs: (True, 300, "lock-token"),
+    )
     monkeypatch.setattr(api, "release_ingest_lock", lambda *_args, **_kwargs: None)
 
     def _fake_ingest(settings: Any = None, *, service: Any = None) -> list[dict]:
@@ -552,7 +564,11 @@ def test_api_ingest_with_auth_returns_409_when_lock_held(tmp_path: Path, monkeyp
     monkeypatch.setattr(api, "_settings", cfg)
     init_db(cfg)
 
-    monkeypatch.setattr(api, "try_acquire_ingest_lock", lambda *_args, **_kwargs: (False, 19))
+    monkeypatch.setattr(
+        api,
+        "try_acquire_ingest_lock",
+        lambda *_args, **_kwargs: (False, 19, None),
+    )
 
     client = TestClient(api.app)
     resp = client.post(
diff --git a/tests/test_locks.py b/tests/test_locks.py
index 0ba3660..340059f 100644
--- a/tests/test_locks.py
+++ b/tests/test_locks.py
@@ -8,20 +8,36 @@ from lan_app.locks import release_ingest_lock, try_acquire_ingest_lock
 
 class _FakeRedis:
     def __init__(self):
-        self._store: dict[str, int] = {}
+        self._store: dict[str, tuple[str, int]] = {}
 
-    def set(self, key: str, _value: str, *, nx: bool, ex: int) -> bool:
+    def set(self, key: str, value: str, *, nx: bool, ex: int) -> bool:
         if nx and key in self._store:
             return False
-        self._store[key] = ex
+        self._store[key] = (str(value), int(ex))
         return True
 
     def ttl(self, key: str) -> int:
-        return self._store.get(key, -2)
+        row = self._store.get(key)
+        if row is None:
+            return -2
+        return row[1]
+
+    def get(self, key: str) -> str | None:
+        row = self._store.get(key)
+        if row is None:
+            return None
+        return row[0]
 
     def delete(self, key: str) -> int:
         return int(self._store.pop(key, None) is not None)
 
+    def eval(self, _script: str, _numkeys: int, key: str, token: str) -> int:
+        current = self.get(key)
+        if current == token:
+            self.delete(key)
+            return 1
+        return 0
+
 
 def _cfg(tmp_path: Path) -> AppSettings:
     cfg = AppSettings(
@@ -37,30 +53,49 @@ def test_try_acquire_ingest_lock_returns_true_when_free(tmp_path: Path):
     cfg = _cfg(tmp_path)
     redis = _FakeRedis()
 
-    acquired, retry_after = try_acquire_ingest_lock(cfg, redis_client=redis)
+    acquired, retry_after, token = try_acquire_ingest_lock(cfg, redis_client=redis)
 
     assert acquired is True
     assert retry_after == 123
+    assert token is not None
 
 
 def test_try_acquire_ingest_lock_returns_retry_after_when_held(tmp_path: Path):
     cfg = _cfg(tmp_path)
     redis = _FakeRedis()
-    redis.set("lan:ingest:lock", "1", nx=True, ex=31)
+    redis.set("lan:ingest:lock", "owned", nx=True, ex=31)
 
-    acquired, retry_after = try_acquire_ingest_lock(cfg, redis_client=redis)
+    acquired, retry_after, token = try_acquire_ingest_lock(cfg, redis_client=redis)
 
     assert acquired is False
     assert retry_after == 31
+    assert token is None
 
 
 def test_release_ingest_lock_deletes_key(tmp_path: Path):
     cfg = _cfg(tmp_path)
     redis = _FakeRedis()
-    redis.set("lan:ingest:lock", "1", nx=True, ex=31)
+    acquired, _retry_after, token = try_acquire_ingest_lock(cfg, redis_client=redis)
+    assert acquired is True
+    assert token is not None
 
-    release_ingest_lock(cfg, redis_client=redis)
+    released = release_ingest_lock(cfg, token=token, redis_client=redis)
+    assert released is True
 
-    acquired, retry_after = try_acquire_ingest_lock(cfg, redis_client=redis)
+    acquired, retry_after, _token = try_acquire_ingest_lock(cfg, redis_client=redis)
     assert acquired is True
     assert retry_after == 123
+
+
+def test_release_ingest_lock_does_not_delete_new_owner_lock(tmp_path: Path):
+    cfg = _cfg(tmp_path)
+    redis = _FakeRedis()
+    acquired, _retry_after, token = try_acquire_ingest_lock(cfg, redis_client=redis)
+    assert acquired is True
+    assert token is not None
+    redis._store["lan:ingest:lock"] = ("other-owner", 31)
+
+    released = release_ingest_lock(cfg, token=token, redis_client=redis)
+
+    assert released is False
+    assert redis.get("lan:ingest:lock") == "other-owner"
