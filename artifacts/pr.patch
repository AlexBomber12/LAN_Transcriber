diff --git a/lan_transcriber/pipeline.py b/lan_transcriber/pipeline.py
index acba096..20accea 100644
--- a/lan_transcriber/pipeline.py
+++ b/lan_transcriber/pipeline.py
@@ -182,10 +182,7 @@ def _safe_diarization_segments(diarization: Any) -> list[dict[str, Any]]:
     out: list[dict[str, Any]] = []
     if diarization is None or not hasattr(diarization, "itertracks"):
         return out
-    try:
-        tracks = diarization.itertracks(yield_label=True)
-    except Exception:
-        return out
+    tracks = diarization.itertracks(yield_label=True)
     for item in tracks:
         if not isinstance(item, tuple) or len(item) < 2:
             continue
diff --git a/tests/test_pipeline.py b/tests/test_pipeline.py
index 702a38a..962db07 100644
--- a/tests/test_pipeline.py
+++ b/tests/test_pipeline.py
@@ -119,6 +119,15 @@ class FailingDiariser:
         raise RuntimeError("diariser boom")
 
 
+class FailingItertracksDiariser:
+    async def __call__(self, audio_path: Path):
+        class Ann:
+            def itertracks(self, yield_label: bool = False):
+                raise RuntimeError("itertracks boom")
+
+        return Ann()
+
+
 @pytest.mark.asyncio
 @respx.mock
 async def test_tripled_dedup(tmp_path: Path, mocker):
@@ -700,6 +709,47 @@ async def test_pipeline_diariser_error_marks_metrics_failed(tmp_path: Path, mock
     assert metrics_data["error"] == "diariser boom"
 
 
+@pytest.mark.asyncio
+async def test_pipeline_itertracks_error_marks_metrics_failed(tmp_path: Path, mocker):
+    mocker.patch(
+        "whisperx.transcribe",
+        return_value=(
+            [{"start": 0.0, "end": 1.0, "text": "hello world today"}],
+            {"language": "en", "language_probability": 0.9},
+        ),
+    )
+
+    cfg = pipeline.Settings(
+        speaker_db=tmp_path / "db.yaml",
+        tmp_root=tmp_path,
+        recordings_root=tmp_path / "recordings",
+    )
+    audio = wav_audio(
+        tmp_path,
+        name="itertracks-fail.wav",
+        duration_sec=24.0,
+        speech=True,
+    )
+
+    with pytest.raises(RuntimeError, match="itertracks boom"):
+        await pipeline.run_pipeline(
+            audio_path=audio,
+            cfg=cfg,
+            llm=llm_client.LLMClient(),
+            diariser=FailingItertracksDiariser(),
+            recording_id="rec-itertracks-fail-1",
+            precheck=precheck_ok(),
+        )
+
+    derived = cfg.recordings_root / "rec-itertracks-fail-1" / "derived"
+    summary_data = json.loads((derived / "summary.json").read_text(encoding="utf-8"))
+    metrics_data = json.loads((derived / "metrics.json").read_text(encoding="utf-8"))
+
+    assert summary_data["status"] == "failed"
+    assert metrics_data["status"] == "failed"
+    assert metrics_data["error"] == "itertracks boom"
+
+
 def test_run_precheck_quarantine_rules(tmp_path: Path):
     cfg = pipeline.Settings(
         speaker_db=tmp_path / "db.yaml",
