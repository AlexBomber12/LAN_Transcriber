diff --git a/lan_app/db.py b/lan_app/db.py
index 9f86d7e..c1e2ffc 100644
--- a/lan_app/db.py
+++ b/lan_app/db.py
@@ -139,6 +139,30 @@ def _read_user_version(conn: sqlite3.Connection) -> int:
     return int(conn.execute("PRAGMA user_version").fetchone()[0])
 
 
+def _executescript_allowing_duplicate_columns(
+    conn: sqlite3.Connection,
+    migration_sql: str,
+) -> None:
+    try:
+        conn.executescript(migration_sql)
+    except sqlite3.OperationalError as error:
+        if "duplicate column name" not in str(error).strip().lower():
+            raise
+        for statement in migration_sql.split(";"):
+            sql = statement.strip()
+            if not sql:
+                continue
+            try:
+                conn.execute(sql)
+            except sqlite3.OperationalError as statement_error:
+                if (
+                    "duplicate column name"
+                    in str(statement_error).strip().lower()
+                ):
+                    continue
+                raise
+
+
 def connect_db(
     path: Path,
     *,
@@ -190,7 +214,7 @@ def init_db(settings: AppSettings | None = None) -> Path:
                 live_version = _read_user_version(conn)
                 if live_version >= target_version:
                     return live_version
-                conn.executescript(migration_sql)
+                _executescript_allowing_duplicate_columns(conn, migration_sql)
                 conn.execute(f"PRAGMA user_version = {target_version}")
                 conn.commit()
                 return target_version
@@ -320,6 +344,77 @@ def list_recordings(
     return [_as_dict(row) or {} for row in rows], total
 
 
+def set_recording_progress(
+    recording_id: str,
+    stage: str,
+    progress: float,
+    *,
+    settings: AppSettings | None = None,
+) -> bool:
+    init_db(settings)
+    stage_name = str(stage).strip()
+    if not stage_name:
+        raise ValueError("stage is required")
+    progress_value = max(0.0, min(float(progress), 1.0))
+    now = _utc_now()
+
+    def _update() -> bool:
+        with connect(settings) as conn:
+            updated = conn.execute(
+                """
+                UPDATE recordings
+                SET
+                    pipeline_stage = ?,
+                    pipeline_progress = ?,
+                    pipeline_updated_at = ?,
+                    updated_at = ?
+                WHERE id = ?
+                """,
+                (
+                    stage_name,
+                    progress_value,
+                    now,
+                    now,
+                    recording_id,
+                ),
+            )
+            conn.commit()
+            return updated.rowcount > 0
+
+    return with_db_retry(_update)
+
+
+def clear_recording_progress(
+    recording_id: str,
+    *,
+    settings: AppSettings | None = None,
+) -> bool:
+    init_db(settings)
+    now = _utc_now()
+
+    def _clear() -> bool:
+        with connect(settings) as conn:
+            updated = conn.execute(
+                """
+                UPDATE recordings
+                SET
+                    pipeline_stage = NULL,
+                    pipeline_progress = NULL,
+                    pipeline_updated_at = NULL,
+                    updated_at = ?
+                WHERE id = ?
+                """,
+                (
+                    now,
+                    recording_id,
+                ),
+            )
+            conn.commit()
+            return updated.rowcount > 0
+
+    return with_db_retry(_clear)
+
+
 def set_recording_status(
     recording_id: str,
     status: str,
@@ -1859,6 +1954,8 @@ __all__ = [
     "create_recording",
     "get_recording",
     "list_recordings",
+    "set_recording_progress",
+    "clear_recording_progress",
     "set_recording_status",
     "set_recording_status_if_current_in",
     "set_recording_status_if_current_in_and_no_started_job",
diff --git a/lan_app/templates/connections.html b/lan_app/templates/connections.html
index 3a39615..97cf29c 100644
--- a/lan_app/templates/connections.html
+++ b/lan_app/templates/connections.html
@@ -5,16 +5,86 @@
 
 <div class="conn-card">
   <h3>Google Drive (Service Account)</h3>
+  {% if gdrive.configured %}
+  <div class="conn-row">
+    <span class="dot-ok"></span>
+    <span>Configured</span>
+  </div>
+  <div class="info-grid" style="margin-top:8px">
+    <div class="k">Service account key</div>
+    <div class="v"><code>{{ gdrive.sa_path }}</code></div>
+    <div class="k">Inbox folder ID</div>
+    <div class="v"><code>{{ gdrive.folder_id }}</code></div>
+  </div>
+  <div class="conn-row" style="margin-top:8px">
+    <button
+      type="button"
+      class="btn"
+      hx-post="/ui/connections/gdrive/test"
+      hx-target="#gdrive-test-result"
+      hx-swap="innerHTML"
+    >
+      Test connection
+    </button>
+    <button type="button" class="btn" onclick="runIngestNow()">Run ingest now</button>
+  </div>
+  <div id="gdrive-test-result" style="font-size:11px;color:#666;margin-top:4px"></div>
+  <div id="gdrive-ingest-result" style="font-size:11px;color:#666;margin-top:2px"></div>
+  {% else %}
   <div class="conn-row">
     <span class="dot-na"></span>
-    <span style="color:#888">Not configured — available after PR-GDRIVE-INGEST-01.</span>
+    <span style="color:#888">Not configured.</span>
   </div>
+  <div style="font-size:11px;color:#999;margin-top:4px">
+    Set <code>GDRIVE_SA_JSON_PATH</code> and <code>GDRIVE_INBOX_FOLDER_ID</code>.
+  </div>
+  {% endif %}
   <div style="font-size:11px;color:#999;margin-top:4px">
     Ingest: Service Account + shared folder (Inbox).<br>
     Credentials: <code>/data/secrets/gdrive_sa.json</code>
   </div>
 </div>
 
+<script>
+async function runIngestNow() {
+  var out = document.getElementById('gdrive-ingest-result');
+  if (!out) return;
+  out.style.color = '#666';
+  out.textContent = 'Running ingest...';
+
+  var response = null;
+  var data = {};
+  try {
+    response = await fetch('/api/actions/ingest', {method: 'POST'});
+    data = await response.json();
+  } catch (_err) {
+    out.style.color = '#b42318';
+    out.textContent = 'Ingest request failed.';
+    return;
+  }
+
+  if (response.status === 409) {
+    out.style.color = '#92400e';
+    var retryAfter = data.retry_after_seconds;
+    if (typeof retryAfter === 'number') {
+      out.textContent = 'Ingest is already running. Retry in ' + retryAfter + 's.';
+    } else {
+      out.textContent = 'Ingest is already running.';
+    }
+    return;
+  }
+
+  if (!response.ok) {
+    out.style.color = '#b42318';
+    out.textContent = data.detail || 'Ingest failed.';
+    return;
+  }
+
+  out.style.color = '#14532d';
+  out.textContent = 'Ingest completed. New recordings: ' + (data.count || 0) + '.';
+}
+</script>
+
 <div class="conn-card">
   <h3>Microsoft Graph (Work account)</h3>
   {% if ms.status == "connected" %}
diff --git a/lan_app/templates/recording_detail.html b/lan_app/templates/recording_detail.html
index 66b6592..ab04ffc 100644
--- a/lan_app/templates/recording_detail.html
+++ b/lan_app/templates/recording_detail.html
@@ -22,6 +22,18 @@
 </p>
 {% endif %}
 
+{% if rec.status == 'Processing' %}
+<div
+  id="recording-progress"
+  hx-get="/ui/recordings/{{ rec.id }}/progress"
+  hx-trigger="load, every 2s"
+  hx-swap="innerHTML"
+  style="margin:0 0 10px 0"
+>
+  <div class="placeholder" style="padding:8px 10px">Loading pipeline progress…</div>
+</div>
+{% endif %}
+
 <div class="tabs">
   {% for t in tabs %}
   <a href="/recordings/{{ rec.id }}?tab={{ t }}" class="tab {% if current_tab == t %}active{% endif %}">{{ t|capitalize }}</a>
diff --git a/lan_app/ui_routes.py b/lan_app/ui_routes.py
index d15c24b..d2e4128 100644
--- a/lan_app/ui_routes.py
+++ b/lan_app/ui_routes.py
@@ -40,6 +40,7 @@ from .constants import (
     JOB_STATUSES,
     JOB_TYPE_PRECHECK,
     RECORDING_STATUSES,
+    RECORDING_STATUS_PROCESSING,
     RECORDING_STATUS_QUARANTINE,
 )
 from .db import (
@@ -66,6 +67,7 @@ from .db import (
     set_recording_language_settings,
     set_recording_status,
 )
+from .gdrive import build_drive_service
 from .jobs import (
     DuplicateRecordingJobError,
     enqueue_recording_job,
@@ -152,6 +154,63 @@ def _recording_recovery_warning(jobs: list[dict[str, Any]]) -> str | None:
     return None
 
 
+def _safe_pipeline_progress(value: object) -> float:
+    try:
+        progress = float(value)
+    except (TypeError, ValueError):
+        return 0.0
+    return max(0.0, min(progress, 1.0))
+
+
+def _pipeline_stage_label(stage: object) -> str:
+    text = str(stage or "").strip()
+    if not text:
+        return "Waiting"
+    return text.replace("_", " ").title()
+
+
+def _gdrive_connection_state(settings: AppSettings) -> dict[str, Any]:
+    sa_path_value = str(settings.gdrive_sa_json_path or "").strip()
+    folder_id_value = str(settings.gdrive_inbox_folder_id or "").strip()
+    return {
+        "configured": bool(sa_path_value and folder_id_value),
+        "sa_path": sa_path_value,
+        "folder_id": folder_id_value,
+    }
+
+
+def _test_gdrive_connection(settings: AppSettings) -> dict[str, Any]:
+    state = _gdrive_connection_state(settings)
+    if not state["configured"]:
+        raise ValueError("Google Drive is not configured.")
+    sa_path = Path(str(state["sa_path"]))
+    folder_id = str(state["folder_id"])
+    service = build_drive_service(sa_path)
+    query = f"'{folder_id}' in parents and trashed=false"
+    response = (
+        service.files()
+        .list(
+            q=query,
+            fields="files(id,name,createdTime)",
+            pageSize=1,
+        )
+        .execute()
+    )
+    rows = response.get("files", []) or []
+    if rows:
+        first = rows[0]
+        name = str(first.get("name") or "").strip() or "(unnamed)"
+        file_id = str(first.get("id") or "").strip() or "unknown-id"
+        return {
+            "ok": True,
+            "message": f"Connected. Sample file: {name} ({file_id}).",
+        }
+    return {
+        "ok": True,
+        "message": "Connected. Inbox is reachable, but no files were found.",
+    }
+
+
 def _load_json_dict(path: Path) -> dict[str, Any]:
     if not path.exists():
         return {}
@@ -1114,6 +1173,29 @@ async def ui_recording_detail(
     )
 
 
+@ui_router.get("/ui/recordings/{recording_id}/progress", response_class=HTMLResponse)
+async def ui_recording_progress(request: Request, recording_id: str) -> Any:
+    rec = get_recording(recording_id, settings=_settings)
+    if rec is None:
+        return HTMLResponse("Not found", status_code=404)
+    progress_ratio = _safe_pipeline_progress(rec.get("pipeline_progress"))
+    progress_percent = int(round(progress_ratio * 100))
+    return templates.TemplateResponse(
+        request,
+        "partials/recording_progress.html",
+        {
+            "rec": rec,
+            "progress_ratio": progress_ratio,
+            "progress_percent": progress_percent,
+            "stage_code": str(rec.get("pipeline_stage") or "").strip() or "waiting",
+            "stage_label": _pipeline_stage_label(rec.get("pipeline_stage")),
+            "updated_at": str(rec.get("pipeline_updated_at") or "").strip(),
+            "warning": str(rec.get("last_warning") or "").strip(),
+            "is_processing": str(rec.get("status") or "") == RECORDING_STATUS_PROCESSING,
+        },
+    )
+
+
 # ---------------------------------------------------------------------------
 # Recording speaker assignment + snippet audio
 # ---------------------------------------------------------------------------
@@ -1514,16 +1596,29 @@ async def ui_queue(
 @ui_router.get("/connections", response_class=HTMLResponse)
 async def ui_connections(request: Request) -> Any:
     ms_state = await run_in_threadpool(ms_connection_state, _settings)
+    gdrive_state = _gdrive_connection_state(_settings)
     return templates.TemplateResponse(
         request,
         "connections.html",
         {
             "active": "connections",
             "ms": ms_state,
+            "gdrive": gdrive_state,
         },
     )
 
 
+@ui_router.post("/ui/connections/gdrive/test", response_class=HTMLResponse)
+async def ui_test_gdrive_connection() -> Any:
+    try:
+        result = await run_in_threadpool(_test_gdrive_connection, _settings)
+    except ValueError as exc:
+        return HTMLResponse(f"<span style='color:#92400e'>{exc}</span>")
+    except Exception as exc:
+        return HTMLResponse(f"<span style='color:#b42318'>Google Drive test failed: {exc}</span>")
+    return HTMLResponse(f"<span style='color:#14532d'>{result['message']}</span>")
+
+
 # ---------------------------------------------------------------------------
 # Inline recording actions (HTMX targets returning HX-Redirect)
 # ---------------------------------------------------------------------------
diff --git a/lan_app/worker_tasks.py b/lan_app/worker_tasks.py
index 15306bc..a2c9fc7 100644
--- a/lan_app/worker_tasks.py
+++ b/lan_app/worker_tasks.py
@@ -33,6 +33,7 @@ from .constants import (
     RECORDING_STATUS_READY,
 )
 from .db import (
+    clear_recording_progress,
     fail_job,
     fail_job_if_started,
     finish_job_if_started,
@@ -42,6 +43,7 @@ from .db import (
     init_db,
     list_jobs,
     requeue_job_if_started,
+    set_recording_progress,
     set_recording_language_settings,
     set_recording_status,
     set_recording_status_if_current_in_and_job_started,
@@ -498,6 +500,15 @@ def _run_precheck_pipeline(
         recording_id,
         settings,
     )
+
+    def _progress_callback(stage: str, progress: float) -> None:
+        set_recording_progress(
+            recording_id,
+            stage=stage,
+            progress=progress,
+            settings=settings,
+        )
+
     asyncio.run(
         run_pipeline(
             audio_path=audio_path,
@@ -510,6 +521,7 @@ def _run_precheck_pipeline(
             transcript_language_override=transcript_language_override,
             calendar_title=calendar_title,
             calendar_attendees=calendar_attendees,
+            progress_callback=_progress_callback,
         )
     )
     metrics_payload = refresh_recording_metrics(
@@ -732,12 +744,14 @@ def process_job(job_id: str, recording_id: str, job_type: str) -> dict[str, str]
                     )
                     return _ignored_result(job_id, recording_id, job_type)
                 raise ValueError(f"Job not found: {job_id}")
+            clear_recording_progress(recording_id, settings=settings)
             _append_step_log(
                 log_path,
                 f"finished job={job_id} type={job_type} recording_status={final_status}",
             )
             break
         except Exception as exc:
+            clear_recording_progress(recording_id, settings=settings)
             current_job_status = _job_status(job_id, settings)
             if current_job_status and current_job_status != JOB_STATUS_STARTED:
                 _log_stale_inflight_execution(
diff --git a/lan_transcriber/pipeline_steps/orchestrator.py b/lan_transcriber/pipeline_steps/orchestrator.py
index 73fcc24..a16a91e 100644
--- a/lan_transcriber/pipeline_steps/orchestrator.py
+++ b/lan_transcriber/pipeline_steps/orchestrator.py
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 import asyncio
+import inspect
 import shutil
 import time
 from pathlib import Path
 from types import SimpleNamespace
-from typing import Any, Iterable, List, Protocol, Sequence
+from typing import Any, Awaitable, Callable, Iterable, List, Protocol, Sequence
 
 from pydantic_settings import BaseSettings
 
@@ -173,6 +174,26 @@ def run_precheck(audio_path: Path, cfg: Settings | None = None) -> PrecheckResul
     )
 
 
+ProgressCallback = Callable[[str, float], Any | Awaitable[Any]]
+
+
+async def _emit_progress(
+    callback: ProgressCallback | None,
+    *,
+    stage: str,
+    progress: float,
+) -> None:
+    if callback is None:
+        return
+    try:
+        out = callback(stage, progress)
+        if inspect.isawaitable(out):
+            await out
+    except Exception:
+        # Progress reporting is best-effort and must not break processing.
+        return
+
+
 async def run_pipeline(
     audio_path: Path,
     cfg: Settings,
@@ -184,11 +205,13 @@ async def run_pipeline(
     transcript_language_override: str | None = None,
     calendar_title: str | None = None,
     calendar_attendees: Sequence[str] | None = None,
+    progress_callback: ProgressCallback | None = None,
 ) -> TranscriptResult:
     start = time.perf_counter()
     artifacts = build_recording_artifacts(cfg.recordings_root, recording_id or _default_recording_id(audio_path), audio_path.suffix)
     stage_raw_audio(audio_path, artifacts.raw_audio_path)
 
+    await _emit_progress(progress_callback, stage="precheck", progress=0.05)
     precheck_result = precheck or run_precheck(audio_path, cfg)
     override_lang = normalise_language_code(transcript_language_override)
     summary_lang = resolve_target_summary_language(target_summary_language, dominant_language=override_lang or "unknown", detected_language=None)
@@ -201,6 +224,7 @@ async def run_pipeline(
     )
 
     if precheck_result.quarantine_reason:
+        await _emit_progress(progress_callback, stage="metrics", progress=0.95)
         _clear_dir(artifacts.snippets_dir)
         atomic_write_text(artifacts.transcript_txt_path, "")
         atomic_write_json(
@@ -239,10 +263,12 @@ async def run_pipeline(
             ),
         )
         atomic_write_json(artifacts.metrics_json_path, {"status": "quarantined", "version": 1, "precheck": precheck_result.__dict__})
+        await _emit_progress(progress_callback, stage="done", progress=1.0)
         p95_latency_seconds.observe(time.perf_counter() - start)
         return TranscriptResult(summary="Quarantined", body="", friendly=0, speakers=[], summary_path=artifacts.summary_json_path, body_path=artifacts.transcript_txt_path, unknown_chunks=[], segments=[])
 
     try:
+        await _emit_progress(progress_callback, stage="stt", progress=0.30)
         import whisperx
 
         def _asr() -> tuple[list[dict[str, Any]], dict[str, Any]]:
@@ -254,9 +280,12 @@ async def run_pipeline(
                 segments, info = whisperx.transcribe(str(audio_path), **kwargs)
             return list(segments), dict(info or {})
 
+        await _emit_progress(progress_callback, stage="diarize", progress=0.50)
         (raw_segments, info), diarization = await asyncio.gather(asyncio.to_thread(_asr), diariser(audio_path))
+        await _emit_progress(progress_callback, stage="align", progress=0.60)
         asr_segments = normalise_asr_segments(raw_segments)
         language_info = _language_payload(info)
+        await _emit_progress(progress_callback, stage="language", progress=0.70)
         detected_language = normalise_language_code(language_info["detected"]) if language_info["detected"] != "unknown" else None
         language_analysis = analyse_languages(asr_segments, detected_language=detected_language, transcript_language_override=override_lang)
         if language_info["detected"] == "unknown" and language_analysis.dominant_language != "unknown":
@@ -341,7 +370,9 @@ async def run_pipeline(
                 status="no_speech",
             ),
         )
+        await _emit_progress(progress_callback, stage="metrics", progress=0.95)
         atomic_write_json(artifacts.metrics_json_path, {"status": "no_speech", "version": 1, "precheck": {**precheck_result.__dict__, "quarantine_reason": None}, "language": language_info, "asr_segments": len(language_analysis.segments), "diar_segments": len(diar_segments), "speaker_turns": len(speaker_turns)})
+        await _emit_progress(progress_callback, stage="done", progress=1.0)
         p95_latency_seconds.observe(time.perf_counter() - start)
         return TranscriptResult(summary="No speech detected", body="", friendly=0, speakers=speakers, summary_path=artifacts.summary_json_path, body_path=artifacts.transcript_txt_path, unknown_chunks=[], segments=[])
 
@@ -357,6 +388,7 @@ async def run_pipeline(
     sys_prompt, user_prompt = build_structured_summary_prompts(speaker_turns, summary_lang, calendar_title=cal_title, calendar_attendees=cal_attendees)
 
     try:
+        await _emit_progress(progress_callback, stage="llm", progress=0.85)
         msg = await llm.generate(system_prompt=sys_prompt, user_prompt=user_prompt, model=cfg.llm_model, response_format={"type": "json_object"})
         raw_summary = msg.get("content", "") if isinstance(msg, dict) else str(msg)
         summary_payload = build_summary_payload(
@@ -389,7 +421,9 @@ async def run_pipeline(
         atomic_write_json(artifacts.segments_json_path, diar_segments)
         atomic_write_json(artifacts.speaker_turns_json_path, speaker_turns)
         atomic_write_json(artifacts.summary_json_path, summary_payload)
+        await _emit_progress(progress_callback, stage="metrics", progress=0.95)
         atomic_write_json(artifacts.metrics_json_path, {"status": "ok", "version": 1, "precheck": {**precheck_result.__dict__, "quarantine_reason": None}, "language": language_info, "asr_segments": len(language_analysis.segments), "diar_segments": len(diar_segments), "speaker_turns": len(speaker_turns), "snippets": len(snippet_paths)})
+        await _emit_progress(progress_callback, stage="done", progress=1.0)
         return TranscriptResult(summary=str(summary_payload.get("summary") or ""), body=clean_text, friendly=friendly, speakers=speakers, summary_path=artifacts.summary_json_path, body_path=artifacts.transcript_txt_path, unknown_chunks=snippet_paths, segments=serialised_segments)
     except Exception as exc:
         error_rate_total.inc()
diff --git a/tasks/QUEUE.md b/tasks/QUEUE.md
index d584481..a158031 100644
--- a/tasks/QUEUE.md
+++ b/tasks/QUEUE.md
@@ -122,7 +122,7 @@ Queue (in order)
 - Depends on: PR-PIPELINE-MODULAR-01
 
 23) PR-UI-PROGRESS-01: UI feedback: pipeline progress/stage + Connections page real status and Run ingest button
-- Status: TODO
+- Status: DONE
 - Tasks file: tasks/PR-UI-PROGRESS-01.md
 - Depends on: PR-DB-RESILIENCE-01
 
diff --git a/tests/test_db_queue.py b/tests/test_db_queue.py
index bca1de7..27056df 100644
--- a/tests/test_db_queue.py
+++ b/tests/test_db_queue.py
@@ -36,6 +36,7 @@ from lan_app.constants import (
     RECORDING_STATUS_READY,
 )
 from lan_app.db import (
+    clear_recording_progress,
     connect,
     connect_db,
     create_job,
@@ -46,6 +47,7 @@ from lan_app.db import (
     get_recording,
     init_db,
     list_jobs,
+    set_recording_progress,
     set_recording_status,
     start_job,
     upsert_calendar_match,
@@ -120,6 +122,34 @@ def test_connect_sets_wal_and_busy_timeout(tmp_path: Path):
     assert busy_timeout == 4321
 
 
+def test_recording_progress_helpers_set_and_clear(tmp_path: Path):
+    cfg = _test_settings(tmp_path)
+    init_db(cfg)
+    create_recording(
+        "rec-progress-helpers-1",
+        source="test",
+        source_filename="progress.mp3",
+        settings=cfg,
+    )
+
+    assert set_recording_progress(
+        "rec-progress-helpers-1",
+        stage="stt",
+        progress=0.3,
+        settings=cfg,
+    )
+    after_set = get_recording("rec-progress-helpers-1", settings=cfg) or {}
+    assert after_set["pipeline_stage"] == "stt"
+    assert after_set["pipeline_progress"] == 0.3
+    assert after_set["pipeline_updated_at"]
+
+    assert clear_recording_progress("rec-progress-helpers-1", settings=cfg)
+    after_clear = get_recording("rec-progress-helpers-1", settings=cfg) or {}
+    assert after_clear["pipeline_stage"] is None
+    assert after_clear["pipeline_progress"] is None
+    assert after_clear["pipeline_updated_at"] is None
+
+
 def test_with_db_retry_retries_on_forced_lock(tmp_path: Path):
     db_file = tmp_path / "db" / "locked.db"
     with connect_db(db_file, timeout=0.01, busy_timeout_ms=1) as conn:
@@ -237,9 +267,10 @@ def test_placeholder_cleanup_migration_only_removes_legacy_placeholders(tmp_path
             status=JOB_STATUS_QUEUED,
         )
 
+    # Force re-running migration 011 (legacy placeholder cleanup) even when
+    # newer migrations exist.
     with connect(cfg) as conn:
-        current_version = int(conn.execute("PRAGMA user_version").fetchone()[0])
-        conn.execute(f"PRAGMA user_version = {current_version - 1}")
+        conn.execute("PRAGMA user_version = 10")
         conn.commit()
 
     init_db(cfg)
@@ -1527,10 +1558,18 @@ def test_worker_precheck_runs_pipeline_when_safe(tmp_path: Path, monkeypatch):
     )
     called = {"value": False}
     observed_llm: dict[str, object] = {}
+    observed_progress: dict[str, object] = {}
 
     async def _fake_run_pipeline(*_args, **kwargs):
         called["value"] = True
         observed_llm["value"] = kwargs.get("llm")
+        progress_callback = kwargs.get("progress_callback")
+        observed_progress["has_callback"] = callable(progress_callback)
+        if callable(progress_callback):
+            progress_callback("stt", 0.3)
+            mid = get_recording("rec-precheck-ok-1", settings=cfg) or {}
+            observed_progress["stage"] = mid.get("pipeline_stage")
+            observed_progress["progress"] = mid.get("pipeline_progress")
         return None
 
     monkeypatch.setattr("lan_app.worker_tasks.run_pipeline", _fake_run_pipeline)
@@ -1539,15 +1578,75 @@ def test_worker_precheck_runs_pipeline_when_safe(tmp_path: Path, monkeypatch):
     assert result["status"] == "ok"
     assert called["value"] is True
     assert isinstance(observed_llm["value"], LLMClient)
+    assert observed_progress["has_callback"] is True
+    assert observed_progress["stage"] == "stt"
+    assert observed_progress["progress"] == 0.3
 
     recording = get_recording("rec-precheck-ok-1", settings=cfg)
     job = get_job("job-precheck-ok-1", settings=cfg)
     assert recording is not None
     assert recording["status"] == RECORDING_STATUS_READY
+    assert recording["pipeline_stage"] is None
+    assert recording["pipeline_progress"] is None
+    assert recording["pipeline_updated_at"] is None
     assert job is not None
     assert job["status"] == JOB_STATUS_FINISHED
 
 
+def test_worker_clears_progress_when_pipeline_fails(tmp_path: Path, monkeypatch):
+    cfg = _test_settings(tmp_path)
+    monkeypatch.setenv("LAN_DATA_ROOT", str(cfg.data_root))
+    monkeypatch.setenv("LAN_RECORDINGS_ROOT", str(cfg.recordings_root))
+    monkeypatch.setenv("LAN_DB_PATH", str(cfg.db_path))
+    monkeypatch.setenv("LAN_PROM_SNAPSHOT_PATH", str(cfg.metrics_snapshot_path))
+    monkeypatch.setenv("LAN_MAX_JOB_ATTEMPTS", "1")
+
+    init_db(cfg)
+    create_recording(
+        "rec-progress-fail-1",
+        source="test",
+        source_filename="broken.wav",
+        settings=cfg,
+    )
+    create_job(
+        "job-progress-fail-1",
+        recording_id="rec-progress-fail-1",
+        job_type=JOB_TYPE_PRECHECK,
+        settings=cfg,
+    )
+
+    raw_audio = cfg.recordings_root / "rec-progress-fail-1" / "raw" / "audio.wav"
+    raw_audio.parent.mkdir(parents=True, exist_ok=True)
+    raw_audio.write_bytes(b"\x00")
+
+    monkeypatch.setattr("lan_app.worker_tasks._resolve_raw_audio_path", lambda *_a, **_k: raw_audio)
+    monkeypatch.setattr(
+        "lan_app.worker_tasks.run_precheck",
+        lambda *_a, **_k: PrecheckResult(
+            duration_sec=60.0,
+            speech_ratio=0.8,
+            quarantine_reason=None,
+        ),
+    )
+
+    async def _failing_run_pipeline(*_args, **kwargs):
+        progress_callback = kwargs.get("progress_callback")
+        if callable(progress_callback):
+            progress_callback("stt", 0.3)
+        raise RuntimeError("pipeline boom")
+
+    monkeypatch.setattr("lan_app.worker_tasks.run_pipeline", _failing_run_pipeline)
+
+    with pytest.raises(RuntimeError, match="pipeline boom"):
+        process_job("job-progress-fail-1", "rec-progress-fail-1", JOB_TYPE_PRECHECK)
+
+    recording = get_recording("rec-progress-fail-1", settings=cfg)
+    assert recording is not None
+    assert recording["pipeline_stage"] is None
+    assert recording["pipeline_progress"] is None
+    assert recording["pipeline_updated_at"] is None
+
+
 def test_worker_precheck_keeps_auto_summary_target_unset(tmp_path: Path, monkeypatch):
     cfg = _test_settings(tmp_path)
     monkeypatch.setenv("LAN_DATA_ROOT", str(cfg.data_root))
diff --git a/tests/test_pipeline.py b/tests/test_pipeline.py
index 5f74550..392f21e 100644
--- a/tests/test_pipeline.py
+++ b/tests/test_pipeline.py
@@ -176,6 +176,57 @@ async def test_tripled_dedup(tmp_path: Path, mocker):
     assert summary_data["summary"] == "- ok"
 
 
+@pytest.mark.asyncio
+@respx.mock
+async def test_pipeline_emits_progress_stages_in_order(tmp_path: Path, mocker):
+    mocker.patch(
+        "whisperx.transcribe",
+        return_value=(
+            [{"start": 0.0, "end": 1.0, "text": "hello world team"}],
+            {"language": "en", "language_probability": 0.95},
+        ),
+    )
+    respx.post("http://llm:8000/v1/chat/completions").mock(
+        return_value=httpx.Response(
+            200,
+            json={"choices": [{"message": {"content": "- summary"}}]},
+        ),
+    )
+    mocker.patch(
+        "transformers.pipeline",
+        lambda *a, **k: lambda text: [{"label": "positive", "score": 0.7}],
+    )
+    cfg = pipeline.Settings(
+        speaker_db=tmp_path / "db.yaml",
+        tmp_root=tmp_path,
+        recordings_root=tmp_path / "recordings",
+    )
+    events: list[tuple[str, float]] = []
+
+    def _progress(stage: str, progress: float) -> None:
+        events.append((stage, progress))
+
+    await pipeline.run_pipeline(
+        fake_audio(tmp_path, "progress.mp3"),
+        cfg,
+        llm_client.LLMClient(),
+        DummyDiariser(),
+        precheck=precheck_ok(),
+        progress_callback=_progress,
+    )
+
+    assert events == [
+        ("precheck", 0.05),
+        ("stt", 0.30),
+        ("diarize", 0.50),
+        ("align", 0.60),
+        ("language", 0.70),
+        ("llm", 0.85),
+        ("metrics", 0.95),
+        ("done", 1.0),
+    ]
+
+
 @pytest.mark.asyncio
 @respx.mock
 async def test_alias_persist(tmp_path: Path, mocker):
diff --git a/tests/test_ui_routes.py b/tests/test_ui_routes.py
index 625ca7b..cfa561e 100644
--- a/tests/test_ui_routes.py
+++ b/tests/test_ui_routes.py
@@ -26,6 +26,7 @@ from lan_app.db import (
     list_project_keyword_weights,
     replace_participant_metrics,
     list_voice_profiles,
+    set_recording_progress,
     set_speaker_assignment,
     upsert_calendar_match,
     upsert_meeting_metrics,
@@ -36,6 +37,7 @@ from lan_app.constants import (
     JOB_TYPE_PRECHECK,
     JOB_TYPE_STT,
     RECORDING_STATUS_NEEDS_REVIEW,
+    RECORDING_STATUS_PROCESSING,
     RECORDING_STATUS_READY,
 )
 
@@ -176,6 +178,54 @@ def test_recording_detail_overview(seeded_client):
     assert "Ready" in r.text
 
 
+def test_recording_detail_processing_polls_progress(tmp_path, monkeypatch):
+    cfg = _cfg(tmp_path)
+    monkeypatch.setattr(api, "_settings", cfg)
+    monkeypatch.setattr(ui_routes, "_settings", cfg)
+    init_db(cfg)
+    create_recording(
+        "rec-ui-processing-1",
+        source="drive",
+        source_filename="processing.mp3",
+        status=RECORDING_STATUS_PROCESSING,
+        settings=cfg,
+    )
+
+    c = TestClient(api.app, follow_redirects=True)
+    r = c.get("/recordings/rec-ui-processing-1")
+    assert r.status_code == 200
+    assert "/ui/recordings/rec-ui-processing-1/progress" in r.text
+    assert "every 2s" in r.text
+
+
+def test_recording_progress_endpoint_renders_expected_html(tmp_path, monkeypatch):
+    cfg = _cfg(tmp_path)
+    monkeypatch.setattr(api, "_settings", cfg)
+    monkeypatch.setattr(ui_routes, "_settings", cfg)
+    init_db(cfg)
+    create_recording(
+        "rec-ui-progress-1",
+        source="drive",
+        source_filename="progress.mp3",
+        status=RECORDING_STATUS_PROCESSING,
+        settings=cfg,
+    )
+    set_recording_progress(
+        "rec-ui-progress-1",
+        stage="diarize",
+        progress=0.5,
+        settings=cfg,
+    )
+
+    c = TestClient(api.app, follow_redirects=True)
+    r = c.get("/ui/recordings/rec-ui-progress-1/progress")
+    assert r.status_code == 200
+    assert "Pipeline:" in r.text
+    assert "Diarize" in r.text
+    assert "50%" in r.text
+    assert "stage=<code>diarize</code>" in r.text
+
+
 def test_recording_detail_log_tab(seeded_client):
     r = seeded_client.get("/recordings/rec-ui-1?tab=log")
     assert r.status_code == 200
@@ -822,6 +872,62 @@ def test_connections(client):
     assert "Microsoft Graph" in r.text
 
 
+def test_connections_shows_configured_gdrive(tmp_path, monkeypatch):
+    cfg = _cfg(tmp_path)
+    cfg.gdrive_sa_json_path = cfg.data_root / "secrets" / "gdrive_sa.json"
+    cfg.gdrive_inbox_folder_id = "folder-abc"
+    monkeypatch.setattr(api, "_settings", cfg)
+    monkeypatch.setattr(ui_routes, "_settings", cfg)
+    monkeypatch.setattr(
+        ui_routes,
+        "ms_connection_state",
+        lambda _settings: {"configured": False, "status": "not_configured"},
+    )
+    init_db(cfg)
+
+    client = TestClient(api.app, follow_redirects=True)
+    resp = client.get("/connections")
+    assert resp.status_code == 200
+    assert "Configured" in resp.text
+    assert "folder-abc" in resp.text
+    assert "Test connection" in resp.text
+    assert "Run ingest now" in resp.text
+
+
+def test_ui_test_gdrive_connection_endpoint_success(tmp_path, monkeypatch):
+    cfg = _cfg(tmp_path)
+    monkeypatch.setattr(api, "_settings", cfg)
+    monkeypatch.setattr(ui_routes, "_settings", cfg)
+    init_db(cfg)
+    monkeypatch.setattr(
+        ui_routes,
+        "_test_gdrive_connection",
+        lambda _settings: {"ok": True, "message": "Connected. Sample file: demo.mp3 (id-1)."},
+    )
+
+    client = TestClient(api.app, follow_redirects=True)
+    resp = client.post("/ui/connections/gdrive/test")
+    assert resp.status_code == 200
+    assert "Connected. Sample file: demo.mp3 (id-1)." in resp.text
+
+
+def test_ui_test_gdrive_connection_endpoint_not_configured(tmp_path, monkeypatch):
+    cfg = _cfg(tmp_path)
+    monkeypatch.setattr(api, "_settings", cfg)
+    monkeypatch.setattr(ui_routes, "_settings", cfg)
+    init_db(cfg)
+
+    def _boom(_settings):
+        raise ValueError("Google Drive is not configured.")
+
+    monkeypatch.setattr(ui_routes, "_test_gdrive_connection", _boom)
+
+    client = TestClient(api.app, follow_redirects=True)
+    resp = client.post("/ui/connections/gdrive/test")
+    assert resp.status_code == 200
+    assert "Google Drive is not configured." in resp.text
+
+
 def test_ui_root_redirects_to_login_when_auth_enabled(tmp_path, monkeypatch):
     cfg = _cfg(tmp_path)
     cfg.api_bearer_token = "secret-ui-token"
diff --git a/lan_app/migrations/012_add_recording_pipeline_progress.sql b/lan_app/migrations/012_add_recording_pipeline_progress.sql
new file mode 100644
index 0000000..f2cf005
--- /dev/null
+++ b/lan_app/migrations/012_add_recording_pipeline_progress.sql
@@ -0,0 +1,4 @@
+ALTER TABLE recordings ADD COLUMN pipeline_stage TEXT;
+ALTER TABLE recordings ADD COLUMN pipeline_progress REAL;
+ALTER TABLE recordings ADD COLUMN pipeline_updated_at TEXT;
+ALTER TABLE recordings ADD COLUMN last_warning TEXT;
diff --git a/lan_app/templates/partials/recording_progress.html b/lan_app/templates/partials/recording_progress.html
new file mode 100644
index 0000000..4554ad7
--- /dev/null
+++ b/lan_app/templates/partials/recording_progress.html
@@ -0,0 +1,21 @@
+<div style="border:1px solid #ccd;background:#fff;padding:8px 10px">
+  <div style="display:flex;gap:8px;align-items:center;justify-content:space-between;margin-bottom:6px;font-size:12px">
+    <span><strong>Pipeline:</strong> {{ stage_label }}</span>
+    <span>{{ progress_percent }}%</span>
+  </div>
+  <div style="width:100%;max-width:520px;height:14px;border:1px solid #99b;background:#eef2ff">
+    <div style="height:100%;background:#4a6080;width:{{ progress_percent }}%"></div>
+  </div>
+  <div style="font-size:11px;color:#555;margin-top:5px">
+    stage=<code>{{ stage_code }}</code>
+    {% if updated_at %}
+    &nbsp;updated={{ updated_at[:19].replace('T', ' ') }}
+    {% endif %}
+    {% if warning %}
+    &nbsp;warning={{ warning }}
+    {% endif %}
+    {% if not is_processing %}
+    &nbsp;status={{ rec.status }}
+    {% endif %}
+  </div>
+</div>
