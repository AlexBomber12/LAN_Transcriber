diff --git a/lan_transcriber/pipeline_steps/orchestrator.py b/lan_transcriber/pipeline_steps/orchestrator.py
index a16a91e..d30b8a1 100644
--- a/lan_transcriber/pipeline_steps/orchestrator.py
+++ b/lan_transcriber/pipeline_steps/orchestrator.py
@@ -49,6 +49,11 @@ class Settings(BaseSettings):
     unknown_dir: Path = default_unknown_dir()
     tmp_root: Path = default_tmp_root()
     llm_model: str = "llama3:8b"
+    asr_model: str = "large-v3"
+    asr_device: str = "auto"
+    asr_compute_type: str | None = None
+    asr_batch_size: int = 16
+    asr_enable_align: bool = True
     embed_threshold: float = 0.65
     merge_similar: float = 0.9
     precheck_min_duration_sec: float = 20.0
@@ -117,6 +122,82 @@ def _language_payload(info: dict[str, Any]) -> dict[str, Any]:
     return {"detected": detected, "confidence": confidence}
 
 
+def _select_asr_device(cfg: Settings) -> str:
+    preferred = str(cfg.asr_device or "auto").strip().lower()
+    if preferred in {"cpu", "cuda"}:
+        return preferred
+    try:
+        import torch
+    except Exception:
+        return "cpu"
+    return "cuda" if torch.cuda.is_available() else "cpu"
+
+
+def _select_compute_type(cfg: Settings, device: str) -> str:
+    configured = str(cfg.asr_compute_type or "").strip()
+    if configured:
+        return configured
+    return "float16" if device == "cuda" else "int8"
+
+
+def _whisperx_asr(
+    audio_path: Path,
+    *,
+    override_lang: str | None,
+    cfg: Settings,
+) -> tuple[list[dict[str, Any]], dict[str, Any]]:
+    import whisperx
+
+    if hasattr(whisperx, "transcribe"):
+        kwargs: dict[str, Any] = {
+            "vad_filter": True,
+            "language": override_lang or "auto",
+        }
+        try:
+            segments, info = whisperx.transcribe(str(audio_path), word_timestamps=True, **kwargs)
+        except TypeError:
+            segments, info = whisperx.transcribe(str(audio_path), **kwargs)
+        return list(segments), dict(info or {})
+
+    device = _select_asr_device(cfg)
+    compute_type = _select_compute_type(cfg, device)
+    audio = whisperx.load_audio(str(audio_path))
+    try:
+        model = whisperx.load_model(cfg.asr_model, device, compute_type=compute_type)
+    except TypeError:
+        model = whisperx.load_model(cfg.asr_model, device)
+
+    result = model.transcribe(
+        audio,
+        batch_size=cfg.asr_batch_size,
+        vad_filter=True,
+        language=(override_lang if override_lang else None),
+    )
+    segments = list(result.get("segments", []))
+    info: dict[str, Any] = {"language": result.get("language") or (override_lang or "unknown")}
+
+    if cfg.asr_enable_align:
+        try:
+            align_lang = normalise_language_code(info.get("language")) or "en"
+            model_a, metadata = whisperx.load_align_model(language_code=align_lang, device=device)
+            try:
+                aligned = whisperx.align(
+                    segments,
+                    model_a,
+                    metadata,
+                    audio,
+                    device,
+                    return_char_alignments=False,
+                )
+            except TypeError:
+                aligned = whisperx.align(segments, model_a, metadata, audio, device)
+            segments = list(aligned.get("segments", segments))
+        except Exception:
+            pass
+
+    return segments, info
+
+
 def _empty_questions() -> dict[str, Any]:
     return {
         "total_count": 0,
@@ -269,19 +350,12 @@ async def run_pipeline(
 
     try:
         await _emit_progress(progress_callback, stage="stt", progress=0.30)
-        import whisperx
-
-        def _asr() -> tuple[list[dict[str, Any]], dict[str, Any]]:
-            asr_language = override_lang or "auto"
-            kwargs: dict[str, Any] = {"vad_filter": True, "language": asr_language}
-            try:
-                segments, info = whisperx.transcribe(str(audio_path), word_timestamps=True, **kwargs)
-            except TypeError:
-                segments, info = whisperx.transcribe(str(audio_path), **kwargs)
-            return list(segments), dict(info or {})
 
         await _emit_progress(progress_callback, stage="diarize", progress=0.50)
-        (raw_segments, info), diarization = await asyncio.gather(asyncio.to_thread(_asr), diariser(audio_path))
+        (raw_segments, info), diarization = await asyncio.gather(
+            asyncio.to_thread(_whisperx_asr, audio_path, override_lang=override_lang, cfg=cfg),
+            diariser(audio_path),
+        )
         await _emit_progress(progress_callback, stage="align", progress=0.60)
         asr_segments = normalise_asr_segments(raw_segments)
         language_info = _language_payload(info)
diff --git a/tasks/QUEUE.md b/tasks/QUEUE.md
index 0c6a1f6..c1bdcad 100644
--- a/tasks/QUEUE.md
+++ b/tasks/QUEUE.md
@@ -172,6 +172,6 @@ Queue (in order)
 - Depends on: PR-DOCS-EXPORT-ONLY-01
 
 33) PR-FIX-WHISPERX-API-01: Fix WhisperX API usage (no whisperx.transcribe) and add modern-path unit test
-- Status: TODO
+- Status: DONE
 - Tasks file: tasks/PR-FIX-WHISPERX-API-01.md
 - Depends on: PR-FIX-DIARIZATION-REVISION-01
diff --git a/tests/test_whisperx_api.py b/tests/test_whisperx_api.py
new file mode 100644
index 0000000..6fb37e1
--- /dev/null
+++ b/tests/test_whisperx_api.py
@@ -0,0 +1,84 @@
+from __future__ import annotations
+
+import sys
+from pathlib import Path
+from types import ModuleType
+
+from lan_transcriber.pipeline_steps import orchestrator as pipeline
+
+
+def test_whisperx_asr_modern_path(tmp_path: Path, monkeypatch):
+    fake_whisperx = ModuleType("whisperx")
+
+    class _FakeModel:
+        def transcribe(
+            self,
+            audio: str,
+            *,
+            batch_size: int,
+            vad_filter: bool,
+            language: str | None,
+        ) -> dict[str, object]:
+            assert audio == "audio"
+            assert batch_size == 16
+            assert vad_filter is True
+            assert language is None
+            return {"segments": [{"start": 0.0, "end": 1.0, "text": "hello"}], "language": "en"}
+
+    def _load_audio(path: str) -> str:
+        assert path.endswith("a.wav")
+        return "audio"
+
+    def _load_model(model_name: str, device: str, compute_type: str = "int8") -> _FakeModel:
+        assert model_name == "large-v3"
+        assert device == "cpu"
+        assert compute_type == "int8"
+        return _FakeModel()
+
+    def _load_align_model(language_code: str, device: str) -> tuple[str, dict[str, str]]:
+        assert language_code == "en"
+        assert device == "cpu"
+        return "align_model", {"lang": language_code}
+
+    def _align(
+        segments: list[dict[str, object]],
+        model_a: str,
+        metadata: dict[str, str],
+        audio: str,
+        device: str,
+        return_char_alignments: bool = False,
+    ) -> dict[str, object]:
+        assert model_a == "align_model"
+        assert metadata == {"lang": "en"}
+        assert audio == "audio"
+        assert device == "cpu"
+        assert return_char_alignments is False
+        enriched = []
+        for segment in segments:
+            row = dict(segment)
+            row["words"] = [{"word": "hello", "start": 0.0, "end": 1.0}]
+            enriched.append(row)
+        return {"segments": enriched}
+
+    fake_whisperx.load_audio = _load_audio
+    fake_whisperx.load_model = _load_model
+    fake_whisperx.load_align_model = _load_align_model
+    fake_whisperx.align = _align
+    monkeypatch.setitem(sys.modules, "whisperx", fake_whisperx)
+
+    audio_path = tmp_path / "a.wav"
+    audio_path.write_bytes(b"")
+    cfg = pipeline.Settings(
+        asr_device="cpu",
+        asr_enable_align=True,
+        tmp_root=tmp_path,
+        recordings_root=tmp_path / "recordings",
+    )
+
+    segments, info = pipeline._whisperx_asr(audio_path, override_lang=None, cfg=cfg)
+
+    assert isinstance(segments, list)
+    assert segments
+    assert all("start" in segment and "end" in segment and "text" in segment for segment in segments)
+    assert any(segment.get("words") for segment in segments)
+    assert info["language"] == "en"
