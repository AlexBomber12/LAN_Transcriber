diff --git a/.coveragerc b/.coveragerc
index 6cc6e65..facc48b 100644
--- a/.coveragerc
+++ b/.coveragerc
@@ -1,4 +1,3 @@
 [run]
 omit =
     lan_transcriber/api.py
-    lan_transcriber/pipeline.py
diff --git a/lan_app/conversation_metrics.py b/lan_app/conversation_metrics.py
index bb1cd77..4bef248 100644
--- a/lan_app/conversation_metrics.py
+++ b/lan_app/conversation_metrics.py
@@ -6,6 +6,8 @@ from pathlib import Path
 from typing import Any, Sequence
 
 from lan_transcriber.artifacts import atomic_write_json
+from lan_transcriber.utils import normalise_language_code as _normalise_language_code_shared
+from lan_transcriber.utils import safe_float as _safe_float_shared
 
 from .config import AppSettings
 from .db import replace_participant_metrics, upsert_meeting_metrics
@@ -46,25 +48,11 @@ _TASK_CUES = (
 )
 
 def _safe_float(value: Any, *, default: float = 0.0) -> float:
-    try:
-        out = float(value)
-    except (TypeError, ValueError):
-        return default
-    if out < 0:
-        return default
-    return out
+    return _safe_float_shared(value, default=default, min_value=0.0)
 
 
 def _normalise_language_code(value: Any) -> str | None:
-    if not isinstance(value, str):
-        return None
-    raw = value.strip().lower()
-    if not raw:
-        return None
-    token = raw.replace("_", "-").split("-", 1)[0]
-    if len(token) == 2 and token.isalpha():
-        return token
-    return None
+    return _normalise_language_code_shared(value)
 
 
 def _normalise_turns(turns: Sequence[dict[str, Any]]) -> list[dict[str, Any]]:
diff --git a/lan_app/ui_routes.py b/lan_app/ui_routes.py
index 4ced40f..d15c24b 100644
--- a/lan_app/ui_routes.py
+++ b/lan_app/ui_routes.py
@@ -83,6 +83,7 @@ from lan_transcriber.artifacts import atomic_write_json
 from lan_transcriber.llm_client import LLMClient
 from lan_transcriber.pipeline import Settings as PipelineSettings
 from lan_transcriber.pipeline import build_structured_summary_prompts, build_summary_payload
+from lan_transcriber.utils import normalise_language_code as _normalise_language_code_shared
 
 _TEMPLATES_DIR = Path(__file__).parent / "templates"
 _STATIC_DIR = Path(__file__).parent / "static"
@@ -110,39 +111,12 @@ _LANGUAGE_NAME_MAP: dict[str, str] = {
     "zh": "Chinese",
 }
 
-_LANGUAGE_CODE_MAP: dict[str, str] = {
-    "eng": "en",
-    "spa": "es",
-    "fra": "fr",
-    "fre": "fr",
-    "deu": "de",
-    "ger": "de",
-    "ita": "it",
-    "por": "pt",
-    "rus": "ru",
-    "ukr": "uk",
-    "jpn": "ja",
-    "kor": "ko",
-    "zho": "zh",
-    "chi": "zh",
-}
-
 _COMMON_LANGUAGE_CODES = ("en", "es", "fr", "de", "pt", "it", "zh", "ja", "ko", "ru")
 _STUCK_JOB_RECOVERY_ERROR = "stuck job recovered"
 
 
 def _normalise_language_code(value: object | None) -> str | None:
-    if not isinstance(value, str):
-        return None
-    raw = value.strip().lower()
-    if not raw:
-        return None
-    token = raw.replace("_", "-").split("-", 1)[0]
-    if len(token) == 2 and token.isalpha():
-        return token
-    if len(token) == 3 and token.isalpha():
-        return _LANGUAGE_CODE_MAP.get(token, None)
-    return None
+    return _normalise_language_code_shared(value)
 
 
 def _language_display_name(code: str | None) -> str:
diff --git a/lan_transcriber/pipeline.py b/lan_transcriber/pipeline.py
index e5e2321..3e29ed9 100644
--- a/lan_transcriber/pipeline.py
+++ b/lan_transcriber/pipeline.py
@@ -1,1692 +1,19 @@
 from __future__ import annotations
 
-import asyncio
-import audioop
-import json
-import math
-import re
-import shutil
-import subprocess
-import time
-import wave
-from dataclasses import dataclass
-from pathlib import Path
-from types import SimpleNamespace
-from typing import Any, Iterable, List, Protocol, Sequence
-
-from pydantic_settings import BaseSettings
-
-from . import normalizer
-from .aliases import ALIAS_PATH, load_aliases as _load_aliases, save_aliases as _save_aliases
-from .artifacts import (
-    atomic_write_json,
-    atomic_write_text,
-    build_recording_artifacts,
-    stage_raw_audio,
-)
-from .llm_client import LLMClient
-from .metrics import error_rate_total, p95_latency_seconds
-from .models import SpeakerSegment, TranscriptResult
-from .runtime_paths import (
-    default_recordings_root,
-    default_tmp_root,
-    default_unknown_dir,
-    default_voices_dir,
-)
-
-
-@dataclass(frozen=True)
-class PrecheckResult:
-    duration_sec: float | None
-    speech_ratio: float | None
-    quarantine_reason: str | None = None
-
-
-class Diariser(Protocol):
-    """Minimal interface for speaker diarisation."""
-
-    async def __call__(self, audio_path: Path): ...
-
-
-class Settings(BaseSettings):
-    """Runtime configuration for the transcription pipeline."""
-
-    speaker_db: Path = ALIAS_PATH
-    recordings_root: Path = default_recordings_root()
-    voices_dir: Path = default_voices_dir()
-    unknown_dir: Path = default_unknown_dir()
-    tmp_root: Path = default_tmp_root()
-    llm_model: str = "llama3:8b"
-    embed_threshold: float = 0.65
-    merge_similar: float = 0.9
-    precheck_min_duration_sec: float = 20.0
-    precheck_min_speech_ratio: float = 0.10
-
-    class Config:
-        env_prefix = "LAN_"
-
-
-def _merge_similar(
-    lines: Iterable[str], threshold: float
-) -> List[str]:  # pragma: no cover - simple heuristic
-    out: List[str] = []
-    for line in lines:
-        if not out:
-            out.append(line)
-            continue
-        prev = out[-1]
-        sim = sum(a == b for a, b in zip(prev, line)) / max(len(prev), len(line))
-        if sim >= threshold:
-            continue
-        out.append(line)
-    return out
-
-
-def _sentiment_score(text: str) -> int:  # pragma: no cover - trivial wrapper
-    from transformers import pipeline as hf_pipeline
-
-    sent = hf_pipeline("sentiment-analysis")(text[:4000])[0]
-    if sent["label"] == "positive":
-        return int(sent["score"] * 100)
-    if sent["label"] == "negative":
-        return int((1 - sent["score"]) * 100)
-    return 50
-
-
-def refresh_aliases(result: TranscriptResult, alias_path: Path = ALIAS_PATH) -> None:
-    """Reload aliases from disk and update ``result`` in-place."""
-    aliases = _load_aliases(alias_path)
-    result.speakers = sorted({aliases.get(s.speaker, s.speaker) for s in result.segments})
-
-
-def _default_recording_id(audio_path: Path) -> str:
-    stem = audio_path.stem.strip()
-    return stem or "recording"
-
-
-def _safe_float(value: object, default: float = 0.0) -> float:
-    try:
-        parsed = float(value)
-    except (TypeError, ValueError):
-        return default
-    if not math.isfinite(parsed):
-        return default
-    return parsed
-
-
-def _normalise_word(word: dict[str, Any], seg_start: float, seg_end: float) -> dict[str, Any] | None:
-    text = str(word.get("word") or word.get("text") or "").strip()
-    if not text:
-        return None
-    start = _safe_float(word.get("start"), default=seg_start)
-    end = _safe_float(word.get("end"), default=max(start, seg_end))
-    if end < start:
-        end = start
-    return {
-        "start": round(start, 3),
-        "end": round(end, 3),
-        "word": text,
-    }
-
-
-def _normalise_asr_segments(raw_segments: Sequence[dict[str, Any]]) -> list[dict[str, Any]]:
-    out: list[dict[str, Any]] = []
-    for idx, raw in enumerate(raw_segments):
-        start = _safe_float(raw.get("start"), default=float(idx))
-        end = _safe_float(raw.get("end"), default=start)
-        if end < start:
-            end = start
-        text = str(raw.get("text") or "").strip()
-        words_raw = raw.get("words")
-        words: list[dict[str, Any]] = []
-        if isinstance(words_raw, list):
-            for word in words_raw:
-                if not isinstance(word, dict):
-                    continue
-                normalised = _normalise_word(word, start, end)
-                if normalised is not None:
-                    words.append(normalised)
-        if not words and text:
-            words = [{"start": round(start, 3), "end": round(end, 3), "word": text}]
-        payload: dict[str, Any] = {
-            "start": round(start, 3),
-            "end": round(end, 3),
-            "text": text,
-            "words": words,
-        }
-        language = raw.get("language")
-        if isinstance(language, str) and language.strip():
-            payload["language"] = language.strip()
-        out.append(payload)
-    return out
-
-
-_LANGUAGE_NAME_MAP: dict[str, str] = {
-    "ar": "Arabic",
-    "de": "German",
-    "en": "English",
-    "es": "Spanish",
-    "fr": "French",
-    "hi": "Hindi",
-    "it": "Italian",
-    "ja": "Japanese",
-    "ko": "Korean",
-    "nl": "Dutch",
-    "pl": "Polish",
-    "pt": "Portuguese",
-    "ru": "Russian",
-    "tr": "Turkish",
-    "uk": "Ukrainian",
-    "zh": "Chinese",
-}
-
-_LANGUAGE_CODE_MAP: dict[str, str] = {
-    "eng": "en",
-    "spa": "es",
-    "fra": "fr",
-    "fre": "fr",
-    "deu": "de",
-    "ger": "de",
-    "ita": "it",
-    "por": "pt",
-    "rus": "ru",
-    "ukr": "uk",
-    "jpn": "ja",
-    "kor": "ko",
-    "zho": "zh",
-    "chi": "zh",
-}
-
-_EN_STOPWORDS = {
-    "the",
-    "and",
-    "to",
-    "of",
-    "in",
-    "for",
-    "with",
-    "on",
-    "is",
-    "are",
-    "we",
-    "you",
-    "hello",
-    "thanks",
-    "meeting",
-    "team",
-    "today",
-}
-
-_ES_STOPWORDS = {
-    "el",
-    "la",
-    "los",
-    "las",
-    "de",
-    "que",
-    "y",
-    "en",
-    "para",
-    "con",
-    "es",
-    "somos",
-    "hola",
-    "gracias",
-    "reunion",
-    "equipo",
-    "hoy",
-}
-
-
-def _normalise_language_code(value: object | None) -> str | None:
-    if not isinstance(value, str):
-        return None
-    raw = value.strip().lower()
-    if not raw:
-        return None
-    cleaned = raw.replace("_", "-")
-    base = cleaned.split("-", 1)[0]
-    if not base.isalpha():
-        return None
-    if len(base) == 2:
-        return base
-    if len(base) == 3:
-        return _LANGUAGE_CODE_MAP.get(base, None)
-    return None
-
-
-def _language_name(code: str) -> str:
-    return _LANGUAGE_NAME_MAP.get(code, code.upper())
-
-
-def _guess_language_from_text(text: str) -> str | None:
-    sample = text.strip().lower()
-    if not sample:
-        return None
-    tokens = re.findall(r"[a-zA-Z\u00c0-\u017f]+", sample)
-    if not tokens:
-        return None
-    en_score = sum(1 for token in tokens if token in _EN_STOPWORDS)
-    es_score = sum(1 for token in tokens if token in _ES_STOPWORDS)
-    if any(ch in sample for ch in "áéíóúñ¿¡"):
-        es_score += 2
-    if en_score == 0 and es_score == 0:
-        return None
-    if es_score > en_score:
-        return "es"
-    if en_score > es_score:
-        return "en"
-    return None
-
-
-def _segment_language(
-    segment: dict[str, Any],
-    *,
-    detected_language: str | None,
-    transcript_language_override: str | None,
-) -> str:
-    if transcript_language_override:
-        return transcript_language_override
-    seg_language = _normalise_language_code(segment.get("language"))
-    if seg_language:
-        return seg_language
-    if detected_language:
-        return detected_language
-    text_language = _guess_language_from_text(str(segment.get("text") or ""))
-    if text_language:
-        return text_language
-    return "unknown"
-
-
-def _duration_weight(start: float, end: float, text: str) -> float:
-    duration = max(0.0, end - start)
-    if duration > 0:
-        return duration
-    tokens = max(len(text.split()), 1)
-    return tokens * 0.01
-
-
-def _language_stats(
-    asr_segments: Sequence[dict[str, Any]],
-    *,
-    detected_language: str | None,
-    transcript_language_override: str | None,
-) -> tuple[list[dict[str, Any]], str, dict[str, float], list[dict[str, Any]]]:
-    if not asr_segments:
-        fallback = transcript_language_override or detected_language or "unknown"
-        return [], fallback, {}, []
-
-    enriched = sorted(
-        (dict(seg) for seg in asr_segments),
-        key=lambda row: (
-            _safe_float(row.get("start"), default=0.0),
-            _safe_float(row.get("end"), default=0.0),
-        ),
-    )
-
-    weighted_totals: dict[str, float] = {}
-    spans: list[dict[str, Any]] = []
-    for segment in enriched:
-        start = _safe_float(segment.get("start"), default=0.0)
-        end = _safe_float(segment.get("end"), default=start)
-        if end < start:
-            end = start
-        text = str(segment.get("text") or "").strip()
-        lang = _segment_language(
-            segment,
-            detected_language=detected_language,
-            transcript_language_override=transcript_language_override,
-        )
-        segment["language"] = lang
-
-        weight = _duration_weight(start, end, text)
-        weighted_totals[lang] = weighted_totals.get(lang, 0.0) + weight
-
-        if spans and spans[-1]["lang"] == lang and start <= _safe_float(spans[-1]["end"]) + 0.5:
-            spans[-1]["end"] = round(max(_safe_float(spans[-1]["end"]), end), 3)
-        else:
-            spans.append(
-                {
-                    "start": round(start, 3),
-                    "end": round(end, 3),
-                    "lang": lang,
-                }
-            )
-
-    ordered = sorted(weighted_totals.items(), key=lambda row: (-row[1], row[0]))
-    dominant = "unknown"
-    for lang, _weight in ordered:
-        if lang != "unknown":
-            dominant = lang
-            break
-    if dominant == "unknown" and ordered:
-        dominant = ordered[0][0]
-
-    total_weight = sum(weighted_totals.values())
-    distribution: dict[str, float] = {}
-    if total_weight > 0:
-        for lang, weight in ordered:
-            distribution[lang] = round((weight / total_weight) * 100.0, 2)
-
-    return enriched, dominant, distribution, spans
-
-
-def _resolve_target_summary_language(
-    requested_language: str | None,
-    *,
-    dominant_language: str,
-    detected_language: str | None,
-) -> str:
-    requested = _normalise_language_code(requested_language)
-    if requested:
-        return requested
-    if dominant_language and dominant_language != "unknown":
-        return dominant_language
-    if detected_language:
-        return detected_language
-    return "en"
-
-
-_QUESTION_TYPE_KEYS = (
-    "open",
-    "yes_no",
-    "clarification",
-    "status",
-    "decision_seeking",
+from .pipeline_steps import orchestrator as _orchestrator
+from .pipeline_steps.orchestrator import (
+    Diariser,
+    PrecheckResult,
+    Settings,
+    build_structured_summary_prompts,
+    build_summary_payload,
+    build_summary_prompts,
+    refresh_aliases,
+    run_pipeline,
+    run_precheck,
 )
 
-
-def _chunk_text_for_prompt(text: str, *, max_chars: int = 500) -> list[str]:
-    normalized = " ".join(text.split())
-    if not normalized:
-        return []
-    if len(normalized) <= max_chars:
-        return [normalized]
-
-    chunks: list[str] = []
-    words = normalized.split(" ")
-    current: list[str] = []
-    current_len = 0
-
-    for word in words:
-        word_len = len(word)
-        if not current:
-            if word_len > max_chars:
-                for start in range(0, word_len, max_chars):
-                    chunks.append(word[start : start + max_chars])
-                continue
-            current = [word]
-            current_len = word_len
-            continue
-
-        next_len = current_len + 1 + word_len
-        if next_len > max_chars:
-            chunks.append(" ".join(current))
-            current = [word]
-            current_len = word_len
-        else:
-            current.append(word)
-            current_len = next_len
-
-    if current:
-        chunks.append(" ".join(current))
-    return chunks
-
-
-def _normalise_prompt_speaker_turns(
-    speaker_turns: Sequence[dict[str, Any]],
-    *,
-    max_turns: int | None = None,
-) -> list[dict[str, Any]]:
-    out: list[dict[str, Any]] = []
-    for row in speaker_turns:
-        start = round(_safe_float(row.get("start"), default=0.0), 3)
-        end = round(_safe_float(row.get("end"), default=0.0), 3)
-        speaker = str(row.get("speaker") or "S1")
-        lang = _normalise_language_code(row.get("language"))
-        for chunk in _chunk_text_for_prompt(str(row.get("text") or "").strip()):
-            if max_turns is not None and max_turns >= 0 and len(out) >= max_turns:
-                return out
-            payload: dict[str, Any] = {
-                "start": start,
-                "end": end,
-                "speaker": speaker,
-                "text": chunk,
-            }
-            if lang:
-                payload["language"] = lang
-            out.append(payload)
-    return out
-
-
-def build_structured_summary_prompts(
-    speaker_turns: Sequence[dict[str, Any]],
-    target_summary_language: str,
-    *,
-    calendar_title: str | None = None,
-    calendar_attendees: Sequence[str] | None = None,
-) -> tuple[str, str]:
-    language_name = _language_name(target_summary_language)
-    sys_prompt = (
-        "You are an assistant that summarizes meeting transcripts. "
-        f"Write topic, summary_bullets, decisions, action_items, emotional_summary, and questions in {language_name}. "
-        "Keep names, quotes, and domain terms in their original language when needed. "
-        "Return strict JSON only, with no markdown fences."
-    )
-    prompt_payload = {
-        "target_summary_language": target_summary_language,
-        "calendar": {
-            "title": (calendar_title or "").strip() or None,
-            "attendees": [str(item).strip() for item in (calendar_attendees or []) if str(item).strip()],
-        },
-        "speaker_turns": _normalise_prompt_speaker_turns(speaker_turns),
-        "required_schema": {
-            "topic": "string",
-            "summary_bullets": ["string"],
-            "decisions": ["string"],
-            "action_items": [
-                {
-                    "task": "string",
-                    "owner": "string|null",
-                    "deadline": "string|null",
-                    "confidence": "number [0,1]",
-                }
-            ],
-            "emotional_summary": "1-3 short lines as a string",
-            "questions": {
-                "total_count": "integer >= 0",
-                "types": {key: "integer >= 0" for key in _QUESTION_TYPE_KEYS},
-                "extracted": ["string"],
-            },
-        },
-    }
-    user_prompt = json.dumps(prompt_payload, ensure_ascii=False, indent=2)
-    return sys_prompt, user_prompt
-
-
-def build_summary_prompts(clean_text: str, target_summary_language: str) -> tuple[str, str]:
-    pseudo_turns = []
-    stripped = clean_text.strip()
-    if stripped:
-        pseudo_turns.append({"start": 0.0, "end": 0.0, "speaker": "S1", "text": stripped})
-    return build_structured_summary_prompts(
-        pseudo_turns,
-        target_summary_language,
-    )
-
-
-def _normalise_text_list(value: Any, *, max_items: int) -> list[str]:
-    rows: list[Any]
-    if isinstance(value, list):
-        rows = value
-    elif isinstance(value, str):
-        rows = [line.strip() for line in value.splitlines() if line.strip()]
-    else:
-        return []
-
-    out: list[str] = []
-    for item in rows:
-        if len(out) >= max_items:
-            break
-        text = str(item).strip()
-        if not text:
-            continue
-        text = re.sub(r"^[\-\*\u2022]+\s*", "", text).strip()
-        if text:
-            out.append(text)
-    return out
-
-
-def _extract_json_dict(raw_content: str) -> dict[str, Any] | None:
-    text = raw_content.strip()
-    if not text:
-        return None
-
-    candidates: list[str] = [text]
-    fenced_matches = re.findall(r"```(?:json)?\s*(.*?)```", text, flags=re.IGNORECASE | re.DOTALL)
-    candidates.extend(match.strip() for match in fenced_matches if match.strip())
-
-    first_brace = text.find("{")
-    last_brace = text.rfind("}")
-    if first_brace != -1 and last_brace > first_brace:
-        candidates.append(text[first_brace : last_brace + 1].strip())
-
-    seen: set[str] = set()
-    for candidate in candidates:
-        if candidate in seen:
-            continue
-        seen.add(candidate)
-        try:
-            payload = json.loads(candidate)
-        except ValueError:
-            continue
-        if isinstance(payload, dict):
-            return payload
-    return None
-
-
-def _normalise_confidence(value: Any, *, default: float = 0.5) -> float:
-    try:
-        confidence = float(value)
-    except (TypeError, ValueError):
-        confidence = default
-    if not math.isfinite(confidence):
-        confidence = default
-    return round(min(max(confidence, 0.0), 1.0), 2)
-
-
-def _normalise_action_items(value: Any) -> list[dict[str, Any]]:
-    rows: list[Any]
-    if isinstance(value, list):
-        rows = value
-    elif value is None:
-        rows = []
-    else:
-        rows = [value]
-
-    out: list[dict[str, Any]] = []
-    for row in rows:
-        if len(out) >= 30:
-            break
-        if isinstance(row, dict):
-            task = str(row.get("task") or row.get("action") or row.get("title") or "").strip()
-            owner_raw = row.get("owner")
-            deadline_raw = row.get("deadline") or row.get("due")
-            confidence_raw = row.get("confidence", row.get("score"))
-        else:
-            task = str(row).strip()
-            owner_raw = None
-            deadline_raw = None
-            confidence_raw = None
-        if not task:
-            continue
-        owner = str(owner_raw).strip() if owner_raw is not None else ""
-        deadline = str(deadline_raw).strip() if deadline_raw is not None else ""
-        out.append(
-            {
-                "task": task,
-                "owner": owner or None,
-                "deadline": deadline or None,
-                "confidence": _normalise_confidence(confidence_raw),
-            }
-        )
-    return out
-
-
-def _normalise_question_types(value: Any) -> dict[str, int]:
-    out = {key: 0 for key in _QUESTION_TYPE_KEYS}
-    if not isinstance(value, dict):
-        return out
-    for key in _QUESTION_TYPE_KEYS:
-        out[key] = max(0, int(_safe_float(value.get(key), default=0.0)))
-    return out
-
-
-def _normalise_questions(value: Any) -> dict[str, Any]:
-    total_count = 0
-    question_types = {key: 0 for key in _QUESTION_TYPE_KEYS}
-    extracted: list[str] = []
-
-    if isinstance(value, dict):
-        total_count = max(0, int(_safe_float(value.get("total_count"), default=0.0)))
-        question_types = _normalise_question_types(value.get("types"))
-        if sum(question_types.values()) == 0:
-            question_types = _normalise_question_types(value)
-        extracted = _normalise_text_list(value.get("extracted"), max_items=20)
-
-    inferred_total = max(sum(question_types.values()), len(extracted))
-    if total_count == 0:
-        total_count = inferred_total
-
-    return {
-        "total_count": total_count,
-        "types": question_types,
-        "extracted": extracted,
-    }
-
-
-def _normalise_emotional_summary(value: Any) -> str:
-    if isinstance(value, str):
-        lines = [line.strip() for line in value.splitlines() if line.strip()]
-    elif isinstance(value, list):
-        lines = _normalise_text_list(value, max_items=3)
-    else:
-        lines = []
-    if not lines:
-        lines = ["Neutral and focused discussion."]
-    return "\n".join(lines[:3])
-
-
-def _summary_text_from_bullets(summary_bullets: Sequence[str]) -> str:
-    return "\n".join(f"- {bullet}" for bullet in summary_bullets)
-
-
-def _build_structured_summary_payload(
-    *,
-    model: str,
-    target_summary_language: str,
-    friendly: int,
-    topic: str,
-    summary_bullets: Sequence[str],
-    decisions: Sequence[str],
-    action_items: Sequence[dict[str, Any]],
-    emotional_summary: str,
-    questions: dict[str, Any],
-    status: str | None = None,
-    reason: str | None = None,
-    error: str | None = None,
-) -> dict[str, Any]:
-    payload: dict[str, Any] = {
-        "friendly": int(friendly),
-        "model": model,
-        "target_summary_language": target_summary_language,
-        "topic": topic,
-        "summary_bullets": list(summary_bullets),
-        "summary": _summary_text_from_bullets(summary_bullets),
-        "decisions": list(decisions),
-        "action_items": list(action_items),
-        "emotional_summary": emotional_summary,
-        "questions": questions,
-    }
-    if status:
-        payload["status"] = status
-    if reason:
-        payload["reason"] = reason
-    if error:
-        payload["error"] = error
-    return payload
-
-
-def build_summary_payload(
-    *,
-    raw_llm_content: str,
-    model: str,
-    target_summary_language: str,
-    friendly: int,
-    default_topic: str = "Meeting summary",
-) -> dict[str, Any]:
-    parsed = _extract_json_dict(raw_llm_content) or {}
-
-    summary_bullets = _normalise_text_list(parsed.get("summary_bullets"), max_items=12)
-    if not summary_bullets:
-        summary_bullets = _normalise_text_list(parsed.get("summary"), max_items=12)
-    if not summary_bullets:
-        summary_bullets = _normalise_text_list(raw_llm_content, max_items=12)
-    if not summary_bullets:
-        summary_bullets = ["No summary available."]
-
-    topic = str(parsed.get("topic") or "").strip()
-    if not topic:
-        topic = summary_bullets[0][:120] if summary_bullets else default_topic
-    topic = topic or default_topic
-
-    return _build_structured_summary_payload(
-        model=model,
-        target_summary_language=target_summary_language,
-        friendly=friendly,
-        topic=topic,
-        summary_bullets=summary_bullets,
-        decisions=_normalise_text_list(parsed.get("decisions"), max_items=20),
-        action_items=_normalise_action_items(parsed.get("action_items")),
-        emotional_summary=_normalise_emotional_summary(parsed.get("emotional_summary")),
-        questions=_normalise_questions(parsed.get("questions")),
-    )
-
-
-def _language_payload(info: dict[str, Any]) -> dict[str, Any]:
-    detected_raw = str(
-        info.get("language")
-        or info.get("detected_language")
-        or info.get("lang")
-        or "unknown"
-    )
-    detected = _normalise_language_code(detected_raw) or "unknown"
-    confidence_raw = None
-    for key in (
-        "language_probability",
-        "language_confidence",
-        "language_score",
-        "probability",
-    ):
-        if key in info and info[key] is not None:
-            confidence_raw = info[key]
-            break
-    confidence = None
-    if confidence_raw is not None:
-        confidence = round(_safe_float(confidence_raw, default=0.0), 4)
-    return {"detected": detected, "confidence": confidence}
-
-
-def _safe_diarization_segments(diarization: Any) -> list[dict[str, Any]]:
-    out: list[dict[str, Any]] = []
-    if diarization is None or not hasattr(diarization, "itertracks"):
-        return out
-    tracks = diarization.itertracks(yield_label=True)
-    for item in tracks:
-        if not isinstance(item, tuple) or len(item) < 2:
-            continue
-        seg = item[0]
-        if len(item) == 2:
-            label = item[1]
-        else:
-            label = item[-1]
-        start = _safe_float(getattr(seg, "start", 0.0), default=0.0)
-        end = _safe_float(getattr(seg, "end", start), default=start)
-        if end < start:
-            end = start
-        out.append(
-            {
-                "start": round(start, 3),
-                "end": round(end, 3),
-                "speaker": str(label),
-            }
-        )
-    out.sort(key=lambda row: (row["start"], row["end"], row["speaker"]))
-    return out
-
-
-def _overlap_seconds(
-    left_start: float,
-    left_end: float,
-    right_start: float,
-    right_end: float,
-) -> float:
-    return max(0.0, min(left_end, right_end) - max(left_start, right_start))
-
-
-def _pick_speaker(start: float, end: float, diar_segments: Sequence[dict[str, Any]]) -> str:
-    if not diar_segments:
-        return "S1"
-    best_speaker = str(diar_segments[0]["speaker"])
-    best_overlap = -1.0
-    midpoint = (start + end) / 2.0
-    best_distance = float("inf")
-    for seg in diar_segments:
-        d_start = _safe_float(seg.get("start"), default=0.0)
-        d_end = _safe_float(seg.get("end"), default=d_start)
-        overlap = _overlap_seconds(start, end, d_start, d_end)
-        if overlap > best_overlap:
-            best_overlap = overlap
-            best_speaker = str(seg.get("speaker", "S1"))
-        if overlap == 0.0:
-            if midpoint < d_start:
-                distance = d_start - midpoint
-            elif midpoint > d_end:
-                distance = midpoint - d_end
-            else:
-                distance = 0.0
-            if best_overlap <= 0.0 and distance < best_distance:
-                best_distance = distance
-                best_speaker = str(seg.get("speaker", "S1"))
-    return best_speaker
-
-
-def _words_from_segments(
-    asr_segments: Sequence[dict[str, Any]],
-    *,
-    default_language: str | None,
-) -> list[dict[str, Any]]:
-    words: list[dict[str, Any]] = []
-    for seg in asr_segments:
-        seg_start = _safe_float(seg.get("start"), default=0.0)
-        seg_end = _safe_float(seg.get("end"), default=seg_start)
-        seg_language = seg.get("language")
-        language = (
-            str(seg_language)
-            if isinstance(seg_language, str) and seg_language.strip()
-            else default_language
-        )
-        seg_words = seg.get("words")
-        if not isinstance(seg_words, list):
-            seg_words = []
-        if not seg_words and seg.get("text"):
-            seg_words = [
-                {
-                    "start": seg_start,
-                    "end": seg_end,
-                    "word": str(seg.get("text")),
-                }
-            ]
-        for raw_word in seg_words:
-            if not isinstance(raw_word, dict):
-                continue
-            text = str(raw_word.get("word") or "").strip()
-            if not text:
-                continue
-            start = _safe_float(raw_word.get("start"), default=seg_start)
-            end = _safe_float(raw_word.get("end"), default=max(start, seg_end))
-            if end < start:
-                end = start
-            payload: dict[str, Any] = {
-                "start": round(start, 3),
-                "end": round(end, 3),
-                "word": text,
-            }
-            if language:
-                payload["language"] = language
-            words.append(payload)
-    words.sort(key=lambda row: (row["start"], row["end"], row["word"]))
-    return words
-
-
-def _build_speaker_turns(
-    asr_segments: Sequence[dict[str, Any]],
-    diar_segments: Sequence[dict[str, Any]],
-    *,
-    default_language: str | None,
-) -> list[dict[str, Any]]:
-    words = _words_from_segments(asr_segments, default_language=default_language)
-    if not words:
-        return []
-
-    turns: list[dict[str, Any]] = []
-    current: dict[str, Any] | None = None
-    for word in words:
-        start = _safe_float(word.get("start"), default=0.0)
-        end = _safe_float(word.get("end"), default=start)
-        speaker = _pick_speaker(start, end, diar_segments)
-        language = word.get("language")
-        if (
-            current is not None
-            and current["speaker"] == speaker
-            and start - _safe_float(current["end"], default=start) <= 1.0
-        ):
-            current["end"] = round(max(_safe_float(current["end"]), end), 3)
-            current["text"] = f"{current['text']} {word['word']}".strip()
-        else:
-            if current is not None:
-                turns.append(current)
-            current = {
-                "start": round(start, 3),
-                "end": round(end, 3),
-                "speaker": speaker,
-                "text": str(word["word"]),
-            }
-            if isinstance(language, str) and language:
-                current["language"] = language
-    if current is not None:
-        turns.append(current)
-    return turns
-
-
-def _speaker_slug(label: str) -> str:
-    slug = "".join(ch if ch.isalnum() or ch in {"-", "_"} else "_" for ch in label)
-    slug = slug.strip("_")
-    return slug or "speaker"
-
-
-def _clear_dir(path: Path) -> None:
-    if not path.exists():
-        path.mkdir(parents=True, exist_ok=True)
-        return
-    for child in path.iterdir():
-        if child.is_dir():
-            shutil.rmtree(child)
-        else:
-            child.unlink(missing_ok=True)
-
-
-def _snippet_window(
-    start: float,
-    end: float,
-    *,
-    duration_sec: float | None,
-) -> tuple[float, float]:
-    seg_duration = max(0.0, end - start)
-    target = min(20.0, max(10.0, seg_duration if seg_duration > 0 else 10.0))
-    center = start + (seg_duration / 2.0 if seg_duration > 0 else 0.0)
-    clip_start = max(0.0, center - (target / 2.0))
-    clip_end = clip_start + target
-    if duration_sec is not None and duration_sec > 0:
-        if clip_end > duration_sec:
-            clip_end = duration_sec
-            clip_start = max(0.0, clip_end - target)
-    if clip_end <= clip_start:
-        clip_end = clip_start + min(target, 1.0)
-    return round(clip_start, 3), round(clip_end, 3)
-
-
-def _extract_wav_snippet_with_wave(
-    audio_path: Path,
-    out_path: Path,
-    *,
-    start_sec: float,
-    end_sec: float,
-) -> bool:
-    if audio_path.suffix.lower() != ".wav":
-        return False
-    try:
-        with wave.open(str(audio_path), "rb") as src:
-            rate = src.getframerate()
-            channels = src.getnchannels()
-            sampwidth = src.getsampwidth()
-            start_frame = max(0, int(start_sec * rate))
-            end_frame = max(start_frame + 1, int(end_sec * rate))
-            src.setpos(min(start_frame, src.getnframes()))
-            frames = src.readframes(max(0, end_frame - start_frame))
-        if not frames:
-            return False
-        out_path.parent.mkdir(parents=True, exist_ok=True)
-        with wave.open(str(out_path), "wb") as dst:
-            dst.setnchannels(channels)
-            dst.setsampwidth(sampwidth)
-            dst.setframerate(rate)
-            dst.writeframes(frames)
-        return True
-    except Exception:
-        return False
-
-
-def _extract_wav_snippet_with_ffmpeg(
-    audio_path: Path,
-    out_path: Path,
-    *,
-    start_sec: float,
-    end_sec: float,
-) -> bool:
-    ffmpeg = shutil.which("ffmpeg")
-    if ffmpeg is None:
-        return False
-    duration = max(0.1, end_sec - start_sec)
-    out_path.parent.mkdir(parents=True, exist_ok=True)
-    cmd = [
-        ffmpeg,
-        "-nostdin",
-        "-hide_banner",
-        "-loglevel",
-        "error",
-        "-y",
-        "-ss",
-        f"{start_sec:.3f}",
-        "-t",
-        f"{duration:.3f}",
-        "-i",
-        str(audio_path),
-        "-ac",
-        "1",
-        "-ar",
-        "16000",
-        "-c:a",
-        "pcm_s16le",
-        str(out_path),
-    ]
-    try:
-        proc = subprocess.run(cmd, check=False, capture_output=True, text=True)
-    except Exception:
-        return False
-    return proc.returncode == 0 and out_path.exists() and out_path.stat().st_size > 44
-
-
-def _write_silence_wav(path: Path, duration_sec: float = 1.0) -> None:
-    samples = max(int(16000 * max(duration_sec, 0.1)), 1)
-    payload = b"\x00\x00" * samples
-    path.parent.mkdir(parents=True, exist_ok=True)
-    with wave.open(str(path), "wb") as wav_out:
-        wav_out.setnchannels(1)
-        wav_out.setsampwidth(2)
-        wav_out.setframerate(16000)
-        wav_out.writeframes(payload)
-
-
-def _export_speaker_snippets(
-    *,
-    audio_path: Path,
-    diar_segments: Sequence[dict[str, Any]],
-    snippets_dir: Path,
-    duration_sec: float | None,
-) -> list[Path]:
-    _clear_dir(snippets_dir)
-
-    by_speaker: dict[str, list[dict[str, Any]]] = {}
-    for segment in diar_segments:
-        speaker = str(segment.get("speaker", "S1"))
-        by_speaker.setdefault(speaker, []).append(segment)
-
-    all_outputs: list[Path] = []
-    for speaker in sorted(by_speaker):
-        ranked = sorted(
-            by_speaker[speaker],
-            key=lambda row: (
-                -(_safe_float(row.get("end")) - _safe_float(row.get("start"))),
-                _safe_float(row.get("start")),
-                _safe_float(row.get("end")),
-            ),
-        )
-        chosen: list[dict[str, Any]] = []
-        for candidate in ranked:
-            start = _safe_float(candidate.get("start"))
-            end = _safe_float(candidate.get("end"), default=start)
-            if end <= start:
-                continue
-            overlaps = any(
-                _overlap_seconds(
-                    start,
-                    end,
-                    _safe_float(existing.get("start")),
-                    _safe_float(existing.get("end")),
-                )
-                > 0.5
-                for existing in chosen
-            )
-            if overlaps:
-                continue
-            chosen.append(candidate)
-            if len(chosen) == 3:
-                break
-        if len(chosen) < 2:
-            for candidate in ranked:
-                if candidate in chosen:
-                    continue
-                chosen.append(candidate)
-                if len(chosen) == 2:
-                    break
-
-        speaker_dir = snippets_dir / _speaker_slug(speaker)
-        for idx, segment in enumerate(
-            sorted(chosen, key=lambda row: _safe_float(row.get("start"))), start=1
-        ):
-            seg_start = _safe_float(segment.get("start"))
-            seg_end = _safe_float(segment.get("end"), default=seg_start)
-            clip_start, clip_end = _snippet_window(
-                seg_start,
-                seg_end,
-                duration_sec=duration_sec,
-            )
-            out_path = speaker_dir / f"{idx}.wav"
-            written = _extract_wav_snippet_with_wave(
-                audio_path,
-                out_path,
-                start_sec=clip_start,
-                end_sec=clip_end,
-            )
-            if not written:
-                written = _extract_wav_snippet_with_ffmpeg(
-                    audio_path,
-                    out_path,
-                    start_sec=clip_start,
-                    end_sec=clip_end,
-                )
-            if not written:
-                _write_silence_wav(out_path, duration_sec=min(max(clip_end - clip_start, 1.0), 2.0))
-            all_outputs.append(out_path)
-    all_outputs.sort()
-    return all_outputs
-
-
-def _audio_duration_from_wave(audio_path: Path) -> float | None:
-    if audio_path.suffix.lower() != ".wav":
-        return None
-    try:
-        with wave.open(str(audio_path), "rb") as src:
-            rate = src.getframerate()
-            frames = src.getnframes()
-        if rate <= 0:
-            return None
-        return frames / float(rate)
-    except Exception:
-        return None
-
-
-def _audio_duration_from_ffprobe(audio_path: Path) -> float | None:
-    ffprobe = shutil.which("ffprobe")
-    if ffprobe is None:
-        return None
-    cmd = [
-        ffprobe,
-        "-v",
-        "error",
-        "-show_entries",
-        "format=duration",
-        "-of",
-        "default=noprint_wrappers=1:nokey=1",
-        str(audio_path),
-    ]
-    try:
-        proc = subprocess.run(cmd, check=False, capture_output=True, text=True)
-    except Exception:
-        return None
-    if proc.returncode != 0:
-        return None
-    raw = proc.stdout.strip()
-    if not raw:
-        return None
-    try:
-        value = float(raw)
-    except ValueError:
-        return None
-    if value <= 0:
-        return None
-    return value
-
-
-def _speech_ratio_from_wave(audio_path: Path) -> float | None:
-    if audio_path.suffix.lower() != ".wav":
-        return None
-    try:
-        with wave.open(str(audio_path), "rb") as src:
-            rate = src.getframerate()
-            channels = src.getnchannels()
-            sample_width = src.getsampwidth()
-            frame_samples = max(int(rate * 0.03), 1)
-            frame_bytes = frame_samples * channels * sample_width
-            voiced = 0
-            total = 0
-            while True:
-                chunk = src.readframes(frame_samples)
-                if not chunk:
-                    break
-                if len(chunk) < frame_bytes // 2:
-                    break
-                total += 1
-                if audioop.rms(chunk, sample_width) >= 350:
-                    voiced += 1
-            if total == 0:
-                return 0.0
-            return voiced / float(total)
-    except Exception:
-        return None
-
-
-def _speech_ratio_from_ffmpeg(audio_path: Path) -> float | None:
-    ffmpeg = shutil.which("ffmpeg")
-    if ffmpeg is None:
-        return None
-    cmd = [
-        ffmpeg,
-        "-nostdin",
-        "-hide_banner",
-        "-loglevel",
-        "error",
-        "-i",
-        str(audio_path),
-        "-f",
-        "s16le",
-        "-ac",
-        "1",
-        "-ar",
-        "16000",
-        "-",
-    ]
-    try:
-        with subprocess.Popen(
-            cmd,
-            stdout=subprocess.PIPE,
-            stderr=subprocess.DEVNULL,
-        ) as proc:
-            if proc.stdout is None:
-                return None
-            frame_bytes = 960  # 30ms @ 16kHz * 2 bytes sample
-            voiced = 0
-            total = 0
-            while True:
-                chunk = proc.stdout.read(frame_bytes)
-                if not chunk:
-                    break
-                if len(chunk) < frame_bytes:
-                    break
-                total += 1
-                if audioop.rms(chunk, 2) >= 300:
-                    voiced += 1
-            # Avoid a fixed wait cap; long/slower ffmpeg runs should not
-            # downgrade valid audio into "metrics unavailable" quarantine.
-            proc.wait()
-            if proc.returncode != 0:
-                return None
-            if total == 0:
-                return 0.0
-            return voiced / float(total)
-    except Exception:
-        return None
-
-
-def run_precheck(audio_path: Path, cfg: Settings | None = None) -> PrecheckResult:
-    """Compute duration + VAD speech ratio and decide quarantine status."""
-    settings = cfg or Settings()
-    duration_sec = _audio_duration_from_wave(audio_path)
-    if duration_sec is None:
-        duration_sec = _audio_duration_from_ffprobe(audio_path)
-
-    speech_ratio = _speech_ratio_from_wave(audio_path)
-    if speech_ratio is None:
-        speech_ratio = _speech_ratio_from_ffmpeg(audio_path)
-
-    quarantine_reason: str | None = None
-    if duration_sec is None or speech_ratio is None:
-        quarantine_reason = "precheck_metrics_unavailable"
-    elif duration_sec < settings.precheck_min_duration_sec:
-        quarantine_reason = f"duration_lt_{settings.precheck_min_duration_sec:.0f}s"
-    elif speech_ratio < settings.precheck_min_speech_ratio:
-        quarantine_reason = (
-            f"speech_ratio_lt_{settings.precheck_min_speech_ratio:.2f}"
-        )
-
-    return PrecheckResult(
-        duration_sec=round(duration_sec, 3) if duration_sec is not None else None,
-        speech_ratio=round(speech_ratio, 4) if speech_ratio is not None else None,
-        quarantine_reason=quarantine_reason,
-    )
-
-
-def _fallback_diarization(duration_sec: float | None) -> Any:
-    duration = max(duration_sec or 0.0, 0.1)
-
-    class _Annotation:
-        def itertracks(self, yield_label: bool = False):
-            if yield_label:
-                yield SimpleNamespace(start=0.0, end=duration), "S1"
-            else:  # pragma: no cover - legacy compatibility branch
-                yield (SimpleNamespace(start=0.0, end=duration),)
-
-    return _Annotation()
-
-
-async def run_pipeline(
-    audio_path: Path,
-    cfg: Settings,
-    llm: LLMClient,
-    diariser: Diariser,
-    recording_id: str | None = None,
-    precheck: PrecheckResult | None = None,
-    target_summary_language: str | None = None,
-    transcript_language_override: str | None = None,
-    calendar_title: str | None = None,
-    calendar_attendees: Sequence[str] | None = None,
-) -> TranscriptResult:
-    """Transcribe ``audio_path`` and return a structured result."""
-    start = time.perf_counter()
-
-    artifact_paths = build_recording_artifacts(
-        cfg.recordings_root,
-        recording_id=recording_id or _default_recording_id(audio_path),
-        audio_ext=audio_path.suffix,
-    )
-    stage_raw_audio(audio_path, artifact_paths.raw_audio_path)
-
-    precheck_result = precheck or run_precheck(audio_path, cfg)
-    normalized_transcript_language_override = _normalise_language_code(
-        transcript_language_override
-    )
-    resolved_summary_language = _resolve_target_summary_language(
-        target_summary_language,
-        dominant_language=normalized_transcript_language_override or "unknown",
-        detected_language=None,
-    )
-    normalized_calendar_title = str(calendar_title or "").strip() or None
-    normalized_calendar_attendees = [
-        str(attendee).strip()
-        for attendee in (calendar_attendees or [])
-        if str(attendee).strip()
-    ]
-
-    atomic_write_json(
-        artifact_paths.metrics_json_path,
-        {
-            "status": "running",
-            "version": 1,
-            "precheck": {
-                "duration_sec": precheck_result.duration_sec,
-                "speech_ratio": precheck_result.speech_ratio,
-                "quarantine_reason": precheck_result.quarantine_reason,
-            },
-        },
-    )
-
-    if precheck_result.quarantine_reason:
-        _clear_dir(artifact_paths.snippets_dir)
-        atomic_write_text(artifact_paths.transcript_txt_path, "")
-        atomic_write_json(
-            artifact_paths.transcript_json_path,
-            {
-                "recording_id": artifact_paths.recording_id,
-                "language": {"detected": "unknown", "confidence": None},
-                "dominant_language": normalized_transcript_language_override or "unknown",
-                "language_distribution": {},
-                "language_spans": [],
-                "target_summary_language": resolved_summary_language,
-                "transcript_language_override": normalized_transcript_language_override,
-                "calendar_title": normalized_calendar_title,
-                "calendar_attendees": normalized_calendar_attendees,
-                "segments": [],
-                "speakers": [],
-                "text": "",
-            },
-        )
-        atomic_write_json(artifact_paths.segments_json_path, [])
-        atomic_write_json(artifact_paths.speaker_turns_json_path, [])
-        atomic_write_json(
-            artifact_paths.summary_json_path,
-            _build_structured_summary_payload(
-                model=cfg.llm_model,
-                target_summary_language=resolved_summary_language,
-                friendly=0,
-                topic="Quarantined recording",
-                summary_bullets=["Recording was quarantined before transcription."],
-                decisions=[],
-                action_items=[],
-                emotional_summary="No emotional summary available.",
-                questions=_normalise_questions(None),
-                status="quarantined",
-                reason=precheck_result.quarantine_reason,
-            ),
-        )
-        atomic_write_json(
-            artifact_paths.metrics_json_path,
-            {
-                "status": "quarantined",
-                "version": 1,
-                "precheck": {
-                    "duration_sec": precheck_result.duration_sec,
-                    "speech_ratio": precheck_result.speech_ratio,
-                    "quarantine_reason": precheck_result.quarantine_reason,
-                },
-            },
-        )
-        p95_latency_seconds.observe(time.perf_counter() - start)
-        return TranscriptResult(
-            summary="Quarantined",
-            body="",
-            friendly=0,
-            speakers=[],
-            summary_path=artifact_paths.summary_json_path,
-            body_path=artifact_paths.transcript_txt_path,
-            unknown_chunks=[],
-            segments=[],
-        )
-
-    def _write_failed_artifacts(
-        exc: Exception,
-        *,
-        friendly_score: int = 0,
-        language_payload: dict[str, Any] | None = None,
-        asr_count: int = 0,
-        diar_count: int = 0,
-        speaker_turn_count: int = 0,
-    ) -> None:
-        atomic_write_json(
-            artifact_paths.summary_json_path,
-            _build_structured_summary_payload(
-                model=cfg.llm_model,
-                target_summary_language=resolved_summary_language,
-                friendly=friendly_score,
-                topic="Summary generation failed",
-                summary_bullets=["Unable to produce a summary due to a processing error."],
-                decisions=[],
-                action_items=[],
-                emotional_summary="No emotional summary available.",
-                questions=_normalise_questions(None),
-                status="failed",
-                error=str(exc) or exc.__class__.__name__,
-            ),
-        )
-        atomic_write_json(
-            artifact_paths.metrics_json_path,
-            {
-                "status": "failed",
-                "version": 1,
-                "precheck": {
-                    "duration_sec": precheck_result.duration_sec,
-                    "speech_ratio": precheck_result.speech_ratio,
-                    "quarantine_reason": None,
-                },
-                "language": language_payload or {"detected": "unknown", "confidence": None},
-                "asr_segments": asr_count,
-                "diar_segments": diar_count,
-                "speaker_turns": speaker_turn_count,
-                "error": str(exc) or exc.__class__.__name__,
-            },
-        )
-
-    try:
-        import whisperx
-
-        def _asr() -> tuple[list[dict[str, Any]], dict[str, Any]]:
-            asr_language = normalized_transcript_language_override or "auto"
-            kwargs: dict[str, Any] = {"vad_filter": True, "language": asr_language}
-            try:
-                segments, info = whisperx.transcribe(
-                    str(audio_path),
-                    word_timestamps=True,
-                    **kwargs,
-                )
-            except TypeError:
-                segments, info = whisperx.transcribe(str(audio_path), **kwargs)
-            return list(segments), dict(info or {})
-
-        asr_task = asyncio.to_thread(_asr)
-        diar_task = diariser(audio_path)
-        (raw_segments, info), diarization = await asyncio.gather(asr_task, diar_task)
-
-        asr_segments = _normalise_asr_segments(raw_segments)
-        language_info = _language_payload(info)
-        detected_language = (
-            _normalise_language_code(language_info["detected"])
-            if language_info["detected"] != "unknown"
-            else None
-        )
-        (
-            asr_segments,
-            dominant_language,
-            language_distribution,
-            language_spans,
-        ) = _language_stats(
-            asr_segments,
-            detected_language=detected_language,
-            transcript_language_override=normalized_transcript_language_override,
-        )
-        if language_info["detected"] == "unknown" and dominant_language != "unknown":
-            language_info["detected"] = dominant_language
-        resolved_summary_language = _resolve_target_summary_language(
-            target_summary_language,
-            dominant_language=dominant_language,
-            detected_language=detected_language,
-        )
-
-        asr_text = " ".join(seg.get("text", "").strip() for seg in asr_segments).strip()
-        clean_text = normalizer.dedup(asr_text)
-        diar_segments = _safe_diarization_segments(diarization)
-        if not diar_segments and asr_segments:
-            fallback_end = max(_safe_float(seg.get("end")) for seg in asr_segments)
-            diar_segments = [
-                {"start": 0.0, "end": round(max(fallback_end, 0.1), 3), "speaker": "S1"}
-            ]
-
-        speaker_turns = _build_speaker_turns(
-            asr_segments,
-            diar_segments,
-            default_language=dominant_language if dominant_language != "unknown" else detected_language,
-        )
-
-        aliases = _load_aliases(cfg.speaker_db)
-        for row in diar_segments:
-            label = str(row["speaker"])
-            aliases.setdefault(label, label)
-        _save_aliases(aliases, cfg.speaker_db)
-    except Exception as exc:
-        error_rate_total.inc()
-        _write_failed_artifacts(exc)
-        raise
-
-    if not clean_text:
-        _clear_dir(artifact_paths.snippets_dir)
-        atomic_write_text(artifact_paths.transcript_txt_path, "")
-        atomic_write_json(
-            artifact_paths.transcript_json_path,
-            {
-                "recording_id": artifact_paths.recording_id,
-                "language": language_info,
-                "dominant_language": dominant_language,
-                "language_distribution": language_distribution,
-                "language_spans": language_spans,
-                "target_summary_language": resolved_summary_language,
-                "transcript_language_override": normalized_transcript_language_override,
-                "calendar_title": normalized_calendar_title,
-                "calendar_attendees": normalized_calendar_attendees,
-                "segments": asr_segments,
-                "speakers": sorted({aliases.get(row["speaker"], row["speaker"]) for row in diar_segments}),
-                "text": "",
-            },
-        )
-        atomic_write_json(artifact_paths.segments_json_path, diar_segments)
-        atomic_write_json(artifact_paths.speaker_turns_json_path, speaker_turns)
-        atomic_write_json(
-            artifact_paths.summary_json_path,
-            _build_structured_summary_payload(
-                model=cfg.llm_model,
-                target_summary_language=resolved_summary_language,
-                friendly=0,
-                topic="No speech detected",
-                summary_bullets=["No speech detected."],
-                decisions=[],
-                action_items=[],
-                emotional_summary="No emotional summary available.",
-                questions=_normalise_questions(None),
-                status="no_speech",
-            ),
-        )
-        atomic_write_json(
-            artifact_paths.metrics_json_path,
-            {
-                "status": "no_speech",
-                "version": 1,
-                "precheck": {
-                    "duration_sec": precheck_result.duration_sec,
-                    "speech_ratio": precheck_result.speech_ratio,
-                    "quarantine_reason": None,
-                },
-                "language": language_info,
-                "asr_segments": len(asr_segments),
-                "diar_segments": len(diar_segments),
-                "speaker_turns": len(speaker_turns),
-            },
-        )
-        p95_latency_seconds.observe(time.perf_counter() - start)
-        return TranscriptResult(
-            summary="No speech detected",
-            body="",
-            friendly=0,
-            speakers=sorted({aliases.get(row["speaker"], row["speaker"]) for row in diar_segments}),
-            summary_path=artifact_paths.summary_json_path,
-            body_path=artifact_paths.transcript_txt_path,
-            unknown_chunks=[],
-            segments=[],
-        )
-
-    snippet_paths = _export_speaker_snippets(
-        audio_path=audio_path,
-        diar_segments=diar_segments,
-        snippets_dir=artifact_paths.snippets_dir,
-        duration_sec=precheck_result.duration_sec,
-    )
-
-    speaker_lines = [
-        f"[{turn['start']:.2f}-{turn['end']:.2f}] **{aliases.get(turn['speaker'], turn['speaker'])}:** {turn['text']}"
-        for turn in speaker_turns
-    ]
-    speaker_lines = _merge_similar(speaker_lines, cfg.merge_similar)
-
-    friendly = _sentiment_score(clean_text)
-    sys_prompt, user_prompt = build_structured_summary_prompts(
-        speaker_turns,
-        resolved_summary_language,
-        calendar_title=normalized_calendar_title,
-        calendar_attendees=normalized_calendar_attendees,
-    )
-
-    try:
-        msg = await llm.generate(
-            system_prompt=sys_prompt,
-            user_prompt=user_prompt,
-            model=cfg.llm_model,
-            response_format={"type": "json_object"},
-        )
-        raw_summary = msg.get("content", "") if isinstance(msg, dict) else str(msg)
-        summary_payload = build_summary_payload(
-            raw_llm_content=raw_summary,
-            model=cfg.llm_model,
-            target_summary_language=resolved_summary_language,
-            friendly=friendly,
-            default_topic=normalized_calendar_title or "Meeting summary",
-        )
-        summary = str(summary_payload.get("summary") or "")
-
-        serialised_segments = [
-            SpeakerSegment(
-                start=_safe_float(turn["start"]),
-                end=_safe_float(turn["end"]),
-                speaker=str(turn["speaker"]),
-                text=str(turn["text"]),
-            )
-            for turn in speaker_turns
-        ]
-        atomic_write_text(artifact_paths.transcript_txt_path, clean_text)
-        atomic_write_json(
-            artifact_paths.transcript_json_path,
-            {
-                "recording_id": artifact_paths.recording_id,
-                "language": language_info,
-                "dominant_language": dominant_language,
-                "language_distribution": language_distribution,
-                "language_spans": language_spans,
-                "target_summary_language": resolved_summary_language,
-                "transcript_language_override": normalized_transcript_language_override,
-                "calendar_title": normalized_calendar_title,
-                "calendar_attendees": normalized_calendar_attendees,
-                "segments": asr_segments,
-                "speaker_lines": speaker_lines,
-                "speakers": sorted(set(aliases.get(turn["speaker"], turn["speaker"]) for turn in speaker_turns)),
-                "text": clean_text,
-            },
-        )
-        atomic_write_json(artifact_paths.segments_json_path, diar_segments)
-        atomic_write_json(artifact_paths.speaker_turns_json_path, speaker_turns)
-        atomic_write_json(
-            artifact_paths.summary_json_path,
-            summary_payload,
-        )
-        atomic_write_json(
-            artifact_paths.metrics_json_path,
-            {
-                "status": "ok",
-                "version": 1,
-                "precheck": {
-                    "duration_sec": precheck_result.duration_sec,
-                    "speech_ratio": precheck_result.speech_ratio,
-                    "quarantine_reason": None,
-                },
-                "language": language_info,
-                "asr_segments": len(asr_segments),
-                "diar_segments": len(diar_segments),
-                "speaker_turns": len(speaker_turns),
-                "snippets": len(snippet_paths),
-            },
-        )
-
-        result = TranscriptResult(
-            summary=summary,
-            body=clean_text,
-            friendly=friendly,
-            speakers=sorted(set(aliases.get(turn["speaker"], turn["speaker"]) for turn in speaker_turns)),
-            summary_path=artifact_paths.summary_json_path,
-            body_path=artifact_paths.transcript_txt_path,
-            unknown_chunks=snippet_paths,
-            segments=serialised_segments,
-        )
-    except Exception as exc:
-        error_rate_total.inc()
-        _write_failed_artifacts(
-            exc,
-            friendly_score=friendly,
-            language_payload=language_info,
-            asr_count=len(asr_segments),
-            diar_count=len(diar_segments),
-            speaker_turn_count=len(speaker_turns),
-        )
-        raise
-    finally:
-        p95_latency_seconds.observe(time.perf_counter() - start)
-
-    return result
-
+_segment_language = _orchestrator._segment_language
 
 __all__ = [
     "run_pipeline",
diff --git a/tasks/QUEUE.md b/tasks/QUEUE.md
index 2cb4621..92c4dad 100644
--- a/tasks/QUEUE.md
+++ b/tasks/QUEUE.md
@@ -112,7 +112,7 @@ Queue (in order)
 - Depends on: PR-SECURITY-01
 
 21) PR-PIPELINE-MODULAR-01: Split pipeline.py into testable modules + consolidate utils + robust LLM parsing with schema and raw artifacts
-- Status: TODO
+- Status: DONE
 - Tasks file: tasks/PR-PIPELINE-MODULAR-01.md
 - Depends on: PR-WORKER-ROBUST-01
 
diff --git a/tests/test_pipeline.py b/tests/test_pipeline.py
index 3c673d7..5f74550 100644
--- a/tests/test_pipeline.py
+++ b/tests/test_pipeline.py
@@ -27,6 +27,7 @@ transformers.pipeline = lambda *a, **k: lambda text: [
 sys.modules["transformers"] = transformers
 
 from lan_transcriber import llm_client, pipeline  # noqa: E402
+from lan_transcriber.pipeline_steps import precheck as precheck_step  # noqa: E402
 
 
 def fake_audio(tmp_path: Path, name: str = "sample.mp3") -> Path:
@@ -1153,10 +1154,10 @@ def test_run_precheck_quarantines_when_metrics_unavailable(tmp_path: Path, monke
     audio = tmp_path / "probe-fail.mp3"
     audio.write_bytes(b"not-real-audio")
 
-    monkeypatch.setattr(pipeline, "_audio_duration_from_wave", lambda _path: None)
-    monkeypatch.setattr(pipeline, "_audio_duration_from_ffprobe", lambda _path: None)
-    monkeypatch.setattr(pipeline, "_speech_ratio_from_wave", lambda _path: None)
-    monkeypatch.setattr(pipeline, "_speech_ratio_from_ffmpeg", lambda _path: None)
+    monkeypatch.setattr(precheck_step, "_audio_duration_from_wave", lambda _path: None)
+    monkeypatch.setattr(precheck_step, "_audio_duration_from_ffprobe", lambda _path: None)
+    monkeypatch.setattr(precheck_step, "_speech_ratio_from_wave", lambda _path: None)
+    monkeypatch.setattr(precheck_step, "_speech_ratio_from_ffmpeg", lambda _path: None)
 
     result = pipeline.run_precheck(audio, cfg)
     assert result.quarantine_reason == "precheck_metrics_unavailable"
@@ -1168,7 +1169,7 @@ def test_speech_ratio_from_ffmpeg_uses_unbounded_wait(tmp_path: Path, monkeypatc
     audio = tmp_path / "ffmpeg-input.mp3"
     audio.write_bytes(b"audio")
 
-    monkeypatch.setattr(pipeline.shutil, "which", lambda _name: "/usr/bin/ffmpeg")
+    monkeypatch.setattr(precheck_step.shutil, "which", lambda _name: "/usr/bin/ffmpeg")
 
     class _Stdout:
         def __init__(self):
@@ -1194,8 +1195,8 @@ def test_speech_ratio_from_ffmpeg_uses_unbounded_wait(tmp_path: Path, monkeypatc
             return False
 
     proc = _Proc()
-    monkeypatch.setattr(pipeline.subprocess, "Popen", lambda *_args, **_kwargs: proc)
+    monkeypatch.setattr(precheck_step.subprocess, "Popen", lambda *_args, **_kwargs: proc)
 
-    ratio = pipeline._speech_ratio_from_ffmpeg(audio)
+    ratio = precheck_step._speech_ratio_from_ffmpeg(audio)
     assert ratio == 0.0
     assert proc.wait_timeout is None
diff --git a/lan_transcriber/pipeline_steps/__init__.py b/lan_transcriber/pipeline_steps/__init__.py
new file mode 100644
index 0000000..b1ab429
--- /dev/null
+++ b/lan_transcriber/pipeline_steps/__init__.py
@@ -0,0 +1,36 @@
+from .artifacts import LlmDebugArtifacts, write_json_artifact, write_llm_debug_artifacts
+from .language import LanguageAnalysis, analyse_languages, resolve_target_summary_language, segment_language
+from .precheck import PrecheckResult, run_precheck
+from .snippets import SnippetExportRequest, export_speaker_snippets
+from .speaker_turns import build_speaker_turns, count_interruptions, normalise_asr_segments
+from .summary_builder import (
+    ActionItem,
+    Question,
+    SummaryResponse,
+    build_structured_summary_prompts,
+    build_summary_payload,
+    build_summary_prompts,
+)
+
+__all__ = [
+    "PrecheckResult",
+    "run_precheck",
+    "LanguageAnalysis",
+    "segment_language",
+    "analyse_languages",
+    "resolve_target_summary_language",
+    "normalise_asr_segments",
+    "build_speaker_turns",
+    "count_interruptions",
+    "SnippetExportRequest",
+    "export_speaker_snippets",
+    "ActionItem",
+    "Question",
+    "SummaryResponse",
+    "build_structured_summary_prompts",
+    "build_summary_payload",
+    "build_summary_prompts",
+    "LlmDebugArtifacts",
+    "write_json_artifact",
+    "write_llm_debug_artifacts",
+]
diff --git a/lan_transcriber/pipeline_steps/artifacts.py b/lan_transcriber/pipeline_steps/artifacts.py
new file mode 100644
index 0000000..cc2a526
--- /dev/null
+++ b/lan_transcriber/pipeline_steps/artifacts.py
@@ -0,0 +1,29 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from pathlib import Path
+from typing import Any
+
+from lan_transcriber.artifacts import atomic_write_json, atomic_write_text
+
+
+@dataclass(frozen=True)
+class LlmDebugArtifacts:
+    derived_dir: Path
+    raw_output: str
+    extracted_payload: dict[str, Any] | None
+    validation_error: dict[str, Any]
+
+
+def write_json_artifact(path: Path, payload: Any) -> Path:
+    atomic_write_json(path, payload)
+    return path
+
+
+def write_llm_debug_artifacts(spec: LlmDebugArtifacts) -> None:
+    write_json_artifact(spec.derived_dir / "llm_extract.json", spec.extracted_payload or {})
+    write_json_artifact(spec.derived_dir / "llm_validation_error.json", spec.validation_error)
+    atomic_write_text(spec.derived_dir / "llm_raw.txt", spec.raw_output)
+
+
+__all__ = ["LlmDebugArtifacts", "write_json_artifact", "write_llm_debug_artifacts"]
diff --git a/lan_transcriber/pipeline_steps/language.py b/lan_transcriber/pipeline_steps/language.py
new file mode 100644
index 0000000..10ff8ac
--- /dev/null
+++ b/lan_transcriber/pipeline_steps/language.py
@@ -0,0 +1,186 @@
+from __future__ import annotations
+
+import re
+from dataclasses import dataclass
+from typing import Any, Sequence
+
+from lan_transcriber.utils import normalise_language_code, safe_float
+
+_EN_STOPWORDS = {
+    "the",
+    "and",
+    "to",
+    "of",
+    "in",
+    "for",
+    "with",
+    "on",
+    "is",
+    "are",
+    "we",
+    "you",
+    "hello",
+    "thanks",
+    "meeting",
+    "team",
+    "today",
+}
+
+_ES_STOPWORDS = {
+    "el",
+    "la",
+    "los",
+    "las",
+    "de",
+    "que",
+    "y",
+    "en",
+    "para",
+    "con",
+    "es",
+    "somos",
+    "hola",
+    "gracias",
+    "reunion",
+    "equipo",
+    "hoy",
+}
+
+
+@dataclass(frozen=True)
+class LanguageAnalysis:
+    segments: list[dict[str, Any]]
+    dominant_language: str
+    distribution: dict[str, float]
+    spans: list[dict[str, Any]]
+
+
+def _guess_language_from_text(text: str) -> str | None:
+    sample = text.strip().lower()
+    if not sample:
+        return None
+    tokens = re.findall(r"[a-zA-Z\u00c0-\u017f]+", sample)
+    if not tokens:
+        return None
+    en_score = sum(1 for token in tokens if token in _EN_STOPWORDS)
+    es_score = sum(1 for token in tokens if token in _ES_STOPWORDS)
+    if any(ch in sample for ch in "áéíóúñ¿¡"):
+        es_score += 2
+    if en_score == 0 and es_score == 0:
+        return None
+    if es_score > en_score:
+        return "es"
+    if en_score > es_score:
+        return "en"
+    return None
+
+
+def segment_language(
+    segment: dict[str, Any],
+    *,
+    detected_language: str | None,
+    transcript_language_override: str | None,
+) -> str:
+    if transcript_language_override:
+        return transcript_language_override
+    seg_language = normalise_language_code(segment.get("language"))
+    if seg_language:
+        return seg_language
+    if detected_language:
+        return detected_language
+    text_language = _guess_language_from_text(str(segment.get("text") or ""))
+    if text_language:
+        return text_language
+    return "unknown"
+
+
+def _duration_weight(start: float, end: float, text: str) -> float:
+    duration = max(0.0, end - start)
+    if duration > 0:
+        return duration
+    tokens = max(len(text.split()), 1)
+    return tokens * 0.01
+
+
+def analyse_languages(
+    asr_segments: Sequence[dict[str, Any]],
+    *,
+    detected_language: str | None,
+    transcript_language_override: str | None,
+) -> LanguageAnalysis:
+    if not asr_segments:
+        fallback = transcript_language_override or detected_language or "unknown"
+        return LanguageAnalysis([], fallback, {}, [])
+
+    enriched = sorted(
+        (dict(seg) for seg in asr_segments),
+        key=lambda row: (
+            safe_float(row.get("start"), default=0.0),
+            safe_float(row.get("end"), default=0.0),
+        ),
+    )
+
+    weighted_totals: dict[str, float] = {}
+    spans: list[dict[str, Any]] = []
+    for segment in enriched:
+        start = safe_float(segment.get("start"), default=0.0)
+        end = safe_float(segment.get("end"), default=start)
+        if end < start:
+            end = start
+        text = str(segment.get("text") or "").strip()
+        lang = segment_language(
+            segment,
+            detected_language=detected_language,
+            transcript_language_override=transcript_language_override,
+        )
+        segment["language"] = lang
+
+        weight = _duration_weight(start, end, text)
+        weighted_totals[lang] = weighted_totals.get(lang, 0.0) + weight
+
+        if spans and spans[-1]["lang"] == lang and start <= safe_float(spans[-1]["end"]) + 0.5:
+            spans[-1]["end"] = round(max(safe_float(spans[-1]["end"]), end), 3)
+        else:
+            spans.append(
+                {
+                    "start": round(start, 3),
+                    "end": round(end, 3),
+                    "lang": lang,
+                }
+            )
+
+    ordered = sorted(weighted_totals.items(), key=lambda row: (-row[1], row[0]))
+    dominant = "unknown"
+    for lang, _weight in ordered:
+        if lang != "unknown":
+            dominant = lang
+            break
+    if dominant == "unknown" and ordered:
+        dominant = ordered[0][0]
+
+    total_weight = sum(weighted_totals.values())
+    distribution: dict[str, float] = {}
+    if total_weight > 0:
+        for lang, weight in ordered:
+            distribution[lang] = round((weight / total_weight) * 100.0, 2)
+
+    return LanguageAnalysis(enriched, dominant, distribution, spans)
+
+
+def resolve_target_summary_language(
+    requested_language: str | None,
+    *,
+    dominant_language: str,
+    detected_language: str | None,
+) -> str:
+    requested = normalise_language_code(requested_language)
+    if requested:
+        return requested
+    if dominant_language and dominant_language != "unknown":
+        return dominant_language
+    if detected_language:
+        return detected_language
+    return "en"
+
+
+__all__ = ["LanguageAnalysis", "segment_language", "analyse_languages", "resolve_target_summary_language"]
diff --git a/lan_transcriber/pipeline_steps/orchestrator.py b/lan_transcriber/pipeline_steps/orchestrator.py
new file mode 100644
index 0000000..73fcc24
--- /dev/null
+++ b/lan_transcriber/pipeline_steps/orchestrator.py
@@ -0,0 +1,442 @@
+from __future__ import annotations
+
+import asyncio
+import shutil
+import time
+from pathlib import Path
+from types import SimpleNamespace
+from typing import Any, Iterable, List, Protocol, Sequence
+
+from pydantic_settings import BaseSettings
+
+from .. import normalizer
+from ..aliases import ALIAS_PATH, load_aliases as _load_aliases, save_aliases as _save_aliases
+from ..artifacts import atomic_write_json, atomic_write_text, build_recording_artifacts, stage_raw_audio
+from ..llm_client import LLMClient
+from ..metrics import error_rate_total, p95_latency_seconds
+from ..models import SpeakerSegment, TranscriptResult
+from .language import analyse_languages, resolve_target_summary_language, segment_language
+from .precheck import PrecheckResult, run_precheck as _run_precheck
+from .snippets import SnippetExportRequest, export_speaker_snippets
+from .speaker_turns import (
+    _diarization_segments,
+    build_speaker_turns,
+    normalise_asr_segments,
+)
+from .summary_builder import (
+    _build_structured_summary_payload,
+    build_structured_summary_prompts,
+    build_summary_payload,
+    build_summary_prompts,
+)
+from ..runtime_paths import default_recordings_root, default_tmp_root, default_unknown_dir, default_voices_dir
+from ..utils import normalise_language_code, safe_float
+
+
+class Diariser(Protocol):
+    """Minimal interface for speaker diarisation."""
+
+    async def __call__(self, audio_path: Path): ...
+
+
+class Settings(BaseSettings):
+    """Runtime configuration for the transcription pipeline."""
+
+    speaker_db: Path = ALIAS_PATH
+    recordings_root: Path = default_recordings_root()
+    voices_dir: Path = default_voices_dir()
+    unknown_dir: Path = default_unknown_dir()
+    tmp_root: Path = default_tmp_root()
+    llm_model: str = "llama3:8b"
+    embed_threshold: float = 0.65
+    merge_similar: float = 0.9
+    precheck_min_duration_sec: float = 20.0
+    precheck_min_speech_ratio: float = 0.10
+
+    class Config:
+        env_prefix = "LAN_"
+
+
+def _merge_similar(lines: Iterable[str], threshold: float) -> List[str]:
+    out: List[str] = []
+    for line in lines:
+        if not out:
+            out.append(line)
+            continue
+        prev = out[-1]
+        sim = sum(a == b for a, b in zip(prev, line)) / max(len(prev), len(line))
+        if sim < threshold:
+            out.append(line)
+    return out
+
+
+def _sentiment_score(text: str) -> int:  # pragma: no cover - trivial wrapper
+    from transformers import pipeline as hf_pipeline
+
+    sent = hf_pipeline("sentiment-analysis")(text[:4000])[0]
+    if sent["label"] == "positive":
+        return int(sent["score"] * 100)
+    if sent["label"] == "negative":
+        return int((1 - sent["score"]) * 100)
+    return 50
+
+
+def refresh_aliases(result: TranscriptResult, alias_path: Path = ALIAS_PATH) -> None:
+    aliases = _load_aliases(alias_path)
+    result.speakers = sorted({aliases.get(s.speaker, s.speaker) for s in result.segments})
+
+
+def _default_recording_id(audio_path: Path) -> str:
+    stem = audio_path.stem.strip()
+    return stem or "recording"
+
+
+def _fallback_diarization(duration_sec: float | None):
+    duration = max(duration_sec or 0.0, 0.1)
+
+    class _Annotation:
+        def itertracks(self, yield_label: bool = False):
+            if yield_label:
+                yield SimpleNamespace(start=0.0, end=duration), "S1"
+            else:  # pragma: no cover - legacy branch
+                yield (SimpleNamespace(start=0.0, end=duration),)
+
+    return _Annotation()
+
+
+def _language_payload(info: dict[str, Any]) -> dict[str, Any]:
+    detected_raw = str(info.get("language") or info.get("detected_language") or info.get("lang") or "unknown")
+    detected = normalise_language_code(detected_raw) or "unknown"
+    confidence_raw = None
+    for key in ("language_probability", "language_confidence", "language_score", "probability"):
+        if key in info and info[key] is not None:
+            confidence_raw = info[key]
+            break
+    confidence = None if confidence_raw is None else round(safe_float(confidence_raw, default=0.0), 4)
+    return {"detected": detected, "confidence": confidence}
+
+
+def _empty_questions() -> dict[str, Any]:
+    return {
+        "total_count": 0,
+        "types": {"open": 0, "yes_no": 0, "clarification": 0, "status": 0, "decision_seeking": 0},
+        "extracted": [],
+    }
+
+
+def _clear_dir(path: Path) -> None:
+    for child in path.iterdir() if path.exists() else []:
+        if child.is_dir():
+            shutil.rmtree(child)
+        else:
+            child.unlink(missing_ok=True)
+    path.mkdir(parents=True, exist_ok=True)
+
+
+def _base_transcript_payload(
+    *,
+    recording_id: str,
+    language: dict[str, Any],
+    dominant_language: str,
+    language_distribution: dict[str, float],
+    language_spans: list[dict[str, Any]],
+    target_summary_language: str,
+    transcript_language_override: str | None,
+    calendar_title: str | None,
+    calendar_attendees: list[str],
+    segments: list[dict[str, Any]],
+    speakers: list[str],
+    text: str,
+) -> dict[str, Any]:
+    return {
+        "recording_id": recording_id,
+        "language": language,
+        "dominant_language": dominant_language,
+        "language_distribution": language_distribution,
+        "language_spans": language_spans,
+        "target_summary_language": target_summary_language,
+        "transcript_language_override": transcript_language_override,
+        "calendar_title": calendar_title,
+        "calendar_attendees": calendar_attendees,
+        "segments": segments,
+        "speakers": speakers,
+        "text": text,
+    }
+
+
+def run_precheck(audio_path: Path, cfg: Settings | None = None) -> PrecheckResult:
+    settings = cfg or Settings()
+    return _run_precheck(
+        audio_path,
+        min_duration_sec=settings.precheck_min_duration_sec,
+        min_speech_ratio=settings.precheck_min_speech_ratio,
+    )
+
+
+async def run_pipeline(
+    audio_path: Path,
+    cfg: Settings,
+    llm: LLMClient,
+    diariser: Diariser,
+    recording_id: str | None = None,
+    precheck: PrecheckResult | None = None,
+    target_summary_language: str | None = None,
+    transcript_language_override: str | None = None,
+    calendar_title: str | None = None,
+    calendar_attendees: Sequence[str] | None = None,
+) -> TranscriptResult:
+    start = time.perf_counter()
+    artifacts = build_recording_artifacts(cfg.recordings_root, recording_id or _default_recording_id(audio_path), audio_path.suffix)
+    stage_raw_audio(audio_path, artifacts.raw_audio_path)
+
+    precheck_result = precheck or run_precheck(audio_path, cfg)
+    override_lang = normalise_language_code(transcript_language_override)
+    summary_lang = resolve_target_summary_language(target_summary_language, dominant_language=override_lang or "unknown", detected_language=None)
+    cal_title = str(calendar_title or "").strip() or None
+    cal_attendees = [str(item).strip() for item in (calendar_attendees or []) if str(item).strip()]
+
+    atomic_write_json(
+        artifacts.metrics_json_path,
+        {"status": "running", "version": 1, "precheck": precheck_result.__dict__},
+    )
+
+    if precheck_result.quarantine_reason:
+        _clear_dir(artifacts.snippets_dir)
+        atomic_write_text(artifacts.transcript_txt_path, "")
+        atomic_write_json(
+            artifacts.transcript_json_path,
+            _base_transcript_payload(
+                recording_id=artifacts.recording_id,
+                language={"detected": "unknown", "confidence": None},
+                dominant_language=override_lang or "unknown",
+                language_distribution={},
+                language_spans=[],
+                target_summary_language=summary_lang,
+                transcript_language_override=override_lang,
+                calendar_title=cal_title,
+                calendar_attendees=cal_attendees,
+                segments=[],
+                speakers=[],
+                text="",
+            ),
+        )
+        atomic_write_json(artifacts.segments_json_path, [])
+        atomic_write_json(artifacts.speaker_turns_json_path, [])
+        atomic_write_json(
+            artifacts.summary_json_path,
+            _build_structured_summary_payload(
+                model=cfg.llm_model,
+                target_summary_language=summary_lang,
+                friendly=0,
+                topic="Quarantined recording",
+                summary_bullets=["Recording was quarantined before transcription."],
+                decisions=[],
+                action_items=[],
+                emotional_summary="No emotional summary available.",
+                questions=_empty_questions(),
+                status="quarantined",
+                reason=precheck_result.quarantine_reason,
+            ),
+        )
+        atomic_write_json(artifacts.metrics_json_path, {"status": "quarantined", "version": 1, "precheck": precheck_result.__dict__})
+        p95_latency_seconds.observe(time.perf_counter() - start)
+        return TranscriptResult(summary="Quarantined", body="", friendly=0, speakers=[], summary_path=artifacts.summary_json_path, body_path=artifacts.transcript_txt_path, unknown_chunks=[], segments=[])
+
+    try:
+        import whisperx
+
+        def _asr() -> tuple[list[dict[str, Any]], dict[str, Any]]:
+            asr_language = override_lang or "auto"
+            kwargs: dict[str, Any] = {"vad_filter": True, "language": asr_language}
+            try:
+                segments, info = whisperx.transcribe(str(audio_path), word_timestamps=True, **kwargs)
+            except TypeError:
+                segments, info = whisperx.transcribe(str(audio_path), **kwargs)
+            return list(segments), dict(info or {})
+
+        (raw_segments, info), diarization = await asyncio.gather(asyncio.to_thread(_asr), diariser(audio_path))
+        asr_segments = normalise_asr_segments(raw_segments)
+        language_info = _language_payload(info)
+        detected_language = normalise_language_code(language_info["detected"]) if language_info["detected"] != "unknown" else None
+        language_analysis = analyse_languages(asr_segments, detected_language=detected_language, transcript_language_override=override_lang)
+        if language_info["detected"] == "unknown" and language_analysis.dominant_language != "unknown":
+            language_info["detected"] = language_analysis.dominant_language
+        summary_lang = resolve_target_summary_language(target_summary_language, dominant_language=language_analysis.dominant_language, detected_language=detected_language)
+
+        asr_text = " ".join(seg.get("text", "").strip() for seg in language_analysis.segments).strip()
+        clean_text = normalizer.dedup(asr_text)
+        diar_segments = _diarization_segments(diarization)
+        if not diar_segments and language_analysis.segments:
+            fallback_end = max(safe_float(seg.get("end")) for seg in language_analysis.segments)
+            diar_segments = _diarization_segments(_fallback_diarization(max(fallback_end, 0.1)))
+        speaker_turns = build_speaker_turns(
+            language_analysis.segments,
+            diar_segments,
+            default_language=language_analysis.dominant_language if language_analysis.dominant_language != "unknown" else detected_language,
+        )
+
+        aliases = _load_aliases(cfg.speaker_db)
+        for row in diar_segments:
+            aliases.setdefault(str(row["speaker"]), str(row["speaker"]))
+        _save_aliases(aliases, cfg.speaker_db)
+    except Exception as exc:
+        error_rate_total.inc()
+        atomic_write_json(
+            artifacts.summary_json_path,
+            _build_structured_summary_payload(
+                model=cfg.llm_model,
+                target_summary_language=summary_lang,
+                friendly=0,
+                topic="Summary generation failed",
+                summary_bullets=["Unable to produce a summary due to a processing error."],
+                decisions=[],
+                action_items=[],
+                emotional_summary="No emotional summary available.",
+                questions=_empty_questions(),
+                status="failed",
+                error=str(exc) or exc.__class__.__name__,
+            ),
+        )
+        atomic_write_json(
+            artifacts.metrics_json_path,
+            {"status": "failed", "version": 1, "precheck": {**precheck_result.__dict__, "quarantine_reason": None}, "error": str(exc) or exc.__class__.__name__},
+        )
+        raise
+
+    if not clean_text:
+        _clear_dir(artifacts.snippets_dir)
+        atomic_write_text(artifacts.transcript_txt_path, "")
+        speakers = sorted({aliases.get(row["speaker"], row["speaker"]) for row in diar_segments})
+        atomic_write_json(
+            artifacts.transcript_json_path,
+            _base_transcript_payload(
+                recording_id=artifacts.recording_id,
+                language=language_info,
+                dominant_language=language_analysis.dominant_language,
+                language_distribution=language_analysis.distribution,
+                language_spans=language_analysis.spans,
+                target_summary_language=summary_lang,
+                transcript_language_override=override_lang,
+                calendar_title=cal_title,
+                calendar_attendees=cal_attendees,
+                segments=language_analysis.segments,
+                speakers=speakers,
+                text="",
+            ),
+        )
+        atomic_write_json(artifacts.segments_json_path, diar_segments)
+        atomic_write_json(artifacts.speaker_turns_json_path, speaker_turns)
+        atomic_write_json(
+            artifacts.summary_json_path,
+            _build_structured_summary_payload(
+                model=cfg.llm_model,
+                target_summary_language=summary_lang,
+                friendly=0,
+                topic="No speech detected",
+                summary_bullets=["No speech detected."],
+                decisions=[],
+                action_items=[],
+                emotional_summary="No emotional summary available.",
+                questions=_empty_questions(),
+                status="no_speech",
+            ),
+        )
+        atomic_write_json(artifacts.metrics_json_path, {"status": "no_speech", "version": 1, "precheck": {**precheck_result.__dict__, "quarantine_reason": None}, "language": language_info, "asr_segments": len(language_analysis.segments), "diar_segments": len(diar_segments), "speaker_turns": len(speaker_turns)})
+        p95_latency_seconds.observe(time.perf_counter() - start)
+        return TranscriptResult(summary="No speech detected", body="", friendly=0, speakers=speakers, summary_path=artifacts.summary_json_path, body_path=artifacts.transcript_txt_path, unknown_chunks=[], segments=[])
+
+    snippet_paths = export_speaker_snippets(SnippetExportRequest(audio_path=audio_path, diar_segments=diar_segments, snippets_dir=artifacts.snippets_dir, duration_sec=precheck_result.duration_sec))
+    speaker_lines = _merge_similar(
+        [
+            f"[{turn['start']:.2f}-{turn['end']:.2f}] **{aliases.get(turn['speaker'], turn['speaker'])}:** {turn['text']}"
+            for turn in speaker_turns
+        ],
+        cfg.merge_similar,
+    )
+    friendly = _sentiment_score(clean_text)
+    sys_prompt, user_prompt = build_structured_summary_prompts(speaker_turns, summary_lang, calendar_title=cal_title, calendar_attendees=cal_attendees)
+
+    try:
+        msg = await llm.generate(system_prompt=sys_prompt, user_prompt=user_prompt, model=cfg.llm_model, response_format={"type": "json_object"})
+        raw_summary = msg.get("content", "") if isinstance(msg, dict) else str(msg)
+        summary_payload = build_summary_payload(
+            raw_llm_content=raw_summary,
+            model=cfg.llm_model,
+            target_summary_language=summary_lang,
+            friendly=friendly,
+            default_topic=cal_title or "Meeting summary",
+            derived_dir=artifacts.summary_json_path.parent,
+        )
+        serialised_segments = [SpeakerSegment(start=safe_float(turn["start"]), end=safe_float(turn["end"]), speaker=str(turn["speaker"]), text=str(turn["text"])) for turn in speaker_turns]
+        speakers = sorted(set(aliases.get(turn["speaker"], turn["speaker"]) for turn in speaker_turns))
+        atomic_write_text(artifacts.transcript_txt_path, clean_text)
+        payload = _base_transcript_payload(
+            recording_id=artifacts.recording_id,
+            language=language_info,
+            dominant_language=language_analysis.dominant_language,
+            language_distribution=language_analysis.distribution,
+            language_spans=language_analysis.spans,
+            target_summary_language=summary_lang,
+            transcript_language_override=override_lang,
+            calendar_title=cal_title,
+            calendar_attendees=cal_attendees,
+            segments=language_analysis.segments,
+            speakers=speakers,
+            text=clean_text,
+        )
+        payload["speaker_lines"] = speaker_lines
+        atomic_write_json(artifacts.transcript_json_path, payload)
+        atomic_write_json(artifacts.segments_json_path, diar_segments)
+        atomic_write_json(artifacts.speaker_turns_json_path, speaker_turns)
+        atomic_write_json(artifacts.summary_json_path, summary_payload)
+        atomic_write_json(artifacts.metrics_json_path, {"status": "ok", "version": 1, "precheck": {**precheck_result.__dict__, "quarantine_reason": None}, "language": language_info, "asr_segments": len(language_analysis.segments), "diar_segments": len(diar_segments), "speaker_turns": len(speaker_turns), "snippets": len(snippet_paths)})
+        return TranscriptResult(summary=str(summary_payload.get("summary") or ""), body=clean_text, friendly=friendly, speakers=speakers, summary_path=artifacts.summary_json_path, body_path=artifacts.transcript_txt_path, unknown_chunks=snippet_paths, segments=serialised_segments)
+    except Exception as exc:
+        error_rate_total.inc()
+        atomic_write_json(
+            artifacts.summary_json_path,
+            _build_structured_summary_payload(
+                model=cfg.llm_model,
+                target_summary_language=summary_lang,
+                friendly=friendly,
+                topic="Summary generation failed",
+                summary_bullets=["Unable to produce a summary due to a processing error."],
+                decisions=[],
+                action_items=[],
+                emotional_summary="No emotional summary available.",
+                questions=_empty_questions(),
+                status="failed",
+                error=str(exc) or exc.__class__.__name__,
+            ),
+        )
+        atomic_write_json(
+            artifacts.metrics_json_path,
+            {
+                "status": "failed",
+                "version": 1,
+                "precheck": {**precheck_result.__dict__, "quarantine_reason": None},
+                "language": language_info,
+                "asr_segments": len(language_analysis.segments),
+                "diar_segments": len(diar_segments),
+                "speaker_turns": len(speaker_turns),
+                "error": str(exc) or exc.__class__.__name__,
+            },
+        )
+        raise
+    finally:
+        p95_latency_seconds.observe(time.perf_counter() - start)
+
+
+_segment_language = segment_language
+
+__all__ = [
+    "run_pipeline",
+    "run_precheck",
+    "PrecheckResult",
+    "Settings",
+    "Diariser",
+    "refresh_aliases",
+    "build_summary_prompts",
+    "build_structured_summary_prompts",
+    "build_summary_payload",
+]
diff --git a/lan_transcriber/pipeline_steps/precheck.py b/lan_transcriber/pipeline_steps/precheck.py
new file mode 100644
index 0000000..a1d46c0
--- /dev/null
+++ b/lan_transcriber/pipeline_steps/precheck.py
@@ -0,0 +1,172 @@
+from __future__ import annotations
+
+import audioop
+import shutil
+import subprocess
+import wave
+from dataclasses import dataclass
+from pathlib import Path
+
+
+@dataclass(frozen=True)
+class PrecheckResult:
+    duration_sec: float | None
+    speech_ratio: float | None
+    quarantine_reason: str | None = None
+
+
+def _audio_duration_from_wave(audio_path: Path) -> float | None:
+    if audio_path.suffix.lower() != ".wav":
+        return None
+    try:
+        with wave.open(str(audio_path), "rb") as src:
+            rate = src.getframerate()
+            frames = src.getnframes()
+        if rate <= 0:
+            return None
+        return frames / float(rate)
+    except Exception:
+        return None
+
+
+def _audio_duration_from_ffprobe(audio_path: Path) -> float | None:
+    ffprobe = shutil.which("ffprobe")
+    if ffprobe is None:
+        return None
+    cmd = [
+        ffprobe,
+        "-v",
+        "error",
+        "-show_entries",
+        "format=duration",
+        "-of",
+        "default=noprint_wrappers=1:nokey=1",
+        str(audio_path),
+    ]
+    try:
+        proc = subprocess.run(cmd, check=False, capture_output=True, text=True)
+    except Exception:
+        return None
+    if proc.returncode != 0:
+        return None
+    raw = proc.stdout.strip()
+    if not raw:
+        return None
+    try:
+        value = float(raw)
+    except ValueError:
+        return None
+    if value <= 0:
+        return None
+    return value
+
+
+def _speech_ratio_from_wave(audio_path: Path) -> float | None:
+    if audio_path.suffix.lower() != ".wav":
+        return None
+    try:
+        with wave.open(str(audio_path), "rb") as src:
+            rate = src.getframerate()
+            channels = src.getnchannels()
+            sample_width = src.getsampwidth()
+            frame_samples = max(int(rate * 0.03), 1)
+            frame_bytes = frame_samples * channels * sample_width
+            voiced = 0
+            total = 0
+            while True:
+                chunk = src.readframes(frame_samples)
+                if not chunk:
+                    break
+                if len(chunk) < frame_bytes // 2:
+                    break
+                total += 1
+                if audioop.rms(chunk, sample_width) >= 350:
+                    voiced += 1
+            if total == 0:
+                return 0.0
+            return voiced / float(total)
+    except Exception:
+        return None
+
+
+def _speech_ratio_from_ffmpeg(audio_path: Path) -> float | None:
+    ffmpeg = shutil.which("ffmpeg")
+    if ffmpeg is None:
+        return None
+    cmd = [
+        ffmpeg,
+        "-nostdin",
+        "-hide_banner",
+        "-loglevel",
+        "error",
+        "-i",
+        str(audio_path),
+        "-f",
+        "s16le",
+        "-ac",
+        "1",
+        "-ar",
+        "16000",
+        "-",
+    ]
+    try:
+        with subprocess.Popen(
+            cmd,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.DEVNULL,
+        ) as proc:
+            if proc.stdout is None:
+                return None
+            frame_bytes = 960  # 30ms @ 16kHz * 2 bytes sample
+            voiced = 0
+            total = 0
+            while True:
+                chunk = proc.stdout.read(frame_bytes)
+                if not chunk:
+                    break
+                if len(chunk) < frame_bytes:
+                    break
+                total += 1
+                if audioop.rms(chunk, 2) >= 300:
+                    voiced += 1
+            proc.wait()
+            if proc.returncode != 0:
+                return None
+            if total == 0:
+                return 0.0
+            return voiced / float(total)
+    except Exception:
+        return None
+
+
+def run_precheck(
+    audio_path: Path,
+    *,
+    min_duration_sec: float,
+    min_speech_ratio: float,
+) -> PrecheckResult:
+    """Compute duration + VAD speech ratio and decide quarantine status."""
+    duration_sec = _audio_duration_from_wave(audio_path)
+    if duration_sec is None:
+        duration_sec = _audio_duration_from_ffprobe(audio_path)
+
+    speech_ratio = _speech_ratio_from_wave(audio_path)
+    if speech_ratio is None:
+        speech_ratio = _speech_ratio_from_ffmpeg(audio_path)
+
+    quarantine_reason: str | None = None
+    if duration_sec is None or speech_ratio is None:
+        quarantine_reason = "precheck_metrics_unavailable"
+    elif duration_sec < min_duration_sec:
+        quarantine_reason = f"duration_lt_{min_duration_sec:.0f}s"
+    elif speech_ratio < min_speech_ratio:
+        quarantine_reason = f"speech_ratio_lt_{min_speech_ratio:.2f}"
+
+    return PrecheckResult(
+        duration_sec=round(duration_sec, 3) if duration_sec is not None else None,
+        speech_ratio=round(speech_ratio, 4) if speech_ratio is not None else None,
+        quarantine_reason=quarantine_reason,
+    )
+
+
+__all__ = ["PrecheckResult", "run_precheck"]
diff --git a/lan_transcriber/pipeline_steps/snippets.py b/lan_transcriber/pipeline_steps/snippets.py
new file mode 100644
index 0000000..81998f2
--- /dev/null
+++ b/lan_transcriber/pipeline_steps/snippets.py
@@ -0,0 +1,228 @@
+from __future__ import annotations
+
+import shutil
+import subprocess
+import wave
+from dataclasses import dataclass
+from pathlib import Path
+from typing import Any, Sequence
+
+from lan_transcriber.utils import safe_float
+
+
+@dataclass(frozen=True)
+class SnippetExportRequest:
+    audio_path: Path
+    diar_segments: Sequence[dict[str, Any]]
+    snippets_dir: Path
+    duration_sec: float | None
+
+
+def _speaker_slug(label: str) -> str:
+    slug = "".join(ch if ch.isalnum() or ch in {"-", "_"} else "_" for ch in label)
+    slug = slug.strip("_")
+    return slug or "speaker"
+
+
+def _clear_dir(path: Path) -> None:
+    if not path.exists():
+        path.mkdir(parents=True, exist_ok=True)
+        return
+    for child in path.iterdir():
+        if child.is_dir():
+            shutil.rmtree(child)
+        else:
+            child.unlink(missing_ok=True)
+
+
+def _snippet_window(
+    start: float,
+    end: float,
+    *,
+    duration_sec: float | None,
+) -> tuple[float, float]:
+    seg_duration = max(0.0, end - start)
+    target = min(20.0, max(10.0, seg_duration if seg_duration > 0 else 10.0))
+    center = start + (seg_duration / 2.0 if seg_duration > 0 else 0.0)
+    clip_start = max(0.0, center - (target / 2.0))
+    clip_end = clip_start + target
+    if duration_sec is not None and duration_sec > 0:
+        if clip_end > duration_sec:
+            clip_end = duration_sec
+            clip_start = max(0.0, clip_end - target)
+    if clip_end <= clip_start:
+        clip_end = clip_start + min(target, 1.0)
+    return round(clip_start, 3), round(clip_end, 3)
+
+
+def _extract_wav_snippet_with_wave(
+    audio_path: Path,
+    out_path: Path,
+    *,
+    start_sec: float,
+    end_sec: float,
+) -> bool:
+    if audio_path.suffix.lower() != ".wav":
+        return False
+    try:
+        with wave.open(str(audio_path), "rb") as src:
+            rate = src.getframerate()
+            channels = src.getnchannels()
+            sampwidth = src.getsampwidth()
+            start_frame = max(0, int(start_sec * rate))
+            end_frame = max(start_frame + 1, int(end_sec * rate))
+            src.setpos(min(start_frame, src.getnframes()))
+            frames = src.readframes(max(0, end_frame - start_frame))
+        if not frames:
+            return False
+        out_path.parent.mkdir(parents=True, exist_ok=True)
+        with wave.open(str(out_path), "wb") as dst:
+            dst.setnchannels(channels)
+            dst.setsampwidth(sampwidth)
+            dst.setframerate(rate)
+            dst.writeframes(frames)
+        return True
+    except Exception:
+        return False
+
+
+def _extract_wav_snippet_with_ffmpeg(
+    audio_path: Path,
+    out_path: Path,
+    *,
+    start_sec: float,
+    end_sec: float,
+) -> bool:
+    ffmpeg = shutil.which("ffmpeg")
+    if ffmpeg is None:
+        return False
+    duration = max(0.1, end_sec - start_sec)
+    out_path.parent.mkdir(parents=True, exist_ok=True)
+    cmd = [
+        ffmpeg,
+        "-nostdin",
+        "-hide_banner",
+        "-loglevel",
+        "error",
+        "-y",
+        "-ss",
+        f"{start_sec:.3f}",
+        "-t",
+        f"{duration:.3f}",
+        "-i",
+        str(audio_path),
+        "-ac",
+        "1",
+        "-ar",
+        "16000",
+        "-c:a",
+        "pcm_s16le",
+        str(out_path),
+    ]
+    try:
+        proc = subprocess.run(cmd, check=False, capture_output=True, text=True)
+    except Exception:
+        return False
+    return proc.returncode == 0 and out_path.exists() and out_path.stat().st_size > 44
+
+
+def _write_silence_wav(path: Path, duration_sec: float = 1.0) -> None:
+    samples = max(int(16000 * max(duration_sec, 0.1)), 1)
+    payload = b"\x00\x00" * samples
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with wave.open(str(path), "wb") as wav_out:
+        wav_out.setnchannels(1)
+        wav_out.setsampwidth(2)
+        wav_out.setframerate(16000)
+        wav_out.writeframes(payload)
+
+
+def _overlap_seconds(
+    left_start: float,
+    left_end: float,
+    right_start: float,
+    right_end: float,
+) -> float:
+    return max(0.0, min(left_end, right_end) - max(left_start, right_start))
+
+
+def export_speaker_snippets(request: SnippetExportRequest) -> list[Path]:
+    _clear_dir(request.snippets_dir)
+
+    by_speaker: dict[str, list[dict[str, Any]]] = {}
+    for segment in request.diar_segments:
+        speaker = str(segment.get("speaker", "S1"))
+        by_speaker.setdefault(speaker, []).append(segment)
+
+    all_outputs: list[Path] = []
+    for speaker in sorted(by_speaker):
+        ranked = sorted(
+            by_speaker[speaker],
+            key=lambda row: (
+                -(safe_float(row.get("end")) - safe_float(row.get("start"))),
+                safe_float(row.get("start")),
+                safe_float(row.get("end")),
+            ),
+        )
+        chosen: list[dict[str, Any]] = []
+        for candidate in ranked:
+            start = safe_float(candidate.get("start"))
+            end = safe_float(candidate.get("end"), default=start)
+            if end <= start:
+                continue
+            overlaps = any(
+                _overlap_seconds(
+                    start,
+                    end,
+                    safe_float(existing.get("start")),
+                    safe_float(existing.get("end")),
+                )
+                > 0.5
+                for existing in chosen
+            )
+            if overlaps:
+                continue
+            chosen.append(candidate)
+            if len(chosen) == 3:
+                break
+        if len(chosen) < 2:
+            for candidate in ranked:
+                if candidate in chosen:
+                    continue
+                chosen.append(candidate)
+                if len(chosen) == 2:
+                    break
+
+        speaker_dir = request.snippets_dir / _speaker_slug(speaker)
+        for idx, segment in enumerate(
+            sorted(chosen, key=lambda row: safe_float(row.get("start"))), start=1
+        ):
+            seg_start = safe_float(segment.get("start"))
+            seg_end = safe_float(segment.get("end"), default=seg_start)
+            clip_start, clip_end = _snippet_window(
+                seg_start,
+                seg_end,
+                duration_sec=request.duration_sec,
+            )
+            out_path = speaker_dir / f"{idx}.wav"
+            written = _extract_wav_snippet_with_wave(
+                request.audio_path,
+                out_path,
+                start_sec=clip_start,
+                end_sec=clip_end,
+            )
+            if not written:
+                written = _extract_wav_snippet_with_ffmpeg(
+                    request.audio_path,
+                    out_path,
+                    start_sec=clip_start,
+                    end_sec=clip_end,
+                )
+            if not written:
+                _write_silence_wav(out_path, duration_sec=min(max(clip_end - clip_start, 1.0), 2.0))
+            all_outputs.append(out_path)
+    all_outputs.sort()
+    return all_outputs
+
+
+__all__ = ["SnippetExportRequest", "export_speaker_snippets"]
diff --git a/lan_transcriber/pipeline_steps/speaker_turns.py b/lan_transcriber/pipeline_steps/speaker_turns.py
new file mode 100644
index 0000000..8df707f
--- /dev/null
+++ b/lan_transcriber/pipeline_steps/speaker_turns.py
@@ -0,0 +1,290 @@
+from __future__ import annotations
+
+from typing import Any, Sequence
+
+from lan_transcriber.utils import normalise_language_code, safe_float
+
+DEFAULT_INTERRUPTION_OVERLAP_SEC = 0.3
+
+
+def _normalise_word(word: dict[str, Any], seg_start: float, seg_end: float) -> dict[str, Any] | None:
+    text = str(word.get("word") or word.get("text") or "").strip()
+    if not text:
+        return None
+    start = safe_float(word.get("start"), default=seg_start)
+    end = safe_float(word.get("end"), default=max(start, seg_end))
+    if end < start:
+        end = start
+    return {
+        "start": round(start, 3),
+        "end": round(end, 3),
+        "word": text,
+    }
+
+
+def normalise_asr_segments(raw_segments: Sequence[dict[str, Any]]) -> list[dict[str, Any]]:
+    out: list[dict[str, Any]] = []
+    for idx, raw in enumerate(raw_segments):
+        start = safe_float(raw.get("start"), default=float(idx))
+        end = safe_float(raw.get("end"), default=start)
+        if end < start:
+            end = start
+        text = str(raw.get("text") or "").strip()
+        words_raw = raw.get("words")
+        words: list[dict[str, Any]] = []
+        if isinstance(words_raw, list):
+            for word in words_raw:
+                if not isinstance(word, dict):
+                    continue
+                normalised = _normalise_word(word, start, end)
+                if normalised is not None:
+                    words.append(normalised)
+        if not words and text:
+            words = [{"start": round(start, 3), "end": round(end, 3), "word": text}]
+        payload: dict[str, Any] = {
+            "start": round(start, 3),
+            "end": round(end, 3),
+            "text": text,
+            "words": words,
+        }
+        language = raw.get("language")
+        if isinstance(language, str) and language.strip():
+            payload["language"] = language.strip()
+        out.append(payload)
+    return out
+
+
+def _diarization_segments(diarization: Any) -> list[dict[str, Any]]:
+    out: list[dict[str, Any]] = []
+    if diarization is None or not hasattr(diarization, "itertracks"):
+        return out
+    tracks = diarization.itertracks(yield_label=True)
+    for item in tracks:
+        if not isinstance(item, tuple) or len(item) < 2:
+            continue
+        seg = item[0]
+        label = item[1] if len(item) == 2 else item[-1]
+        start = safe_float(getattr(seg, "start", 0.0), default=0.0)
+        end = safe_float(getattr(seg, "end", start), default=start)
+        if end < start:
+            end = start
+        out.append(
+            {
+                "start": round(start, 3),
+                "end": round(end, 3),
+                "speaker": str(label),
+            }
+        )
+    out.sort(key=lambda row: (row["start"], row["end"], row["speaker"]))
+    return out
+
+
+def _overlap_seconds(
+    left_start: float,
+    left_end: float,
+    right_start: float,
+    right_end: float,
+) -> float:
+    return max(0.0, min(left_end, right_end) - max(left_start, right_start))
+
+
+def _pick_speaker(start: float, end: float, diar_segments: Sequence[dict[str, Any]]) -> str:
+    if not diar_segments:
+        return "S1"
+    best_speaker = str(diar_segments[0]["speaker"])
+    best_overlap = -1.0
+    midpoint = (start + end) / 2.0
+    best_distance = float("inf")
+    for seg in diar_segments:
+        d_start = safe_float(seg.get("start"), default=0.0)
+        d_end = safe_float(seg.get("end"), default=d_start)
+        overlap = _overlap_seconds(start, end, d_start, d_end)
+        if overlap > best_overlap:
+            best_overlap = overlap
+            best_speaker = str(seg.get("speaker", "S1"))
+        if overlap == 0.0:
+            if midpoint < d_start:
+                distance = d_start - midpoint
+            elif midpoint > d_end:
+                distance = midpoint - d_end
+            else:
+                distance = 0.0
+            if best_overlap <= 0.0 and distance < best_distance:
+                best_distance = distance
+                best_speaker = str(seg.get("speaker", "S1"))
+    return best_speaker
+
+
+def _words_from_segments(
+    asr_segments: Sequence[dict[str, Any]],
+    *,
+    default_language: str | None,
+) -> list[dict[str, Any]]:
+    words: list[dict[str, Any]] = []
+    for seg in asr_segments:
+        seg_start = safe_float(seg.get("start"), default=0.0)
+        seg_end = safe_float(seg.get("end"), default=seg_start)
+        seg_language = seg.get("language")
+        language = (
+            str(seg_language)
+            if isinstance(seg_language, str) and seg_language.strip()
+            else default_language
+        )
+        seg_words = seg.get("words")
+        if not isinstance(seg_words, list):
+            seg_words = []
+        if not seg_words and seg.get("text"):
+            seg_words = [
+                {
+                    "start": seg_start,
+                    "end": seg_end,
+                    "word": str(seg.get("text")),
+                }
+            ]
+        for raw_word in seg_words:
+            if not isinstance(raw_word, dict):
+                continue
+            text = str(raw_word.get("word") or "").strip()
+            if not text:
+                continue
+            start = safe_float(raw_word.get("start"), default=seg_start)
+            end = safe_float(raw_word.get("end"), default=max(start, seg_end))
+            if end < start:
+                end = start
+            payload: dict[str, Any] = {
+                "start": round(start, 3),
+                "end": round(end, 3),
+                "word": text,
+            }
+            if language:
+                payload["language"] = language
+            words.append(payload)
+    words.sort(key=lambda row: (row["start"], row["end"], row["word"]))
+    return words
+
+
+def build_speaker_turns(
+    asr_segments: Sequence[dict[str, Any]],
+    diar_segments: Sequence[dict[str, Any]],
+    *,
+    default_language: str | None,
+) -> list[dict[str, Any]]:
+    words = _words_from_segments(asr_segments, default_language=default_language)
+    if not words:
+        return []
+
+    turns: list[dict[str, Any]] = []
+    current: dict[str, Any] | None = None
+    for word in words:
+        start = safe_float(word.get("start"), default=0.0)
+        end = safe_float(word.get("end"), default=start)
+        speaker = _pick_speaker(start, end, diar_segments)
+        language = normalise_language_code(word.get("language"))
+        if (
+            current is not None
+            and current["speaker"] == speaker
+            and start - safe_float(current["end"], default=start) <= 1.0
+        ):
+            current["end"] = round(max(safe_float(current["end"]), end), 3)
+            current["text"] = f"{current['text']} {word['word']}".strip()
+        else:
+            if current is not None:
+                turns.append(current)
+            current = {
+                "start": round(start, 3),
+                "end": round(end, 3),
+                "speaker": speaker,
+                "text": str(word["word"]),
+            }
+            if language:
+                current["language"] = language
+    if current is not None:
+        turns.append(current)
+    return turns
+
+
+def _normalise_turns(turns: Sequence[dict[str, Any]]) -> list[dict[str, Any]]:
+    out: list[dict[str, Any]] = []
+    for row in turns:
+        if not isinstance(row, dict):
+            continue
+        text = str(row.get("text") or "").strip()
+        if not text:
+            continue
+        start = safe_float(row.get("start"), default=0.0)
+        end = safe_float(row.get("end"), default=start)
+        if end < start:
+            end = start
+        speaker = str(row.get("speaker") or "S1").strip() or "S1"
+        payload: dict[str, Any] = {
+            "start": round(start, 3),
+            "end": round(end, 3),
+            "speaker": speaker,
+            "text": text,
+        }
+        language = normalise_language_code(row.get("language"))
+        if language:
+            payload["language"] = language
+        out.append(payload)
+    out.sort(key=lambda item: (item["start"], item["end"], item["speaker"]))
+    return out
+
+
+def count_interruptions(
+    speaker_turns: Sequence[dict[str, Any]],
+    *,
+    overlap_threshold: float = DEFAULT_INTERRUPTION_OVERLAP_SEC,
+) -> dict[str, Any]:
+    """Count interruption events and per-speaker done/received counters."""
+    turns = _normalise_turns(speaker_turns)
+    safe_overlap = max(overlap_threshold, 0.0)
+
+    done: dict[str, int] = {}
+    received: dict[str, int] = {}
+    total = 0
+
+    for idx, turn in enumerate(turns):
+        interrupter = str(turn["speaker"])
+        done.setdefault(interrupter, 0)
+        received.setdefault(interrupter, 0)
+
+        turn_start = safe_float(turn.get("start"))
+        turn_end = safe_float(turn.get("end"), default=turn_start)
+        if turn_end <= turn_start:
+            continue
+
+        seen_receivers: set[str] = set()
+        for previous in turns[:idx]:
+            receiver = str(previous["speaker"])
+            if receiver == interrupter or receiver in seen_receivers:
+                continue
+
+            previous_start = safe_float(previous.get("start"), default=0.0)
+            previous_end = safe_float(previous.get("end"), default=safe_float(previous.get("start")))
+            if turn_start <= previous_start:
+                continue
+            if turn_start >= previous_end:
+                continue
+
+            overlap = min(previous_end, turn_end) - turn_start
+            if overlap < safe_overlap:
+                continue
+
+            done[interrupter] = done.get(interrupter, 0) + 1
+            received[receiver] = received.get(receiver, 0) + 1
+            total += 1
+            seen_receivers.add(receiver)
+
+    return {
+        "total": total,
+        "done": done,
+        "received": received,
+    }
+
+
+__all__ = [
+    "DEFAULT_INTERRUPTION_OVERLAP_SEC",
+    "normalise_asr_segments",
+    "build_speaker_turns",
+    "count_interruptions",
+]
diff --git a/lan_transcriber/pipeline_steps/summary_builder.py b/lan_transcriber/pipeline_steps/summary_builder.py
new file mode 100644
index 0000000..e07aa53
--- /dev/null
+++ b/lan_transcriber/pipeline_steps/summary_builder.py
@@ -0,0 +1,541 @@
+from __future__ import annotations
+
+import json
+import re
+from pathlib import Path
+from typing import Any, Sequence
+
+from pydantic import BaseModel, ConfigDict, Field, ValidationError, field_validator, model_validator
+
+from lan_transcriber.pipeline_steps.artifacts import LlmDebugArtifacts, write_llm_debug_artifacts
+from lan_transcriber.utils import normalise_text_items, safe_float
+
+_QUESTION_TYPE_KEYS = (
+    "open",
+    "yes_no",
+    "clarification",
+    "status",
+    "decision_seeking",
+)
+
+_LANGUAGE_NAME_MAP: dict[str, str] = {
+    "ar": "Arabic",
+    "de": "German",
+    "en": "English",
+    "es": "Spanish",
+    "fr": "French",
+    "hi": "Hindi",
+    "it": "Italian",
+    "ja": "Japanese",
+    "ko": "Korean",
+    "nl": "Dutch",
+    "pl": "Polish",
+    "pt": "Portuguese",
+    "ru": "Russian",
+    "tr": "Turkish",
+    "uk": "Ukrainian",
+    "zh": "Chinese",
+}
+
+
+class ActionItem(BaseModel):
+    model_config = ConfigDict(extra="ignore")
+
+    task: str
+    owner: str | None = None
+    deadline: str | None = None
+    confidence: float = Field(default=0.5)
+
+    @field_validator("task", mode="before")
+    @classmethod
+    def _clean_task(cls, value: Any) -> str:
+        text = str(value or "").strip()
+        if not text:
+            raise ValueError("task is required")
+        return text
+
+    @field_validator("owner", "deadline", mode="before")
+    @classmethod
+    def _clean_nullable_text(cls, value: Any) -> str | None:
+        if value is None:
+            return None
+        text = str(value).strip()
+        return text or None
+
+    @field_validator("confidence", mode="before")
+    @classmethod
+    def _clean_confidence(cls, value: Any) -> float:
+        parsed = safe_float(value, default=0.5)
+        return round(min(max(parsed, 0.0), 1.0), 2)
+
+
+class Question(BaseModel):
+    model_config = ConfigDict(extra="ignore")
+
+    total_count: int = 0
+    types: dict[str, int] = Field(default_factory=lambda: {key: 0 for key in _QUESTION_TYPE_KEYS})
+    extracted: list[str] = Field(default_factory=list)
+
+    @field_validator("total_count", mode="before")
+    @classmethod
+    def _clean_total(cls, value: Any) -> int:
+        return max(0, int(safe_float(value, default=0.0)))
+
+    @field_validator("types", mode="before")
+    @classmethod
+    def _clean_types(cls, value: Any) -> dict[str, int]:
+        if not isinstance(value, dict):
+            return {key: 0 for key in _QUESTION_TYPE_KEYS}
+        out = {key: 0 for key in _QUESTION_TYPE_KEYS}
+        for key in _QUESTION_TYPE_KEYS:
+            out[key] = max(0, int(safe_float(value.get(key), default=0.0)))
+        return out
+
+    @field_validator("extracted", mode="before")
+    @classmethod
+    def _clean_extracted(cls, value: Any) -> list[str]:
+        return normalise_text_items(value, max_items=20)
+
+    @model_validator(mode="after")
+    def _ensure_total_count(self) -> "Question":
+        inferred = max(sum(self.types.values()), len(self.extracted))
+        if self.total_count == 0:
+            self.total_count = inferred
+        return self
+
+
+class SummaryResponse(BaseModel):
+    model_config = ConfigDict(extra="ignore")
+
+    topic: str
+    summary_bullets: list[str]
+    decisions: list[str] = Field(default_factory=list)
+    action_items: list[ActionItem] = Field(default_factory=list)
+    emotional_summary: str
+    questions: Question = Field(default_factory=Question)
+
+    @field_validator("topic", mode="before")
+    @classmethod
+    def _clean_topic(cls, value: Any) -> str:
+        return str(value or "").strip()
+
+    @field_validator("summary_bullets", mode="before")
+    @classmethod
+    def _clean_summary_bullets(cls, value: Any) -> list[str]:
+        items = normalise_text_items(value, max_items=12)
+        if not items:
+            raise ValueError("summary_bullets must not be empty")
+        return items
+
+    @field_validator("decisions", mode="before")
+    @classmethod
+    def _clean_decisions(cls, value: Any) -> list[str]:
+        return normalise_text_items(value, max_items=20)
+
+    @field_validator("emotional_summary", mode="before")
+    @classmethod
+    def _clean_emotional_summary(cls, value: Any) -> str:
+        lines: list[str]
+        if isinstance(value, str):
+            lines = [line.strip() for line in value.splitlines() if line.strip()]
+        else:
+            lines = normalise_text_items(value, max_items=3)
+        if not lines:
+            return "Neutral and focused discussion."
+        return "\n".join(lines[:3])
+
+
+def _language_name(code: str) -> str:
+    return _LANGUAGE_NAME_MAP.get(code, code.upper())
+
+
+def _chunk_text_for_prompt(text: str, *, max_chars: int = 500) -> list[str]:
+    normalized = " ".join(text.split())
+    if not normalized:
+        return []
+    if len(normalized) <= max_chars:
+        return [normalized]
+
+    chunks: list[str] = []
+    words = normalized.split(" ")
+    current: list[str] = []
+    current_len = 0
+
+    for word in words:
+        word_len = len(word)
+        if not current:
+            if word_len > max_chars:
+                for start in range(0, word_len, max_chars):
+                    chunks.append(word[start : start + max_chars])
+                continue
+            current = [word]
+            current_len = word_len
+            continue
+
+        next_len = current_len + 1 + word_len
+        if next_len > max_chars:
+            chunks.append(" ".join(current))
+            current = [word]
+            current_len = word_len
+        else:
+            current.append(word)
+            current_len = next_len
+
+    if current:
+        chunks.append(" ".join(current))
+    return chunks
+
+
+def _normalise_prompt_speaker_turns(
+    speaker_turns: Sequence[dict[str, Any]],
+    *,
+    max_turns: int | None = None,
+) -> list[dict[str, Any]]:
+    out: list[dict[str, Any]] = []
+    for row in speaker_turns:
+        start = round(safe_float(row.get("start"), default=0.0), 3)
+        end = round(safe_float(row.get("end"), default=0.0), 3)
+        speaker = str(row.get("speaker") or "S1")
+        language = row.get("language")
+        for chunk in _chunk_text_for_prompt(str(row.get("text") or "").strip()):
+            if max_turns is not None and max_turns >= 0 and len(out) >= max_turns:
+                return out
+            payload: dict[str, Any] = {
+                "start": start,
+                "end": end,
+                "speaker": speaker,
+                "text": chunk,
+            }
+            if isinstance(language, str) and language.strip():
+                payload["language"] = language.strip()
+            out.append(payload)
+    return out
+
+
+def build_structured_summary_prompts(
+    speaker_turns: Sequence[dict[str, Any]],
+    target_summary_language: str,
+    *,
+    calendar_title: str | None = None,
+    calendar_attendees: Sequence[str] | None = None,
+) -> tuple[str, str]:
+    language_name = _language_name(target_summary_language)
+    sys_prompt = (
+        "You are an assistant that summarizes meeting transcripts. "
+        f"Write topic, summary_bullets, decisions, action_items, emotional_summary, and questions in {language_name}. "
+        "Keep names, quotes, and domain terms in their original language when needed. "
+        "Return strict JSON only, with no markdown fences."
+    )
+    prompt_payload = {
+        "target_summary_language": target_summary_language,
+        "calendar": {
+            "title": (calendar_title or "").strip() or None,
+            "attendees": [str(item).strip() for item in (calendar_attendees or []) if str(item).strip()],
+        },
+        "speaker_turns": _normalise_prompt_speaker_turns(speaker_turns),
+        "required_schema": {
+            "topic": "string",
+            "summary_bullets": ["string"],
+            "decisions": ["string"],
+            "action_items": [
+                {
+                    "task": "string",
+                    "owner": "string|null",
+                    "deadline": "string|null",
+                    "confidence": "number [0,1]",
+                }
+            ],
+            "emotional_summary": "1-3 short lines as a string",
+            "questions": {
+                "total_count": "integer >= 0",
+                "types": {key: "integer >= 0" for key in _QUESTION_TYPE_KEYS},
+                "extracted": ["string"],
+            },
+        },
+    }
+    user_prompt = json.dumps(prompt_payload, ensure_ascii=False, indent=2)
+    return sys_prompt, user_prompt
+
+
+def build_summary_prompts(clean_text: str, target_summary_language: str) -> tuple[str, str]:
+    pseudo_turns = []
+    stripped = clean_text.strip()
+    if stripped:
+        pseudo_turns.append({"start": 0.0, "end": 0.0, "speaker": "S1", "text": stripped})
+    return build_structured_summary_prompts(
+        pseudo_turns,
+        target_summary_language,
+    )
+
+
+def _extract_json_dict(raw_content: str) -> dict[str, Any] | None:
+    text = raw_content.strip()
+    if not text:
+        return None
+
+    candidates: list[str] = [text]
+    fenced_matches = re.findall(r"```(?:json)?\s*(.*?)```", text, flags=re.IGNORECASE | re.DOTALL)
+    candidates.extend(match.strip() for match in fenced_matches if match.strip())
+
+    first_brace = text.find("{")
+    last_brace = text.rfind("}")
+    if first_brace != -1 and last_brace > first_brace:
+        candidates.append(text[first_brace : last_brace + 1].strip())
+
+    seen: set[str] = set()
+    for candidate in candidates:
+        if candidate in seen:
+            continue
+        seen.add(candidate)
+        try:
+            payload = json.loads(candidate)
+        except ValueError:
+            continue
+        if isinstance(payload, dict):
+            return payload
+    return None
+
+
+def _summary_text_from_bullets(summary_bullets: Sequence[str]) -> str:
+    return "\n".join(f"- {bullet}" for bullet in summary_bullets)
+
+
+def _normalise_action_items_fallback(value: Any) -> list[dict[str, Any]]:
+    rows: list[Any]
+    if isinstance(value, list):
+        rows = value
+    elif value is None:
+        rows = []
+    else:
+        rows = [value]
+
+    out: list[dict[str, Any]] = []
+    for row in rows:
+        if len(out) >= 30:
+            break
+        if isinstance(row, dict):
+            task = str(row.get("task") or row.get("action") or row.get("title") or "").strip()
+            owner_raw = row.get("owner")
+            deadline_raw = row.get("deadline") or row.get("due")
+            confidence_raw = row.get("confidence", row.get("score"))
+        else:
+            task = str(row).strip()
+            owner_raw = None
+            deadline_raw = None
+            confidence_raw = None
+        if not task:
+            continue
+        owner = str(owner_raw).strip() if owner_raw is not None else ""
+        deadline = str(deadline_raw).strip() if deadline_raw is not None else ""
+        confidence = safe_float(confidence_raw, default=0.5)
+        out.append(
+            {
+                "task": task,
+                "owner": owner or None,
+                "deadline": deadline or None,
+                "confidence": round(min(max(confidence, 0.0), 1.0), 2),
+            }
+        )
+    return out
+
+
+def _normalise_questions_fallback(value: Any) -> dict[str, Any]:
+    total_count = 0
+    question_types = {key: 0 for key in _QUESTION_TYPE_KEYS}
+    extracted: list[str] = []
+
+    if isinstance(value, dict):
+        total_count = max(0, int(safe_float(value.get("total_count"), default=0.0)))
+        types_payload = value.get("types") if isinstance(value.get("types"), dict) else value
+        for key in _QUESTION_TYPE_KEYS:
+            question_types[key] = max(0, int(safe_float(types_payload.get(key), default=0.0)))
+        extracted = normalise_text_items(value.get("extracted"), max_items=20)
+
+    inferred_total = max(sum(question_types.values()), len(extracted))
+    if total_count == 0:
+        total_count = inferred_total
+
+    return {
+        "total_count": total_count,
+        "types": question_types,
+        "extracted": extracted,
+    }
+
+
+def _build_structured_summary_payload(
+    *,
+    model: str,
+    target_summary_language: str,
+    friendly: int,
+    topic: str,
+    summary_bullets: Sequence[str],
+    decisions: Sequence[str],
+    action_items: Sequence[dict[str, Any]],
+    emotional_summary: str,
+    questions: dict[str, Any],
+    status: str | None = None,
+    reason: str | None = None,
+    error: str | None = None,
+    parse_error: bool = False,
+    parse_error_reason: str | None = None,
+) -> dict[str, Any]:
+    payload: dict[str, Any] = {
+        "friendly": int(friendly),
+        "model": model,
+        "target_summary_language": target_summary_language,
+        "topic": topic,
+        "summary_bullets": list(summary_bullets),
+        "summary": _summary_text_from_bullets(summary_bullets),
+        "decisions": list(decisions),
+        "action_items": list(action_items),
+        "emotional_summary": emotional_summary,
+        "questions": questions,
+    }
+    if status:
+        payload["status"] = status
+    if reason:
+        payload["reason"] = reason
+    if error:
+        payload["error"] = error
+    if parse_error:
+        payload["parse_error"] = True
+        payload["parse_error_reason"] = parse_error_reason or "validation_failed"
+    return payload
+
+
+def _fallback_payload(
+    *,
+    raw_llm_content: str,
+    extracted: dict[str, Any],
+    model: str,
+    target_summary_language: str,
+    friendly: int,
+    default_topic: str,
+    parse_error_reason: str,
+) -> dict[str, Any]:
+    summary_bullets = normalise_text_items(extracted.get("summary_bullets"), max_items=12)
+    if not summary_bullets:
+        summary_bullets = normalise_text_items(extracted.get("summary"), max_items=12)
+    if not summary_bullets:
+        summary_bullets = normalise_text_items(raw_llm_content, max_items=12)
+    if not summary_bullets:
+        summary_bullets = ["No summary available."]
+
+    topic = str(extracted.get("topic") or "").strip()
+    if not topic:
+        topic = summary_bullets[0][:120] if summary_bullets else default_topic
+    topic = topic or default_topic
+
+    emotional_lines = normalise_text_items(extracted.get("emotional_summary"), max_items=3)
+    emotional_summary = "\n".join(emotional_lines) if emotional_lines else "Neutral and focused discussion."
+
+    return _build_structured_summary_payload(
+        model=model,
+        target_summary_language=target_summary_language,
+        friendly=friendly,
+        topic=topic,
+        summary_bullets=summary_bullets,
+        decisions=normalise_text_items(extracted.get("decisions"), max_items=20),
+        action_items=_normalise_action_items_fallback(extracted.get("action_items")),
+        emotional_summary=emotional_summary,
+        questions=_normalise_questions_fallback(extracted.get("questions")),
+        parse_error=True,
+        parse_error_reason=parse_error_reason,
+    )
+
+
+def _validation_reason(exc: ValidationError) -> str:
+    errors = exc.errors()
+    if not errors:
+        return "validation_failed"
+    first = errors[0]
+    location = ".".join(str(part) for part in first.get("loc", []))
+    message = str(first.get("msg") or "invalid value")
+    return f"{location}: {message}" if location else message
+
+
+def build_summary_payload(
+    *,
+    raw_llm_content: str,
+    model: str,
+    target_summary_language: str,
+    friendly: int,
+    default_topic: str = "Meeting summary",
+    derived_dir: Path | None = None,
+) -> dict[str, Any]:
+    extracted = _extract_json_dict(raw_llm_content)
+    if extracted is None:
+        extracted = {}
+        reason = "json_object_not_found"
+        if derived_dir is not None:
+            write_llm_debug_artifacts(
+                LlmDebugArtifacts(
+                    derived_dir=derived_dir,
+                    raw_output=raw_llm_content,
+                    extracted_payload=extracted,
+                    validation_error={"reason": reason},
+                )
+            )
+        return _fallback_payload(
+            raw_llm_content=raw_llm_content,
+            extracted=extracted,
+            model=model,
+            target_summary_language=target_summary_language,
+            friendly=friendly,
+            default_topic=default_topic,
+            parse_error_reason=reason,
+        )
+
+    candidate = dict(extracted)
+    if "summary_bullets" not in candidate and "summary" in candidate:
+        candidate["summary_bullets"] = normalise_text_items(candidate.get("summary"), max_items=12)
+    if "topic" not in candidate:
+        candidate["topic"] = default_topic
+
+    try:
+        validated = SummaryResponse.model_validate(candidate)
+    except ValidationError as exc:
+        reason = _validation_reason(exc)
+        if derived_dir is not None:
+            write_llm_debug_artifacts(
+                LlmDebugArtifacts(
+                    derived_dir=derived_dir,
+                    raw_output=raw_llm_content,
+                    extracted_payload=extracted,
+                    validation_error={
+                        "reason": reason,
+                        "errors": json.loads(exc.json()),
+                    },
+                )
+            )
+        return _fallback_payload(
+            raw_llm_content=raw_llm_content,
+            extracted=extracted,
+            model=model,
+            target_summary_language=target_summary_language,
+            friendly=friendly,
+            default_topic=default_topic,
+            parse_error_reason=reason,
+        )
+
+    return _build_structured_summary_payload(
+        model=model,
+        target_summary_language=target_summary_language,
+        friendly=friendly,
+        topic=validated.topic or default_topic,
+        summary_bullets=validated.summary_bullets,
+        decisions=validated.decisions,
+        action_items=[item.model_dump() for item in validated.action_items],
+        emotional_summary=validated.emotional_summary,
+        questions=validated.questions.model_dump(),
+    )
+
+
+__all__ = [
+    "ActionItem",
+    "Question",
+    "SummaryResponse",
+    "build_summary_payload",
+    "build_summary_prompts",
+    "build_structured_summary_prompts",
+]
diff --git a/lan_transcriber/utils.py b/lan_transcriber/utils.py
new file mode 100644
index 0000000..0840cf5
--- /dev/null
+++ b/lan_transcriber/utils.py
@@ -0,0 +1,92 @@
+from __future__ import annotations
+
+import math
+import re
+from typing import Any
+
+_LANGUAGE_CODE_MAP: dict[str, str] = {
+    "eng": "en",
+    "spa": "es",
+    "fra": "fr",
+    "fre": "fr",
+    "deu": "de",
+    "ger": "de",
+    "ita": "it",
+    "por": "pt",
+    "rus": "ru",
+    "ukr": "uk",
+    "jpn": "ja",
+    "kor": "ko",
+    "zho": "zh",
+    "chi": "zh",
+}
+
+
+def safe_float(
+    value: Any,
+    default: float = 0.0,
+    *,
+    min_value: float | None = None,
+    max_value: float | None = None,
+) -> float:
+    """Best-effort numeric parse with optional bounds."""
+    try:
+        parsed = float(value)
+    except (TypeError, ValueError):
+        return default
+    if not math.isfinite(parsed):
+        return default
+    if min_value is not None and parsed < min_value:
+        return default
+    if max_value is not None and parsed > max_value:
+        return default
+    return parsed
+
+
+def normalise_language_code(value: Any) -> str | None:
+    """Convert language values like EN-us / eng into ISO-639-1 codes."""
+    if not isinstance(value, str):
+        return None
+    raw = value.strip().lower()
+    if not raw:
+        return None
+    token = raw.replace("_", "-").split("-", 1)[0]
+    if not token.isalpha():
+        return None
+    if len(token) == 2:
+        return token
+    if len(token) == 3:
+        return _LANGUAGE_CODE_MAP.get(token)
+    return None
+
+
+def normalise_text_items(
+    value: Any,
+    *,
+    max_items: int,
+    strip_bullets: bool = True,
+) -> list[str]:
+    """Normalise list-like or multiline text payloads into clean lines."""
+    rows: list[Any]
+    if isinstance(value, list):
+        rows = value
+    elif isinstance(value, str):
+        rows = [line.strip() for line in value.splitlines() if line.strip()]
+    else:
+        return []
+
+    out: list[str] = []
+    for item in rows:
+        if len(out) >= max_items:
+            break
+        text = str(item).strip()
+        if not text:
+            continue
+        if strip_bullets:
+            text = re.sub(r"^[\-*\u2022]+\s*", "", text).strip()
+        if text:
+            out.append(text)
+    return out
+
+
+__all__ = ["safe_float", "normalise_language_code", "normalise_text_items"]
diff --git a/tests/test_pipeline_steps_artifacts.py b/tests/test_pipeline_steps_artifacts.py
new file mode 100644
index 0000000..bd7df73
--- /dev/null
+++ b/tests/test_pipeline_steps_artifacts.py
@@ -0,0 +1,33 @@
+from __future__ import annotations
+
+import json
+from pathlib import Path
+
+from lan_transcriber.pipeline_steps.artifacts import LlmDebugArtifacts, write_json_artifact, write_llm_debug_artifacts
+
+
+def test_write_json_artifact_replaces_content_with_valid_json(tmp_path: Path):
+    path = tmp_path / "derived" / "metrics.json"
+
+    write_json_artifact(path, {"status": "running", "value": 1})
+    write_json_artifact(path, {"status": "ok", "value": 2})
+
+    payload = json.loads(path.read_text(encoding="utf-8"))
+    assert payload == {"status": "ok", "value": 2}
+
+
+def test_write_llm_debug_artifacts_creates_expected_files(tmp_path: Path):
+    spec = LlmDebugArtifacts(
+        derived_dir=tmp_path / "derived",
+        raw_output="raw",
+        extracted_payload={"topic": "T"},
+        validation_error={"reason": "bad_schema"},
+    )
+
+    write_llm_debug_artifacts(spec)
+
+    assert (spec.derived_dir / "llm_raw.txt").read_text(encoding="utf-8") == "raw"
+    assert json.loads((spec.derived_dir / "llm_extract.json").read_text(encoding="utf-8")) == {"topic": "T"}
+    assert json.loads((spec.derived_dir / "llm_validation_error.json").read_text(encoding="utf-8")) == {
+        "reason": "bad_schema"
+    }
diff --git a/tests/test_pipeline_steps_language.py b/tests/test_pipeline_steps_language.py
new file mode 100644
index 0000000..6a2466a
--- /dev/null
+++ b/tests/test_pipeline_steps_language.py
@@ -0,0 +1,35 @@
+from __future__ import annotations
+
+from lan_transcriber.pipeline_steps.language import (
+    analyse_languages,
+    resolve_target_summary_language,
+    segment_language,
+)
+
+
+def test_segment_language_prefers_override_then_detected_then_guess():
+    segment = {"text": "hola equipo y gracias"}
+    assert segment_language(segment, detected_language="fr", transcript_language_override="es") == "es"
+    assert segment_language(segment, detected_language="fr", transcript_language_override=None) == "fr"
+    assert segment_language(segment, detected_language=None, transcript_language_override=None) == "es"
+
+
+def test_analyse_languages_returns_dominant_distribution_and_spans():
+    rows = [
+        {"start": 0.0, "end": 3.0, "text": "hello team", "language": "en"},
+        {"start": 3.0, "end": 11.0, "text": "hola equipo", "language": "es"},
+    ]
+
+    analysis = analyse_languages(rows, detected_language="en", transcript_language_override=None)
+
+    assert analysis.dominant_language == "es"
+    assert set(analysis.distribution) == {"en", "es"}
+    assert analysis.spans[0]["lang"] == "en"
+    assert analysis.spans[1]["lang"] == "es"
+
+
+def test_resolve_target_summary_language_fallback_order():
+    assert resolve_target_summary_language("fr", dominant_language="en", detected_language="es") == "fr"
+    assert resolve_target_summary_language(None, dominant_language="en", detected_language="es") == "en"
+    assert resolve_target_summary_language(None, dominant_language="unknown", detected_language="es") == "es"
+    assert resolve_target_summary_language(None, dominant_language="unknown", detected_language=None) == "en"
diff --git a/tests/test_pipeline_steps_precheck.py b/tests/test_pipeline_steps_precheck.py
new file mode 100644
index 0000000..585de4e
--- /dev/null
+++ b/tests/test_pipeline_steps_precheck.py
@@ -0,0 +1,150 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+from lan_transcriber.pipeline_steps import precheck
+
+
+def _audio(tmp_path: Path) -> Path:
+    path = tmp_path / "a.wav"
+    path.write_bytes(b"fake")
+    return path
+
+
+def test_run_precheck_duration_threshold(tmp_path: Path, monkeypatch):
+    audio = _audio(tmp_path)
+    monkeypatch.setattr(precheck, "_audio_duration_from_wave", lambda _p: 10.0)
+    monkeypatch.setattr(precheck, "_speech_ratio_from_wave", lambda _p: 0.8)
+
+    result = precheck.run_precheck(audio, min_duration_sec=20.0, min_speech_ratio=0.1)
+
+    assert result.quarantine_reason == "duration_lt_20s"
+
+
+def test_run_precheck_vad_threshold(tmp_path: Path, monkeypatch):
+    audio = _audio(tmp_path)
+    monkeypatch.setattr(precheck, "_audio_duration_from_wave", lambda _p: 30.0)
+    monkeypatch.setattr(precheck, "_speech_ratio_from_wave", lambda _p: 0.02)
+
+    result = precheck.run_precheck(audio, min_duration_sec=20.0, min_speech_ratio=0.1)
+
+    assert result.quarantine_reason == "speech_ratio_lt_0.10"
+
+
+def test_run_precheck_metrics_unavailable(tmp_path: Path, monkeypatch):
+    audio = _audio(tmp_path)
+    monkeypatch.setattr(precheck, "_audio_duration_from_wave", lambda _p: None)
+    monkeypatch.setattr(precheck, "_audio_duration_from_ffprobe", lambda _p: None)
+    monkeypatch.setattr(precheck, "_speech_ratio_from_wave", lambda _p: None)
+    monkeypatch.setattr(precheck, "_speech_ratio_from_ffmpeg", lambda _p: None)
+
+    result = precheck.run_precheck(audio, min_duration_sec=20.0, min_speech_ratio=0.1)
+
+    assert result.quarantine_reason == "precheck_metrics_unavailable"
+    assert result.duration_sec is None
+    assert result.speech_ratio is None
+
+
+def test_audio_duration_from_ffprobe_parses_successful_output(tmp_path: Path, monkeypatch):
+    audio = _audio(tmp_path)
+    monkeypatch.setattr(precheck.shutil, "which", lambda _name: "/usr/bin/ffprobe")
+
+    class _Proc:
+        returncode = 0
+        stdout = "12.5\n"
+
+    monkeypatch.setattr(precheck.subprocess, "run", lambda *_a, **_k: _Proc())
+    assert precheck._audio_duration_from_ffprobe(audio) == 12.5
+
+
+def test_audio_duration_from_wave_zero_rate_and_exception_paths(tmp_path: Path, monkeypatch):
+    audio = _audio(tmp_path)
+    wav = tmp_path / "a.wav"
+    wav.write_bytes(audio.read_bytes())
+
+    class _Wave:
+        def __enter__(self):
+            return self
+
+        def __exit__(self, exc_type, exc, tb):
+            return False
+
+        def getframerate(self):
+            return 0
+
+        def getnframes(self):
+            return 100
+
+    monkeypatch.setattr(precheck.wave, "open", lambda *_a, **_k: _Wave())
+    assert precheck._audio_duration_from_wave(wav) is None
+
+    monkeypatch.setattr(precheck.wave, "open", lambda *_a, **_k: (_ for _ in ()).throw(OSError("boom")))
+    assert precheck._audio_duration_from_wave(wav) is None
+
+
+def test_speech_ratio_from_wave_short_chunk_returns_zero(tmp_path: Path, monkeypatch):
+    wav = tmp_path / "a.wav"
+    wav.write_bytes(b"fake")
+
+    class _Wave:
+        def __enter__(self):
+            return self
+
+        def __exit__(self, exc_type, exc, tb):
+            return False
+
+        def getframerate(self):
+            return 16000
+
+        def getnchannels(self):
+            return 1
+
+        def getsampwidth(self):
+            return 2
+
+        def readframes(self, _n):
+            return b"\x00"  # shorter than required half frame
+
+    monkeypatch.setattr(precheck.wave, "open", lambda *_a, **_k: _Wave())
+    assert precheck._speech_ratio_from_wave(wav) == 0.0
+
+
+def test_speech_ratio_from_ffmpeg_voiced_and_error_paths(tmp_path: Path, monkeypatch):
+    audio = _audio(tmp_path)
+    monkeypatch.setattr(precheck.shutil, "which", lambda _name: "/usr/bin/ffmpeg")
+
+    class _Stdout:
+        def __init__(self, chunks: list[bytes]):
+            self._chunks = chunks
+
+        def read(self, _size: int) -> bytes:
+            return self._chunks.pop(0)
+
+    class _Proc:
+        def __init__(self, chunks: list[bytes], returncode: int):
+            self.stdout = _Stdout(chunks)
+            self.returncode = returncode
+
+        def wait(self, timeout=None):
+            return None
+
+        def __enter__(self):
+            return self
+
+        def __exit__(self, exc_type, exc, tb):
+            return False
+
+    monkeypatch.setattr(precheck.audioop, "rms", lambda _chunk, _width: 400)
+    monkeypatch.setattr(
+        precheck.subprocess,
+        "Popen",
+        lambda *_a, **_k: _Proc([b"\x01" * 960, b""], 0),
+    )
+    assert precheck._speech_ratio_from_ffmpeg(audio) == 1.0
+
+    monkeypatch.setattr(
+        precheck.subprocess,
+        "Popen",
+        lambda *_a, **_k: _Proc([b"\x01" * 100, b""], 1),
+    )
+    assert precheck._speech_ratio_from_ffmpeg(audio) is None
diff --git a/tests/test_pipeline_steps_snippets.py b/tests/test_pipeline_steps_snippets.py
new file mode 100644
index 0000000..3ce5249
--- /dev/null
+++ b/tests/test_pipeline_steps_snippets.py
@@ -0,0 +1,81 @@
+from __future__ import annotations
+
+import wave
+from pathlib import Path
+
+from lan_transcriber.pipeline_steps import snippets
+from lan_transcriber.pipeline_steps.snippets import SnippetExportRequest, export_speaker_snippets
+
+
+def _wav(path: Path, *, duration_sec: float = 1.0) -> Path:
+    rate = 16000
+    samples = int(rate * duration_sec)
+    payload = b"\x00\x00" * samples
+    path.parent.mkdir(parents=True, exist_ok=True)
+    with wave.open(str(path), "wb") as wav_out:
+        wav_out.setnchannels(1)
+        wav_out.setsampwidth(2)
+        wav_out.setframerate(rate)
+        wav_out.writeframes(payload)
+    return path
+
+
+def test_clear_dir_creates_missing_directory(tmp_path: Path):
+    target = tmp_path / "snippets"
+    snippets._clear_dir(target)
+    assert target.exists()
+    assert target.is_dir()
+
+
+def test_snippet_window_clamps_to_recording_duration():
+    start, end = snippets._snippet_window(9.0, 13.0, duration_sec=10.0)
+    assert 0.0 <= start <= end <= 10.0
+
+
+def test_extract_wav_snippet_with_ffmpeg_success_and_failure(tmp_path: Path, monkeypatch):
+    audio = _wav(tmp_path / "in.wav", duration_sec=2.0)
+    out_path = tmp_path / "out" / "1.wav"
+
+    monkeypatch.setattr(snippets.shutil, "which", lambda _name: "/usr/bin/ffmpeg")
+
+    class _Proc:
+        def __init__(self, code: int):
+            self.returncode = code
+
+    def _run_ok(*_a, **_k):
+        out_path.parent.mkdir(parents=True, exist_ok=True)
+        out_path.write_bytes(b"R" * 100)
+        return _Proc(0)
+
+    monkeypatch.setattr(snippets.subprocess, "run", _run_ok)
+    assert snippets._extract_wav_snippet_with_ffmpeg(audio, out_path, start_sec=0.0, end_sec=0.8)
+
+    monkeypatch.setattr(snippets.subprocess, "run", lambda *_a, **_k: (_ for _ in ()).throw(RuntimeError("boom")))
+    assert not snippets._extract_wav_snippet_with_ffmpeg(audio, out_path, start_sec=0.0, end_sec=0.8)
+
+
+def test_export_speaker_snippets_falls_back_to_silence(tmp_path: Path, monkeypatch):
+    audio = _wav(tmp_path / "src.wav", duration_sec=4.0)
+    out_dir = tmp_path / "derived" / "snippets"
+    stale = out_dir / "S1" / "old.wav"
+    stale.parent.mkdir(parents=True, exist_ok=True)
+    stale.write_bytes(b"stale")
+
+    monkeypatch.setattr(snippets, "_extract_wav_snippet_with_wave", lambda *_a, **_k: False)
+    monkeypatch.setattr(snippets, "_extract_wav_snippet_with_ffmpeg", lambda *_a, **_k: False)
+
+    outputs = export_speaker_snippets(
+        SnippetExportRequest(
+            audio_path=audio,
+            diar_segments=[
+                {"start": 0.0, "end": 1.5, "speaker": "S1"},
+                {"start": 2.0, "end": 3.5, "speaker": "S1"},
+            ],
+            snippets_dir=out_dir,
+            duration_sec=4.0,
+        )
+    )
+
+    assert len(outputs) == 2
+    assert all(path.exists() and path.stat().st_size > 44 for path in outputs)
+    assert not stale.exists()
diff --git a/tests/test_pipeline_steps_speaker_turns.py b/tests/test_pipeline_steps_speaker_turns.py
new file mode 100644
index 0000000..f6dc92d
--- /dev/null
+++ b/tests/test_pipeline_steps_speaker_turns.py
@@ -0,0 +1,62 @@
+from __future__ import annotations
+
+from lan_transcriber.pipeline_steps.speaker_turns import (
+    build_speaker_turns,
+    count_interruptions,
+    normalise_asr_segments,
+)
+
+
+def test_normalise_asr_segments_keeps_word_timestamps():
+    rows = [
+        {
+            "start": 0.0,
+            "end": 1.5,
+            "text": "hello team",
+            "words": [{"start": 0.0, "end": 0.4, "word": "hello"}],
+        }
+    ]
+    payload = normalise_asr_segments(rows)
+    assert payload[0]["words"][0]["word"] == "hello"
+
+
+def test_build_speaker_turns_assigns_speakers_from_diarization():
+    asr_segments = [
+        {
+            "start": 0.0,
+            "end": 1.0,
+            "text": "hello",
+            "words": [{"start": 0.0, "end": 0.6, "word": "hello"}],
+            "language": "en",
+        },
+        {
+            "start": 2.0,
+            "end": 3.0,
+            "text": "status",
+            "words": [{"start": 2.0, "end": 2.4, "word": "status"}],
+            "language": "en",
+        },
+    ]
+    diar_segments = [
+        {"start": 0.0, "end": 1.5, "speaker": "S1"},
+        {"start": 1.5, "end": 3.5, "speaker": "S2"},
+    ]
+
+    turns = build_speaker_turns(asr_segments, diar_segments, default_language="en")
+
+    assert turns[0]["speaker"] == "S1"
+    assert turns[-1]["speaker"] == "S2"
+
+
+def test_count_interruptions_small_synthetic_case():
+    turns = [
+        {"start": 0.0, "end": 4.0, "speaker": "S1", "text": "long turn"},
+        {"start": 3.7, "end": 5.0, "speaker": "S2", "text": "interrupts"},
+        {"start": 5.1, "end": 6.0, "speaker": "S1", "text": "continues"},
+    ]
+
+    stats = count_interruptions(turns, overlap_threshold=0.2)
+
+    assert stats["total"] == 1
+    assert stats["done"]["S2"] == 1
+    assert stats["received"]["S1"] == 1
diff --git a/tests/test_pipeline_steps_summary_builder.py b/tests/test_pipeline_steps_summary_builder.py
new file mode 100644
index 0000000..7421178
--- /dev/null
+++ b/tests/test_pipeline_steps_summary_builder.py
@@ -0,0 +1,72 @@
+from __future__ import annotations
+
+import json
+from pathlib import Path
+
+from lan_transcriber.pipeline_steps.summary_builder import build_summary_payload
+
+
+def test_build_summary_payload_valid_json_uses_schema():
+    payload = build_summary_payload(
+        raw_llm_content=json.dumps(
+            {
+                "topic": "Sync",
+                "summary_bullets": ["Discussed rollout"],
+                "decisions": ["Ship Friday"],
+                "action_items": [{"task": "Send recap", "owner": "Alex", "deadline": "2026-03-01", "confidence": 0.9}],
+                "emotional_summary": "Focused and positive.",
+                "questions": {"total_count": 1, "types": {"status": 1}, "extracted": ["Is QA done?"]},
+            }
+        ),
+        model="m",
+        target_summary_language="en",
+        friendly=50,
+    )
+
+    assert payload["topic"] == "Sync"
+    assert payload["action_items"][0]["owner"] == "Alex"
+    assert payload.get("parse_error") is None
+
+
+def test_build_summary_payload_invalid_json_writes_debug_artifacts_and_sets_parse_error(tmp_path: Path):
+    derived = tmp_path / "derived"
+    raw = json.dumps(
+        {
+            "topic": "Bad payload",
+            "summary_bullets": 123,
+            "decisions": ["Keep timeline"],
+            "action_items": [{"task": "Send minutes", "owner": "Mina"}],
+            "emotional_summary": "Neutral",
+            "questions": {"total_count": 2, "types": {"open": 2}, "extracted": ["Who owns this?"]},
+        }
+    )
+
+    payload = build_summary_payload(
+        raw_llm_content=raw,
+        model="m",
+        target_summary_language="en",
+        friendly=20,
+        derived_dir=derived,
+    )
+
+    assert payload["parse_error"] is True
+    assert payload["action_items"][0]["task"] == "Send minutes"
+    assert payload["questions"]["total_count"] >= 1
+    assert (derived / "llm_raw.txt").exists()
+    assert (derived / "llm_extract.json").exists()
+    assert (derived / "llm_validation_error.json").exists()
+
+
+def test_build_summary_payload_no_json_object_sets_parse_error(tmp_path: Path):
+    derived = tmp_path / "derived"
+    payload = build_summary_payload(
+        raw_llm_content="- plain text summary",
+        model="m",
+        target_summary_language="en",
+        friendly=0,
+        derived_dir=derived,
+    )
+
+    assert payload["parse_error"] is True
+    assert payload["parse_error_reason"] == "json_object_not_found"
+    assert json.loads((derived / "llm_extract.json").read_text(encoding="utf-8")) == {}
diff --git a/tests/test_utils.py b/tests/test_utils.py
new file mode 100644
index 0000000..026d4c5
--- /dev/null
+++ b/tests/test_utils.py
@@ -0,0 +1,22 @@
+from __future__ import annotations
+
+from lan_transcriber.utils import normalise_language_code, normalise_text_items, safe_float
+
+
+def test_safe_float_handles_invalid_and_bounds():
+    assert safe_float("1.25") == 1.25
+    assert safe_float("NaN", default=0.7) == 0.7
+    assert safe_float("-5", default=0.1, min_value=0.0) == 0.1
+    assert safe_float("6", default=0.2, max_value=5.0) == 0.2
+
+
+def test_normalise_language_code_handles_iso639_1_and_3():
+    assert normalise_language_code("en") == "en"
+    assert normalise_language_code("EN-us") == "en"
+    assert normalise_language_code("spa") == "es"
+    assert normalise_language_code("___") is None
+
+
+def test_normalise_text_items_strips_bullets_and_caps_items():
+    value = ["- one", "* two", "  ", "three", "four"]
+    assert normalise_text_items(value, max_items=3) == ["one", "two", "three"]
