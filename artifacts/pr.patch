diff --git a/.env.example b/.env.example
index 88dd210..712aa76 100644
--- a/.env.example
+++ b/.env.example
@@ -23,19 +23,9 @@ LAN_DB_PATH=/data/db/app.db
 LAN_SQLITE_BUSY_TIMEOUT_MS=30000
 LAN_REDIS_URL=redis://redis:6379/0
 LAN_RQ_QUEUE_NAME=lan-transcriber
+UPLOAD_MAX_BYTES=
 QUARANTINE_RETENTION_DAYS=7
 
-# Google Drive ingest (Service Account)
-GDRIVE_SA_JSON_PATH=/data/secrets/gdrive_sa.json
-GDRIVE_INBOX_FOLDER_ID=
-GDRIVE_POLL_INTERVAL_SECONDS=60
-
-# Microsoft Graph delegated auth (Device Code Flow)
-MS_TENANT_ID=
-MS_CLIENT_ID=
-MS_SCOPES=offline_access User.Read Notes.ReadWrite Calendars.Read
-CALENDAR_MATCH_WINDOW_MINUTES=45
-CALENDAR_AUTO_MATCH_THRESHOLD=0.6
 ROUTING_AUTO_SELECT_THRESHOLD=0.65
 
 # Plaud fetcher (beta)
diff --git a/README.md b/README.md
index 2106685..850b288 100644
--- a/README.md
+++ b/README.md
@@ -34,6 +34,15 @@ This produces:
 Operational setup, failure handling, backup/restore, and upgrade steps are documented in
 [`docs/runbook.md`](docs/runbook.md).
 
+## Workflow (Upload -> Processing -> Export)
+
+1. Open `/upload` and add one or more audio files.
+2. Track per-file upload progress and processing progress on the same page.
+3. Open the recording detail page at `/recordings/{recording_id}`.
+4. Export results:
+   - Copy markdown from the export tab for manual OneNote paste.
+   - Download ZIP from `/ui/recordings/{recording_id}/export.zip`.
+
 ## Runtime data root
 
 Runtime mutable state must live under `/data` in containers (mounted from `./data` in Docker):
@@ -41,8 +50,7 @@ Runtime mutable state must live under `/data` in containers (mounted from `./dat
 - `/data/db/app.db`
 - `/data/db/speaker_bank.yaml`
 - `/data/recordings/<recording_id>/...`
-- `/data/auth/msal_cache.bin`
-- `/data/secrets/gdrive_sa.json`
+- `/data/secrets/...` (optional runtime secrets)
 - `/data/voices`
 - `/data/tmp`
 
@@ -62,6 +70,14 @@ Canonical artifact layout (v1):
 
 Do not commit secrets or runtime-generated state files.
 
+## LAN deployment notes
+
+- Use a persistent host volume for `/data` (for Docker Compose, `./data:/data` by default).
+- Size disk for model cache + uploads + derived artifacts. A practical baseline is:
+  - small teams: 50-100 GB
+  - medium usage with longer recordings: 200 GB+
+- Monitor free space under `/data/recordings` and `/opt/lan_cache/hf`.
+
 ## Staging
 
 The staging environment spins up the application and a tiny LLM model using
@@ -92,7 +108,7 @@ models are cached across runs.
 | `LAN_REDIS_URL` | Redis endpoint for the RQ queue |
 | `LAN_RQ_QUEUE_NAME` | Queue name consumed by the worker |
 | `LAN_API_BEARER_TOKEN` | Optional bearer token for protected POST actions (`/api` and UI POST routes) |
-| `LAN_INGEST_LOCK_TTL_SECONDS` | Redis ingest lock TTL in seconds (default `300`) |
+| `UPLOAD_MAX_BYTES` | Optional max size per uploaded file in bytes (`413` when exceeded) |
 | `QUARANTINE_RETENTION_DAYS` | Retention period for quarantined recording cleanup (default `7`) |
 | `LAN_API_BIND_HOST` | Published API bind host (default `127.0.0.1`) |
 | `LAN_API_PORT` | Published API port (default `7860`) |
@@ -100,9 +116,6 @@ models are cached across runs.
 | `LLM_API_KEY` | Optional API key for the LLM |
 | `LLM_MODEL` | Model name passed to the OpenAI-compatible endpoint |
 | `LLM_TIMEOUT_SECONDS` | Per-request timeout for LLM calls (default `30`) |
-| `MS_TENANT_ID` | Microsoft Entra tenant ID for delegated Device Code Flow |
-| `MS_CLIENT_ID` | Microsoft app registration client ID |
-| `MS_SCOPES` | Graph scopes (default: `offline_access User.Read Notes.ReadWrite Calendars.Read`) |
 
 `LAN_ENV` controls startup validation:
 
@@ -125,7 +138,7 @@ When `LAN_API_BEARER_TOKEN` is set:
 
 - Protected endpoints accept either `Authorization: Bearer <token>` or the HttpOnly cookie from `POST /ui/login`.
 - `GET /healthz`, `GET /healthz/{component}`, `GET /metrics`, and `GET /openapi.json` remain public.
-- `POST /api/actions/ingest` is guarded by a Redis lock (`lan:ingest:lock`) to prevent concurrent ingest runs.
+- Upload and recording action POST routes require auth (for example `POST /api/uploads` and `/ui/recordings/{id}/...`).
 
 ## Staging deploy secrets
 
diff --git a/docker-compose.yml b/docker-compose.yml
index 468613d..5b1c6f6 100644
--- a/docker-compose.yml
+++ b/docker-compose.yml
@@ -26,14 +26,12 @@ x-lan-service: &lan-service
     - FETCH_INTERVAL_SEC=${FETCH_INTERVAL_SEC:-300}
     - TRANSCRIBER_VERSION=${TRANSCRIBER_VERSION:-local}
     - LANG_DEFAULT=${LANG_DEFAULT:-en}
-    - MS_TENANT_ID=${MS_TENANT_ID:-}
-    - MS_CLIENT_ID=${MS_CLIENT_ID:-}
-    - "MS_SCOPES=${MS_SCOPES:-offline_access User.Read Notes.ReadWrite Calendars.Read}"
     - QUARANTINE_RETENTION_DAYS=${QUARANTINE_RETENTION_DAYS:-7}
   volumes:
     - lan_cache:/root/.cache
     - /opt/lan_cache/hf:/root/.cache/huggingface
     - /opt/lan_cache/ollama:/root/.ollama
+    # Persistent runtime state root: DB, uploads, derived artifacts, logs.
     - ./data:/data
   networks:
     - lan_net
diff --git a/docs/runbook.md b/docs/runbook.md
index 30c6ca6..dc40a5e 100644
--- a/docs/runbook.md
+++ b/docs/runbook.md
@@ -23,33 +23,18 @@ This runbook covers day-2 operations for LAN deployment:
 docker compose run --rm api python -m lan_app.healthchecks app
 ```
 
-### 1.2 Google Drive Service Account ingest
+### 1.2 Upload and export flow
 
-1. Place Service Account JSON under `/data/secrets/gdrive_sa.json`.
-2. Share the Drive Inbox folder with the Service Account principal.
-3. Configure:
-   - `GDRIVE_SA_JSON_PATH=/data/secrets/gdrive_sa.json`
-   - `GDRIVE_INBOX_FOLDER_ID=<shared-folder-id>`
-4. Trigger one ingest cycle:
+1. Open `/upload` and upload one or more files (UI sends multipart to `POST /api/uploads`).
+2. Track upload and processing progress per file on `/upload`.
+3. Open `/recordings/{recording_id}` for transcript, summary, and export actions.
+4. Download export bundle from `GET /ui/recordings/{recording_id}/export.zip`.
 
-```bash
-curl -fsS -X POST http://127.0.0.1:7860/api/actions/ingest
-```
-
-### 1.3 Microsoft Graph delegated auth (Device Code Flow)
-
-1. Register an Entra app with delegated Graph scopes:
-   - `offline_access`
-   - `User.Read`
-   - `Calendars.Read`
-   - `Notes.ReadWrite`
-2. Configure `MS_TENANT_ID` and `MS_CLIENT_ID`.
-3. Open `/connections` and complete device-code auth.
-4. Verify:
+### 1.3 Upload size controls
 
-```bash
-curl -fsS http://127.0.0.1:7860/api/connections/ms/verify
-```
+1. Optionally set `UPLOAD_MAX_BYTES` to cap a single uploaded file size.
+2. If set, uploads above this limit are rejected with HTTP `413`.
+3. Keep app and reverse-proxy limits consistent to avoid mismatched failures.
 
 ## 2) Runtime safety defaults
 
@@ -101,20 +86,15 @@ Actions:
 2. Confirm with `curl -fsS http://127.0.0.1:7860/healthz/worker`
 3. Open `/queue` and verify `started`/`finished` transitions resume
 
-### 4.3 Microsoft Graph auth expired
+### 4.3 Upload rejected (`413`)
 
 Symptoms:
-- publish/calendar calls fail with auth errors
+- upload fails with HTTP `413 Request Entity Too Large`
 
 Actions:
-1. Re-run Device Code Flow in `/connections`
-2. If policy requires hard re-auth, remove cache:
-
-```bash
-rm -f /data/auth/msal_cache.bin
-```
-
-3. Reconnect in `/connections`
+1. Confirm application limit: `UPLOAD_MAX_BYTES` in `.env` (if set).
+2. Confirm reverse-proxy size limit (`client_max_body_size`) allows intended file sizes.
+3. Re-test with a file below the configured limits.
 
 ### 4.4 Quarantine growth
 
@@ -131,9 +111,34 @@ Actions:
 docker compose exec api python -c "from lan_app.ops import run_retention_cleanup; print(run_retention_cleanup())"
 ```
 
-## 5) Backup and restore (`/data`)
+## 5) Nginx reverse proxy for large uploads
+
+Set size and timeout directives high enough for your expected media files.
+
+Example:
+
+```nginx
+server {
+    listen 80;
+    server_name _;
+
+    client_max_body_size 1024m;
+
+    location / {
+        proxy_pass http://127.0.0.1:7860;
+        proxy_read_timeout 600s;
+        proxy_send_timeout 600s;
+    }
+}
+```
+
+Notes:
+- `client_max_body_size` gates request body size before traffic reaches the app.
+- `proxy_read_timeout` and `proxy_send_timeout` should cover long upload/processing responses.
+
+## 6) Backup and restore (`/data`)
 
-### 5.1 Backup
+### 6.1 Backup
 
 Stop writes first:
 
@@ -153,7 +158,7 @@ Restart services:
 docker compose start api worker
 ```
 
-### 5.2 Restore
+### 6.2 Restore
 
 1. Stop stack: `docker compose down`
 2. Restore archive to host `/data` mount source (`./data` in this repo)
@@ -162,7 +167,7 @@ docker compose start api worker
    - `curl -fsS http://127.0.0.1:7860/healthz`
    - check recordings list in UI
 
-## 6) Upgrade steps
+## 7) Upgrade steps
 
 1. Pull new code and review `.env.example` diff.
 2. Ensure secrets still resolve from `/data/secrets` or env.
@@ -177,7 +182,7 @@ docker compose up -d --build
    - `curl -fsS http://127.0.0.1:7860/healthz`
    - enqueue and process one test recording
 
-## 7) Retry and failure operations
+## 8) Retry and failure operations
 
 - Failed step retries are available in recording detail, `Log` tab, button `Retry step`.
 - `NeedsReview` is not a failure; it indicates manual review workflow.
diff --git a/tasks/QUEUE.md b/tasks/QUEUE.md
index df29f4c..d4851c7 100644
--- a/tasks/QUEUE.md
+++ b/tasks/QUEUE.md
@@ -162,6 +162,6 @@ Queue (in order)
 - Depends on: PR-REMOVE-MS-01
 
 31) PR-DOCS-EXPORT-ONLY-01: Docs update for upload + export-only mode (README, runbook, env example, nginx notes)
-- Status: TODO
+- Status: DONE
 - Tasks file: tasks/PR-DOCS-EXPORT-ONLY-01.md
 - Depends on: PR-REMOVE-GDRIVE-01
