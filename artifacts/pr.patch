diff --git a/lan_app/gdrive.py b/lan_app/gdrive.py
index 623e8a1..35ec3a2 100644
--- a/lan_app/gdrive.py
+++ b/lan_app/gdrive.py
@@ -28,6 +28,8 @@ from .constants import (
     JOB_TYPE_METRICS,
     RECORDING_STATUS_QUEUED,
 )
+import shutil
+
 from .db import connect, create_recording, init_db
 
 logger = logging.getLogger(__name__)
@@ -137,15 +139,27 @@ def _enqueue_pipeline_jobs(
     recording_id: str,
     settings: AppSettings,
 ) -> list[str]:
-    """Create DB job rows for the full processing pipeline.
+    """Enqueue the first pipeline step into Redis/RQ and create DB
+    placeholder rows for the remaining steps.
 
-    Jobs are created as DB records only (queued status).  The RQ worker
-    picks up the first job; later PRs will chain them.
+    The first step (precheck) is pushed onto the worker queue so
+    processing starts automatically.  Later PRs will chain subsequent
+    steps.
     """
     from .db import create_job
+    from .jobs import enqueue_recording_job
 
     job_ids: list[str] = []
-    for step in _PIPELINE_STEPS:
+
+    # First step: enqueue into Redis/RQ so the worker picks it up
+    first_step = _PIPELINE_STEPS[0]
+    rj = enqueue_recording_job(
+        recording_id, job_type=first_step, settings=settings
+    )
+    job_ids.append(rj.job_id)
+
+    # Remaining steps: DB placeholders only (chaining not yet wired)
+    for step in _PIPELINE_STEPS[1:]:
         job_id = uuid4().hex
         create_job(
             job_id=job_id,
@@ -168,9 +182,9 @@ def ingest_once(
     """
     cfg = settings or AppSettings()
 
-    if cfg.gdrive_sa_json_path is None:
+    if not cfg.gdrive_sa_json_path or not str(cfg.gdrive_sa_json_path).strip():
         raise ValueError("GDRIVE_SA_JSON_PATH is not configured")
-    if cfg.gdrive_inbox_folder_id is None:
+    if not cfg.gdrive_inbox_folder_id or not cfg.gdrive_inbox_folder_id.strip():
         raise ValueError("GDRIVE_INBOX_FOLDER_ID is not configured")
 
     init_db(cfg)
@@ -198,6 +212,9 @@ def ingest_once(
             download_file(service, file_id, dest)
         except Exception:
             logger.exception("Failed to download %s (%s)", file_name, file_id)
+            # Clean up the partially written recording directory
+            rec_dir = cfg.recordings_root / recording_id
+            shutil.rmtree(rec_dir, ignore_errors=True)
             continue
 
         # Parse captured_at from Plaud filename; fall back to Drive createdTime
diff --git a/tests/test_gdrive.py b/tests/test_gdrive.py
index 65e7b06..f7aea88 100644
--- a/tests/test_gdrive.py
+++ b/tests/test_gdrive.py
@@ -18,6 +18,7 @@ from lan_app.gdrive import (
     list_inbox_files,
     parse_plaud_captured_at,
 )
+from lan_app.jobs import RecordingJob
 
 
 def _test_settings(tmp_path: Path) -> AppSettings:
@@ -42,6 +43,37 @@ def _test_settings_no_gdrive(tmp_path: Path) -> AppSettings:
     return cfg
 
 
+def _stub_enqueue(monkeypatch: Any, cfg: AppSettings) -> None:
+    """Replace enqueue_recording_job with a DB-only stub (no Redis)."""
+    from lan_app.db import create_job, set_recording_status
+    from lan_app.constants import JOB_STATUS_QUEUED, RECORDING_STATUS_QUEUED
+    from uuid import uuid4
+
+    def _fake_enqueue(
+        recording_id: str,
+        *,
+        job_type: str = "precheck",
+        settings: AppSettings | None = None,
+    ) -> RecordingJob:
+        effective = settings or cfg
+        job_id = uuid4().hex
+        create_job(
+            job_id=job_id,
+            recording_id=recording_id,
+            job_type=job_type,
+            status=JOB_STATUS_QUEUED,
+            settings=effective,
+        )
+        set_recording_status(
+            recording_id, RECORDING_STATUS_QUEUED, settings=effective
+        )
+        return RecordingJob(
+            job_id=job_id, recording_id=recording_id, job_type=job_type
+        )
+
+    monkeypatch.setattr("lan_app.jobs.enqueue_recording_job", _fake_enqueue)
+
+
 # --------------------------------------------------------------------------
 # parse_plaud_captured_at
 # --------------------------------------------------------------------------
@@ -230,9 +262,26 @@ def test_ingest_once_raises_when_no_folder_id(tmp_path: Path):
         ingest_once(cfg)
 
 
+def test_ingest_once_raises_when_empty_folder_id(tmp_path: Path):
+    cfg = _test_settings_no_gdrive(tmp_path)
+    cfg.gdrive_sa_json_path = tmp_path / "sa.json"
+    cfg.gdrive_inbox_folder_id = ""
+    with pytest.raises(ValueError, match="GDRIVE_INBOX_FOLDER_ID"):
+        ingest_once(cfg)
+
+
+def test_ingest_once_raises_when_whitespace_folder_id(tmp_path: Path):
+    cfg = _test_settings_no_gdrive(tmp_path)
+    cfg.gdrive_sa_json_path = tmp_path / "sa.json"
+    cfg.gdrive_inbox_folder_id = "   "
+    with pytest.raises(ValueError, match="GDRIVE_INBOX_FOLDER_ID"):
+        ingest_once(cfg)
+
+
 def test_ingest_once_skips_known_files(tmp_path: Path, monkeypatch):
     cfg = _test_settings(tmp_path)
     init_db(cfg)
+    _stub_enqueue(monkeypatch, cfg)
 
     from lan_app.db import create_recording
 
@@ -253,9 +302,6 @@ def test_ingest_once_skips_known_files(tmp_path: Path, monkeypatch):
         },
     ]
     svc = _FakeService([{"files": inbox_files}])
-    monkeypatch.setattr(
-        "lan_app.gdrive.MediaIoBaseDownload", _FakeMediaDownload
-    )
 
     results = ingest_once(cfg, service=svc)
     assert results == []
@@ -264,6 +310,7 @@ def test_ingest_once_skips_known_files(tmp_path: Path, monkeypatch):
 def test_ingest_once_downloads_and_creates_recording(tmp_path: Path, monkeypatch):
     cfg = _test_settings(tmp_path)
     init_db(cfg)
+    _stub_enqueue(monkeypatch, cfg)
 
     inbox_files = [
         {
@@ -304,6 +351,7 @@ def test_ingest_once_downloads_and_creates_recording(tmp_path: Path, monkeypatch
 def test_ingest_once_fallback_captured_at_from_drive(tmp_path: Path, monkeypatch):
     cfg = _test_settings(tmp_path)
     init_db(cfg)
+    _stub_enqueue(monkeypatch, cfg)
 
     inbox_files = [
         {
@@ -328,6 +376,7 @@ def test_ingest_once_fallback_captured_at_from_drive(tmp_path: Path, monkeypatch
 def test_ingest_once_continues_on_download_failure(tmp_path: Path, monkeypatch):
     cfg = _test_settings(tmp_path)
     init_db(cfg)
+    _stub_enqueue(monkeypatch, cfg)
 
     inbox_files = [
         {
@@ -345,6 +394,9 @@ def test_ingest_once_continues_on_download_failure(tmp_path: Path, monkeypatch):
 
     def _download_maybe_fail(svc: Any, fid: str, dest: Path) -> Path:
         if fid == "drive-fail":
+            # Simulate a partial write before failure
+            dest.parent.mkdir(parents=True, exist_ok=True)
+            dest.write_bytes(b"partial")
             raise OSError("network error")
         return _write_fake_download(dest)
 
@@ -355,10 +407,45 @@ def test_ingest_once_continues_on_download_failure(tmp_path: Path, monkeypatch):
     assert results[0]["drive_file_id"] == "drive-ok"
 
 
+def test_ingest_once_cleans_up_partial_files_on_download_failure(
+    tmp_path: Path, monkeypatch
+):
+    """Failed downloads should not leave orphaned recording directories."""
+    cfg = _test_settings(tmp_path)
+    init_db(cfg)
+    _stub_enqueue(monkeypatch, cfg)
+
+    inbox_files = [
+        {
+            "id": "drive-partial",
+            "name": "2026-02-18 10_00_00.mp3",
+            "createdTime": "2026-02-18T10:00:00Z",
+        },
+    ]
+    svc = _FakeService([{"files": inbox_files}])
+
+    created_dirs: list[Path] = []
+
+    def _download_fail(svc: Any, fid: str, dest: Path) -> Path:
+        dest.parent.mkdir(parents=True, exist_ok=True)
+        dest.write_bytes(b"partial data")
+        created_dirs.append(dest.parent.parent)  # recordings/<id>
+        raise OSError("network error")
+
+    monkeypatch.setattr("lan_app.gdrive.download_file", _download_fail)
+
+    results = ingest_once(cfg, service=svc)
+    assert results == []
+    assert len(created_dirs) == 1
+    # The orphaned directory should have been cleaned up
+    assert not created_dirs[0].exists()
+
+
 def test_ingest_idempotent(tmp_path: Path, monkeypatch):
     """Running ingest twice with the same files should not duplicate."""
     cfg = _test_settings(tmp_path)
     init_db(cfg)
+    _stub_enqueue(monkeypatch, cfg)
 
     inbox_files = [
         {
